{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import data_processing as dp\n",
    "from scipy import signal\n",
    "from scipy.stats import skew\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('.\\data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 = '.\\data\\正常\\G11_Walking_trial_4_emg.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv('./processed data/dataframe_W256_S64_DWTLmax_samelabel_sc.csv')\n",
    "\n",
    "# read file name of data with various Labels\n",
    "df = pd.read_csv('./useful_data_label.csv',index_col=0) \n",
    "drop = 'G07_Freezing_Trial1_trial_1_emg.csv'\n",
    "drop2= 'P812_M050_2_B_FoG_trial_2_emg.csv'\n",
    "drop3= 'P812_M050_2_B_FoG_trial_1_emg.csv'\n",
    "ind_drop = (df.columns!=drop)# & (df.columns!=drop2) & (df.columns!=drop3)\n",
    "# read file name of data with only label 0\n",
    "df2 = pd.read_csv('./unuseful_data_label.csv',index_col=0)\n",
    "# read some of the data with only label 0\n",
    "df3 = pd.read_csv('./data/file_name.txt',header=None)\n",
    "\n",
    "files = np.concatenate([np.array(df.columns),np.array(df3.loc[:,0])])\n",
    "ind = Data.File.isin(files)\n",
    "ind2 = Data.File == drop\n",
    "#ind = (Data.File != drop) & (Data.File != drop2)\n",
    "Data_sel = Data[ind]\n",
    "Data_rest = Data[ind2]\n",
    "#ind2 = Data_rest.File == drop\n",
    "#Data_rest = Data_rest[ind2]\n",
    "#Data_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_col = ['_IEMG','_MAV','_SSI','_VAR','_RMS',\n",
    "               '_WL','_ZC','_SSC','_WAMP','_skew','_Acti','_AR','_HIST','_MDF']\n",
    "\n",
    "feature_all = Data_sel.iloc[:,1:-1]\n",
    "#ind_temp1 = feature_all.columns.str.contains('_mDWT')\n",
    "#ind_temp2 = feature_all.columns.str.contains('_Coe')\n",
    "#ind_temp3 = feature_all.columns.str.contains('_Scale')\n",
    "#ind_temp4 = feature_all.columns.str.contains('_HIST')\n",
    "#ind_temp = ind_temp1|ind_temp2|ind_temp3|ind_temp4\n",
    "ind_temp = feature_all.columns.str.contains('_Acti')\n",
    "#feature = feature_all.loc[:,~ind_temp]\n",
    "feature = Data_sel.iloc[:,1:-1]\n",
    "#feature = Data_sel.iloc[:,:-2]\n",
    "y = Data_sel.Label\n",
    "feature2_all = Data_rest.iloc[:,1:-1]\n",
    "#feature2 = feature2_all.loc[:,~ind_temp]\n",
    "feature2 = Data_rest.iloc[:,1:-1]\n",
    "#feature2 = Data_rest.iloc[:,:-2]\n",
    "y2 = Data_rest.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = pd.DataFrame()\n",
    "feature2 = pd.DataFrame()\n",
    "y = pd.DataFrame()\n",
    "y2 = pd.DataFrame()\n",
    "m = 0\n",
    "for i in set(Data_sel.File):\n",
    "    ind = Data_sel.File==i\n",
    "    temp = Data_sel[ind]\n",
    "    #if i != 'G08_FoG_2_trial_1_emg.csv':\n",
    "        #feature = pd.concat([feature,temp.iloc[:,1:-1]])\n",
    "        #y = pd.concat([y,temp.iloc[:,0]])\n",
    "    #else:\n",
    "    for j in set(temp.Label):\n",
    "        ind2 = temp.Label == j\n",
    "        temp2 = temp[ind2]\n",
    "        l = len(temp2)\n",
    "        feature = pd.concat([feature,temp2.iloc[:int(0.8*l),1:-1]])\n",
    "        y = pd.concat([y,temp2.iloc[:int(0.8*l),0]])\n",
    "        feature2 = pd.concat([feature2,temp2.iloc[int(0.8*l):,1:-1]])\n",
    "        y2 = pd.concat([y2,temp2.iloc[int(0.8*l):,0]])\n",
    "y = np.array(y)[:,0]\n",
    "y2 = np.array(y2)[:,0]\n",
    "#ind_temp = feature.columns.str.contains('_mDWT')\n",
    "#feature = feature.loc[:,~ind_temp]\n",
    "#feature2 = feature2.loc[:,~ind_temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ind_temp1 = feature_all.columns.str.contains('_MF')\n",
    "#ind_temp2 = feature_all.columns.str.contains('_MDF')\n",
    "#ind_temp3 = feature_all.columns.str.contains('_AR')\n",
    "#ind_temp4 = feature_all.columns.str.contains('_MNF')\n",
    "#ind_temp = ind_temp1|ind_temp2|ind_temp3|ind_temp4\n",
    "#feature = feature.loc[:,~ind_temp]\n",
    "#feature2 = feature2.loc[:,~ind_temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LEFT_TA_IEMG</th>\n",
       "      <th>LEFT_TS_IEMG</th>\n",
       "      <th>LEFT_BF_IEMG</th>\n",
       "      <th>LEFT_RF_IEMG</th>\n",
       "      <th>RIGHT_TA_IEMG</th>\n",
       "      <th>RIGHT_TS_IEMG</th>\n",
       "      <th>RIGHT_BF_IEMG</th>\n",
       "      <th>RIGHT_RF_IEMG</th>\n",
       "      <th>LEFT_TA_SSI</th>\n",
       "      <th>LEFT_TS_SSI</th>\n",
       "      <th>...</th>\n",
       "      <th>RIGHT_BF_mDWT6</th>\n",
       "      <th>RIGHT_BF_mDWT7</th>\n",
       "      <th>RIGHT_RF_mDWT0</th>\n",
       "      <th>RIGHT_RF_mDWT1</th>\n",
       "      <th>RIGHT_RF_mDWT2</th>\n",
       "      <th>RIGHT_RF_mDWT3</th>\n",
       "      <th>RIGHT_RF_mDWT4</th>\n",
       "      <th>RIGHT_RF_mDWT5</th>\n",
       "      <th>RIGHT_RF_mDWT6</th>\n",
       "      <th>RIGHT_RF_mDWT7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14494</th>\n",
       "      <td>54.495407</td>\n",
       "      <td>41.838260</td>\n",
       "      <td>16.685675</td>\n",
       "      <td>119.245790</td>\n",
       "      <td>136.606220</td>\n",
       "      <td>15.870949</td>\n",
       "      <td>57.179630</td>\n",
       "      <td>52.119680</td>\n",
       "      <td>23.876139</td>\n",
       "      <td>13.751545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333921</td>\n",
       "      <td>0.199823</td>\n",
       "      <td>41.699135</td>\n",
       "      <td>27.142443</td>\n",
       "      <td>9.567805</td>\n",
       "      <td>4.691112</td>\n",
       "      <td>1.127269</td>\n",
       "      <td>0.526081</td>\n",
       "      <td>0.310089</td>\n",
       "      <td>0.097274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14495</th>\n",
       "      <td>63.584263</td>\n",
       "      <td>51.623400</td>\n",
       "      <td>17.644413</td>\n",
       "      <td>133.229200</td>\n",
       "      <td>117.331640</td>\n",
       "      <td>19.025179</td>\n",
       "      <td>52.941120</td>\n",
       "      <td>48.827780</td>\n",
       "      <td>29.531832</td>\n",
       "      <td>17.729313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124742</td>\n",
       "      <td>0.074639</td>\n",
       "      <td>46.585384</td>\n",
       "      <td>33.111549</td>\n",
       "      <td>15.584126</td>\n",
       "      <td>8.166800</td>\n",
       "      <td>5.065614</td>\n",
       "      <td>2.192267</td>\n",
       "      <td>1.282910</td>\n",
       "      <td>0.469431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14496</th>\n",
       "      <td>62.807045</td>\n",
       "      <td>62.645560</td>\n",
       "      <td>21.137068</td>\n",
       "      <td>143.969420</td>\n",
       "      <td>103.604040</td>\n",
       "      <td>17.383175</td>\n",
       "      <td>48.306545</td>\n",
       "      <td>37.493343</td>\n",
       "      <td>29.937807</td>\n",
       "      <td>26.072136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.483473</td>\n",
       "      <td>0.196852</td>\n",
       "      <td>33.096977</td>\n",
       "      <td>22.685007</td>\n",
       "      <td>10.285604</td>\n",
       "      <td>4.408403</td>\n",
       "      <td>2.025230</td>\n",
       "      <td>0.828235</td>\n",
       "      <td>0.442531</td>\n",
       "      <td>0.232302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14497</th>\n",
       "      <td>61.515120</td>\n",
       "      <td>47.318073</td>\n",
       "      <td>62.659695</td>\n",
       "      <td>139.217450</td>\n",
       "      <td>89.551180</td>\n",
       "      <td>17.632595</td>\n",
       "      <td>36.355846</td>\n",
       "      <td>30.375315</td>\n",
       "      <td>27.534600</td>\n",
       "      <td>17.827032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486986</td>\n",
       "      <td>0.268595</td>\n",
       "      <td>29.801613</td>\n",
       "      <td>19.772127</td>\n",
       "      <td>11.282631</td>\n",
       "      <td>6.269420</td>\n",
       "      <td>3.268324</td>\n",
       "      <td>1.412688</td>\n",
       "      <td>0.756074</td>\n",
       "      <td>0.323066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14498</th>\n",
       "      <td>48.220585</td>\n",
       "      <td>40.681900</td>\n",
       "      <td>86.668440</td>\n",
       "      <td>140.539630</td>\n",
       "      <td>89.653465</td>\n",
       "      <td>17.877747</td>\n",
       "      <td>28.514355</td>\n",
       "      <td>27.504467</td>\n",
       "      <td>19.109505</td>\n",
       "      <td>15.961396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164356</td>\n",
       "      <td>0.079370</td>\n",
       "      <td>24.088203</td>\n",
       "      <td>14.794522</td>\n",
       "      <td>6.858083</td>\n",
       "      <td>2.881250</td>\n",
       "      <td>1.461718</td>\n",
       "      <td>0.659022</td>\n",
       "      <td>0.342953</td>\n",
       "      <td>0.183987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>47.052890</td>\n",
       "      <td>40.982760</td>\n",
       "      <td>80.379090</td>\n",
       "      <td>71.945190</td>\n",
       "      <td>81.862625</td>\n",
       "      <td>76.749030</td>\n",
       "      <td>32.495710</td>\n",
       "      <td>33.282760</td>\n",
       "      <td>23.372030</td>\n",
       "      <td>19.965347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.564377</td>\n",
       "      <td>0.265130</td>\n",
       "      <td>22.907181</td>\n",
       "      <td>10.707895</td>\n",
       "      <td>5.132853</td>\n",
       "      <td>2.635980</td>\n",
       "      <td>0.775645</td>\n",
       "      <td>0.324570</td>\n",
       "      <td>0.185962</td>\n",
       "      <td>0.092924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>70.285400</td>\n",
       "      <td>24.368923</td>\n",
       "      <td>65.337105</td>\n",
       "      <td>60.989666</td>\n",
       "      <td>83.281130</td>\n",
       "      <td>63.575268</td>\n",
       "      <td>50.984562</td>\n",
       "      <td>69.853836</td>\n",
       "      <td>38.233658</td>\n",
       "      <td>6.818824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089207</td>\n",
       "      <td>0.015426</td>\n",
       "      <td>40.678551</td>\n",
       "      <td>20.532425</td>\n",
       "      <td>12.888038</td>\n",
       "      <td>6.459993</td>\n",
       "      <td>3.246606</td>\n",
       "      <td>1.451732</td>\n",
       "      <td>0.777140</td>\n",
       "      <td>0.384506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>55.515575</td>\n",
       "      <td>26.936937</td>\n",
       "      <td>67.611600</td>\n",
       "      <td>60.874900</td>\n",
       "      <td>90.378430</td>\n",
       "      <td>66.137920</td>\n",
       "      <td>49.313670</td>\n",
       "      <td>68.964874</td>\n",
       "      <td>27.921959</td>\n",
       "      <td>8.506439</td>\n",
       "      <td>...</td>\n",
       "      <td>1.764694</td>\n",
       "      <td>0.877015</td>\n",
       "      <td>33.579285</td>\n",
       "      <td>14.800857</td>\n",
       "      <td>5.733666</td>\n",
       "      <td>2.417968</td>\n",
       "      <td>0.496807</td>\n",
       "      <td>0.214147</td>\n",
       "      <td>0.012236</td>\n",
       "      <td>0.004933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>66.963390</td>\n",
       "      <td>35.998043</td>\n",
       "      <td>109.073320</td>\n",
       "      <td>93.629906</td>\n",
       "      <td>53.432440</td>\n",
       "      <td>57.514908</td>\n",
       "      <td>30.049881</td>\n",
       "      <td>34.337666</td>\n",
       "      <td>38.418472</td>\n",
       "      <td>13.120143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117951</td>\n",
       "      <td>0.049956</td>\n",
       "      <td>22.185177</td>\n",
       "      <td>12.946556</td>\n",
       "      <td>7.217345</td>\n",
       "      <td>4.386374</td>\n",
       "      <td>2.870004</td>\n",
       "      <td>1.276046</td>\n",
       "      <td>0.617854</td>\n",
       "      <td>0.308553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>65.629370</td>\n",
       "      <td>36.537624</td>\n",
       "      <td>99.552920</td>\n",
       "      <td>101.532710</td>\n",
       "      <td>58.082504</td>\n",
       "      <td>53.767010</td>\n",
       "      <td>26.868986</td>\n",
       "      <td>34.828250</td>\n",
       "      <td>37.430584</td>\n",
       "      <td>12.560530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154663</td>\n",
       "      <td>0.096511</td>\n",
       "      <td>21.927574</td>\n",
       "      <td>12.263198</td>\n",
       "      <td>4.329652</td>\n",
       "      <td>1.174351</td>\n",
       "      <td>0.417749</td>\n",
       "      <td>0.181872</td>\n",
       "      <td>0.084832</td>\n",
       "      <td>0.045866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3244 rows × 232 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       LEFT_TA_IEMG  LEFT_TS_IEMG  LEFT_BF_IEMG  LEFT_RF_IEMG  RIGHT_TA_IEMG  \\\n",
       "14494     54.495407     41.838260     16.685675    119.245790     136.606220   \n",
       "14495     63.584263     51.623400     17.644413    133.229200     117.331640   \n",
       "14496     62.807045     62.645560     21.137068    143.969420     103.604040   \n",
       "14497     61.515120     47.318073     62.659695    139.217450      89.551180   \n",
       "14498     48.220585     40.681900     86.668440    140.539630      89.653465   \n",
       "...             ...           ...           ...           ...            ...   \n",
       "87        47.052890     40.982760     80.379090     71.945190      81.862625   \n",
       "84        70.285400     24.368923     65.337105     60.989666      83.281130   \n",
       "85        55.515575     26.936937     67.611600     60.874900      90.378430   \n",
       "88        66.963390     35.998043    109.073320     93.629906      53.432440   \n",
       "89        65.629370     36.537624     99.552920    101.532710      58.082504   \n",
       "\n",
       "       RIGHT_TS_IEMG  RIGHT_BF_IEMG  RIGHT_RF_IEMG  LEFT_TA_SSI  LEFT_TS_SSI  \\\n",
       "14494      15.870949      57.179630      52.119680    23.876139    13.751545   \n",
       "14495      19.025179      52.941120      48.827780    29.531832    17.729313   \n",
       "14496      17.383175      48.306545      37.493343    29.937807    26.072136   \n",
       "14497      17.632595      36.355846      30.375315    27.534600    17.827032   \n",
       "14498      17.877747      28.514355      27.504467    19.109505    15.961396   \n",
       "...              ...            ...            ...          ...          ...   \n",
       "87         76.749030      32.495710      33.282760    23.372030    19.965347   \n",
       "84         63.575268      50.984562      69.853836    38.233658     6.818824   \n",
       "85         66.137920      49.313670      68.964874    27.921959     8.506439   \n",
       "88         57.514908      30.049881      34.337666    38.418472    13.120143   \n",
       "89         53.767010      26.868986      34.828250    37.430584    12.560530   \n",
       "\n",
       "       ...  RIGHT_BF_mDWT6  RIGHT_BF_mDWT7  RIGHT_RF_mDWT0  RIGHT_RF_mDWT1  \\\n",
       "14494  ...        0.333921        0.199823       41.699135       27.142443   \n",
       "14495  ...        0.124742        0.074639       46.585384       33.111549   \n",
       "14496  ...        0.483473        0.196852       33.096977       22.685007   \n",
       "14497  ...        0.486986        0.268595       29.801613       19.772127   \n",
       "14498  ...        0.164356        0.079370       24.088203       14.794522   \n",
       "...    ...             ...             ...             ...             ...   \n",
       "87     ...        0.564377        0.265130       22.907181       10.707895   \n",
       "84     ...        0.089207        0.015426       40.678551       20.532425   \n",
       "85     ...        1.764694        0.877015       33.579285       14.800857   \n",
       "88     ...        0.117951        0.049956       22.185177       12.946556   \n",
       "89     ...        0.154663        0.096511       21.927574       12.263198   \n",
       "\n",
       "       RIGHT_RF_mDWT2  RIGHT_RF_mDWT3  RIGHT_RF_mDWT4  RIGHT_RF_mDWT5  \\\n",
       "14494        9.567805        4.691112        1.127269        0.526081   \n",
       "14495       15.584126        8.166800        5.065614        2.192267   \n",
       "14496       10.285604        4.408403        2.025230        0.828235   \n",
       "14497       11.282631        6.269420        3.268324        1.412688   \n",
       "14498        6.858083        2.881250        1.461718        0.659022   \n",
       "...               ...             ...             ...             ...   \n",
       "87           5.132853        2.635980        0.775645        0.324570   \n",
       "84          12.888038        6.459993        3.246606        1.451732   \n",
       "85           5.733666        2.417968        0.496807        0.214147   \n",
       "88           7.217345        4.386374        2.870004        1.276046   \n",
       "89           4.329652        1.174351        0.417749        0.181872   \n",
       "\n",
       "       RIGHT_RF_mDWT6  RIGHT_RF_mDWT7  \n",
       "14494        0.310089        0.097274  \n",
       "14495        1.282910        0.469431  \n",
       "14496        0.442531        0.232302  \n",
       "14497        0.756074        0.323066  \n",
       "14498        0.342953        0.183987  \n",
       "...               ...             ...  \n",
       "87           0.185962        0.092924  \n",
       "84           0.777140        0.384506  \n",
       "85           0.012236        0.004933  \n",
       "88           0.617854        0.308553  \n",
       "89           0.084832        0.045866  \n",
       "\n",
       "[3244 rows x 232 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './processed data/data_set_after_window_withoutSC.hdf5'\n",
    "with h5py.File(path,'r') as f:\n",
    "    x = f['cwt_data'][...]\n",
    "    y = f['label2'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0bf126e2acfb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dp' is not defined"
     ]
    }
   ],
   "source": [
    "import imp\n",
    "imp.reload(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ctypes\n",
    "player = ctypes.windll.kernel32\n",
    "player.Beep(1000,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Data_rest.File=='P812_M050_2_B_FoG_trial_1_emg.csv').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 256\n",
    "threshold_WAMP = 30\n",
    "threshold_ZC = 0\n",
    "threshold_SSC = 1\n",
    "bins=9\n",
    "bound = 70\n",
    "HIST_range = (-bound,bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './processed data/data_set_after_window_W128_S64_sameLabel_rect.hdf5'\n",
    "with h5py.File(path,'r') as f:\n",
    "    x = f['data'][...]\n",
    "    y = f['label2'][...]\n",
    "#feature = dp.generate_feature(x,threshold_WAMP=threshold_WAMP,\n",
    "#                              threshold_ZC=threshold_ZC,\n",
    "#                              threshold_SSC=threshold_SSC,\n",
    "#                              bins=bins,ranges=HIST_range)\n",
    "#feature2 = dp.generate_feature(x2)\n",
    "player.Beep(1000,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./processed data/nfeatures_W256_S64_WAMP30.hdf5','r') as f:\n",
    "    feature = f['features'][...]\n",
    "    y = f['labels'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature2,y2 = dp.pipeline_feature(path2,width=256,stride=64,\n",
    "                                  scaler=False,\n",
    "                                  threshold_WAMP=threshold_WAMP,\n",
    "                                  threshold_ZC=threshold_ZC,\n",
    "                                  threshold_SSC=threshold_SSC,\n",
    "                                  bins=bins,ranges=HIST_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./processed data/nfeatures_rest_W256_S64.hdf5','r') as f:\n",
    "    feature2 = f['features'][...]\n",
    "    y2 = f['labels'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler(with_mean=True)\n",
    "feature_sc = sc.fit_transform(feature)\n",
    "feature2_sc = sc.transform(feature2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,Model,callbacks,regularizers,models\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder,normalize,MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE,BorderlineSMOTE,ADASYN,SVMSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,feature,y,binary=True,file=None):\n",
    "    if binary:\n",
    "        ind = ((y==0)|(y==1)|(y==2)|(y==6))\n",
    "        #ind1 = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "        y_01 = y[ind].copy()\n",
    "        ind1 = ((y_01==1)|(y_01==2)|(y_01==6))\n",
    "        y_01[ind1] = 1\n",
    "        oc = OneHotEncoder()\n",
    "        y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "        cw = None#{0:1,1:5}\n",
    "    else:\n",
    "        ind = ((y==1)|(y==2)|(y==6))\n",
    "        #ind = ((y==1)|(y==6))\n",
    "        y_01 = y[ind].copy()\n",
    "        oc = OneHotEncoder()\n",
    "        y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "        cw = None#{0:5,1:1,2:1}#{0:2,1:1,2:10,3:2}\n",
    "    x_full,x_test,y_full,y_test = train_test_split(np.array(feature)[ind,:],y_01,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=123,\n",
    "                                                   shuffle=True)\n",
    "    x_train,x_valid,y_train,y_valid = train_test_split(x_full,y_full,\n",
    "                                                       test_size=0.25,\n",
    "                                                       random_state=555,\n",
    "                                                       shuffle=True)\n",
    "    \n",
    "    #sm = BorderlineSMOTE(random_state=50,kind='borderline-2')\n",
    "    #sm = SMOTE(random_state=50)\n",
    "    #print(y_full.shape)\n",
    "    #x_full,y_full = sm.fit_resample(x_full,y_full)\n",
    "    #print(y_full_n.shape)\n",
    "    sc = StandardScaler(with_mean=True)\n",
    "    #sc = MinMaxScaler()\n",
    "    x_train = sc.fit_transform(x_full)\n",
    "    pca = PCA(n_components=100)\n",
    "    #x_train = pca.fit_transform(x_train)\n",
    "    #x_valid = sc.transform(x_valid)\n",
    "    x_test = sc.transform(x_test)\n",
    "    #x_test = pca.transform(x_test)\n",
    "    #x_train = x_full\n",
    "    \n",
    "    early_stopping = callbacks.EarlyStopping(patience = 20,\n",
    "                                             monitor = 'val_loss', \n",
    "                                             restore_best_weights=True)\n",
    "    history = model.fit(x_train,y_full,validation_data=(x_test,y_test),\n",
    "                        epochs=300,batch_size=32,class_weight=cw,\n",
    "                        callbacks=[early_stopping],\n",
    "                        shuffle=True)\n",
    "    \n",
    "    if binary:\n",
    "        y_pred_t=model.predict(x_test)\n",
    "        test = metrics.accuracy_score(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1))\n",
    "        #test = metrics.accuracy_score(y_test,y_pred_t>0.5)\n",
    "        \n",
    "        #y_pred_v=model.predict(x_valid)\n",
    "        #valid = metrics.accuracy_score(y_valid,np.argmax(y_pred_v,axis=1))\n",
    "        y_pred_ta=model.predict(x_train)\n",
    "        train = metrics.accuracy_score(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1))\n",
    "        #train = metrics.accuracy_score(y_full,y_pred_ta>0.5)\n",
    "        \n",
    "        print('train: \\n',metrics.confusion_matrix(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1)))\n",
    "        #print('valid: \\n',metrics.confusion_matrix(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1)))\n",
    "        print('test: \\n',metrics.confusion_matrix(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1)))\n",
    "        \n",
    "        #print('train: \\n',metrics.confusion_matrix(y_full,y_pred_ta>0.5))\n",
    "        #print('test: \\n',metrics.confusion_matrix(y_test,y_pred_t>0.5))\n",
    "\n",
    "    else:\n",
    "        y_pred_t=model.predict(x_test)\n",
    "        test = metrics.accuracy_score(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1))\n",
    "        #y_pred_v=model.predict(x_valid)\n",
    "        #valid = metrics.accuracy_score(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1))\n",
    "        y_pred_ta=model.predict(x_train)\n",
    "        train = metrics.accuracy_score(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1))        \n",
    "        print('train: \\n',metrics.confusion_matrix(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1)))\n",
    "        #print('valid: \\n',metrics.confusion_matrix(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1)))\n",
    "        print('test: \\n',metrics.confusion_matrix(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1)))\n",
    "    print('test:%f'%test)\n",
    "    #print('valid:%f'%valid)\n",
    "    print('train:%f'%train)\n",
    "    if file != None:\n",
    "        model.save_model(file)\n",
    "    return train,test,sc,pca\n",
    "\n",
    "def test_model(model,feature,y,sc,pca,binary=True):\n",
    "    if binary:\n",
    "        ind = ((y==0)|(y==1)|(y==2)|(y==3)|(y==4)|(y==6))\n",
    "        #ind1 = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "        y_01 = y[ind].copy()\n",
    "        ind1 = ((y_01==1)|(y_01==2)|(y_01==6))\n",
    "        y_01[ind1] = 1\n",
    "        oc = OneHotEncoder()\n",
    "        y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "        cw = None#{0:1,1:5}\n",
    "    else:\n",
    "        ind = ((y==1)|(y==2)|(y==6))\n",
    "        #ind = ((y==1)|(y==6))\n",
    "        y_01 = y[ind].copy()\n",
    "        oc = OneHotEncoder()\n",
    "        y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "        cw = None#{0:5,1:1,2:1}#{0:2,1:1,2:10,3:2}\n",
    "\n",
    "    #print(y_01)\n",
    "    feature=sc.transform(feature[ind])\n",
    "    #feature = feature[ind]\n",
    "    #feature=pca.transform(feature)\n",
    "    y_pred=model.predict(feature)\n",
    "    test = metrics.accuracy_score(np.argmax(y_01,axis=1),np.argmax(y_pred,axis=1))\n",
    "    #test = metrics.accuracy_score(y_01,y_pred>0.5)\n",
    "    \n",
    "    print('acc:%f'%test)\n",
    "    print(metrics.confusion_matrix(np.argmax(y_01,axis=1),np.argmax(y_pred,axis=1)))\n",
    "    #print(metrics.confusion_matrix(y_01,y_pred>0.5))\n",
    "    return test\n",
    "\n",
    "def sparse_cost_sensitive_loss (y_true,y_pred):\n",
    "    #cost_matrix = tf.constant([[0,1.,1,1.],\n",
    "    #              [2,0,5,5],\n",
    "    #              [1,1,0,1],\n",
    "    #              [1.,2.,1,0]])\n",
    "    cost_matrix = tf.constant([[0,2.,2],\n",
    "                  [1,0,1],\n",
    "                  [1.0,1.,0]])\n",
    "    #cost_matrix = tf.constant([[0,1.],\n",
    "    #              [5.,0]])\n",
    "    batch_cost_matrix = tf.nn.embedding_lookup(cost_matrix, tf.argmax(y_true,axis=1))\n",
    "    eps = 1e-6\n",
    "    probability = tf.clip_by_value(y_pred, eps, 1-eps)\n",
    "    cost_values = tf.math.log(1-probability)*batch_cost_matrix\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(cost_values, axis=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = ((y==1)|(y==2)|(y==3)|(y==6))\n",
    "#ind = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "ind_f = [0,1,6,42,46,57,62]\n",
    "#y_01 = y[ind].copy()\n",
    "#y_01[y_01==1]=0\n",
    "#y_01[y_01==2]=1\n",
    "#y_01[y_01==3]=2\n",
    "#y_01[y_01==6]=3\n",
    "y_01 = y[ind].copy()\n",
    "#y_01[ind] = 1\n",
    "oh_ec = OneHotEncoder()\n",
    "y_oh = oh_ec.fit_transform(y_01[:,np.newaxis]).toarray()\n",
    "x_full,x_test,y_full,y_test = train_test_split(feature.loc[ind,:],y_01,test_size=0.2,random_state=123,shuffle=False)\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(x_full,y_full,test_size=0.2,random_state=555,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = layers.Input(shape=feature.shape[1:])#feature.shape[1:]\n",
    "l1 = layers.Dense(128,activation='elu',\n",
    "                  #kernel_initializer='lecun_normal',\n",
    "                  #kernel_regularizer = regularizers.l2(0.001),\n",
    "                 )(input_)\n",
    "#drop1 = layers.Dropout(0.2)(l1)\n",
    "bn1 = layers.BatchNormalization()(l1)\n",
    "l2 = layers.Dense(64,activation='elu',\n",
    "                  #kernel_initializer='lecun_normal',\n",
    "                  #kernel_regularizer = regularizers.l2(0.001),\n",
    "                 )(bn1)\n",
    "#drop2 = layers.Dropout(0.2)(l2)\n",
    "bn2 = layers.BatchNormalization()(l2)\n",
    "l3 = layers.Dense(32,activation='elu',\n",
    "                  #kernel_initializer='lecun_normal',\n",
    "                  #kernel_regularizer = regularizers.l2(0.001),\n",
    "                 )(bn2)\n",
    "#drop3 = layers.Dropout(0.2)(l3)\n",
    "bn3 = layers.BatchNormalization()(l3)\n",
    "l4 = layers.Dense(16,activation='elu',\n",
    "                 #kernel_regularizer = regularizers.l2(0.001),\n",
    "                 )(bn3)\n",
    "#drop4 = layers.Dropout(0.2)(l4)\n",
    "#l5 = layers.Dense(8,activation='relu')(drop4)\n",
    "#drop5 = layers.Dropout(0.5)(l5)\n",
    "output = layers.Dense(3,activation='softmax')(l4)\n",
    "model = Model(inputs=[input_],outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(128,#activation='elu',\n",
    "                  #kernel_initializer='lecun_normal',\n",
    "                  #kernel_regularizer = regularizers.l2(0.001),\n",
    "                       #use_bias=False\n",
    "                 ))\n",
    "#model.add(layers.Dropout(0.2))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('selu'))\n",
    "model.add(layers.Dense(64,#activation='elu',\n",
    "                  #kernel_initializer='lecun_normal',\n",
    "                  #kernel_regularizer = regularizers.l2(0.001),\n",
    "                      # use_bias=False\n",
    "                 ))\n",
    "#model.add(layers.Dropout(0.2))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('selu'))\n",
    "model.add(layers.Dense(32,#activation='elu',\n",
    "                  #kernel_initializer='lecun_normal',\n",
    "                  #kernel_regularizer = regularizers.l2(0.001),\n",
    "                       #use_bias=False\n",
    "                 ))\n",
    "#model.add(layers.Dropout(0.2))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('selu'))\n",
    "model.add(layers.Dense(16,#activation='elu',\n",
    "                 #kernel_regularizer = regularizers.l2(0.001),\n",
    "                       #use_bias=False\n",
    "                 ))\n",
    "#model.add(layers.Dropout(0.2))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(3,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "213/213 [==============================] - 2s 8ms/step - loss: 0.6574 - accuracy: 0.7215 - val_loss: 0.5657 - val_accuracy: 0.7670\n",
      "Epoch 2/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.5432 - accuracy: 0.7757 - val_loss: 0.5497 - val_accuracy: 0.7705\n",
      "Epoch 3/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.5064 - accuracy: 0.7889 - val_loss: 0.5070 - val_accuracy: 0.7811\n",
      "Epoch 4/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.4665 - accuracy: 0.8049 - val_loss: 0.5013 - val_accuracy: 0.7881\n",
      "Epoch 5/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.4479 - accuracy: 0.8131 - val_loss: 0.4648 - val_accuracy: 0.8081\n",
      "Epoch 6/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.4215 - accuracy: 0.8224 - val_loss: 0.4489 - val_accuracy: 0.8151\n",
      "Epoch 7/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.4036 - accuracy: 0.8321 - val_loss: 0.4397 - val_accuracy: 0.8140\n",
      "Epoch 8/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.3730 - accuracy: 0.8441 - val_loss: 0.4132 - val_accuracy: 0.8263\n",
      "Epoch 9/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.3496 - accuracy: 0.8588 - val_loss: 0.4586 - val_accuracy: 0.8075\n",
      "Epoch 10/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.3375 - accuracy: 0.8620 - val_loss: 0.3892 - val_accuracy: 0.8316\n",
      "Epoch 11/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.3074 - accuracy: 0.8711 - val_loss: 0.4125 - val_accuracy: 0.8316\n",
      "Epoch 12/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.2901 - accuracy: 0.8849 - val_loss: 0.3580 - val_accuracy: 0.8592\n",
      "Epoch 13/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.2791 - accuracy: 0.8903 - val_loss: 0.3991 - val_accuracy: 0.8509\n",
      "Epoch 14/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.2657 - accuracy: 0.8946 - val_loss: 0.3619 - val_accuracy: 0.8574\n",
      "Epoch 15/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.2422 - accuracy: 0.9059 - val_loss: 0.3515 - val_accuracy: 0.8597\n",
      "Epoch 16/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.2487 - accuracy: 0.9037 - val_loss: 0.3429 - val_accuracy: 0.8756\n",
      "Epoch 17/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.2380 - accuracy: 0.9058 - val_loss: 0.3666 - val_accuracy: 0.8638\n",
      "Epoch 18/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.2305 - accuracy: 0.9106 - val_loss: 0.4268 - val_accuracy: 0.8439\n",
      "Epoch 19/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.2092 - accuracy: 0.9176 - val_loss: 0.3562 - val_accuracy: 0.8650\n",
      "Epoch 20/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.2030 - accuracy: 0.9201 - val_loss: 0.3340 - val_accuracy: 0.8721\n",
      "Epoch 21/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.1881 - accuracy: 0.9289 - val_loss: 0.3480 - val_accuracy: 0.8703\n",
      "Epoch 22/300\n",
      "213/213 [==============================] - 1s 7ms/step - loss: 0.1754 - accuracy: 0.9322 - val_loss: 0.3452 - val_accuracy: 0.8732\n",
      "Epoch 23/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.1746 - accuracy: 0.9335 - val_loss: 0.2952 - val_accuracy: 0.8991\n",
      "Epoch 24/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.1613 - accuracy: 0.9382 - val_loss: 0.3524 - val_accuracy: 0.8850\n",
      "Epoch 25/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.1580 - accuracy: 0.9414 - val_loss: 0.3262 - val_accuracy: 0.8779\n",
      "Epoch 26/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.1489 - accuracy: 0.9450 - val_loss: 0.3276 - val_accuracy: 0.8797\n",
      "Epoch 27/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.1542 - accuracy: 0.9388 - val_loss: 0.3099 - val_accuracy: 0.8897\n",
      "Epoch 28/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.1418 - accuracy: 0.9458 - val_loss: 0.2998 - val_accuracy: 0.8885\n",
      "Epoch 29/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.1351 - accuracy: 0.9483 - val_loss: 0.3248 - val_accuracy: 0.8914\n",
      "Epoch 30/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.1322 - accuracy: 0.9494 - val_loss: 0.3042 - val_accuracy: 0.8944\n",
      "Epoch 31/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.1270 - accuracy: 0.9491 - val_loss: 0.3077 - val_accuracy: 0.8926\n",
      "Epoch 32/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.1110 - accuracy: 0.9565 - val_loss: 0.3104 - val_accuracy: 0.8932\n",
      "Epoch 33/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.1281 - accuracy: 0.9524 - val_loss: 0.3161 - val_accuracy: 0.8973\n",
      "Epoch 34/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.1330 - accuracy: 0.9499 - val_loss: 0.2982 - val_accuracy: 0.8955\n",
      "Epoch 35/300\n",
      "213/213 [==============================] - 1s 7ms/step - loss: 0.1245 - accuracy: 0.9542 - val_loss: 0.3102 - val_accuracy: 0.9026\n",
      "Epoch 36/300\n",
      "213/213 [==============================] - 1s 7ms/step - loss: 0.1158 - accuracy: 0.9576 - val_loss: 0.3171 - val_accuracy: 0.8897\n",
      "Epoch 37/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.1012 - accuracy: 0.9640 - val_loss: 0.3135 - val_accuracy: 0.8938\n",
      "Epoch 38/300\n",
      "213/213 [==============================] - 2s 8ms/step - loss: 0.1155 - accuracy: 0.9586 - val_loss: 0.3050 - val_accuracy: 0.8961\n",
      "Epoch 39/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.1144 - accuracy: 0.9595 - val_loss: 0.3483 - val_accuracy: 0.8920\n",
      "Epoch 40/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0961 - accuracy: 0.9624 - val_loss: 0.3270 - val_accuracy: 0.8944\n",
      "Epoch 41/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0997 - accuracy: 0.9615 - val_loss: 0.2999 - val_accuracy: 0.8991\n",
      "Epoch 42/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.1042 - accuracy: 0.9598 - val_loss: 0.3207 - val_accuracy: 0.8961\n",
      "Epoch 43/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0875 - accuracy: 0.9680 - val_loss: 0.2810 - val_accuracy: 0.9108\n",
      "Epoch 44/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0839 - accuracy: 0.9718 - val_loss: 0.2865 - val_accuracy: 0.9131\n",
      "Epoch 45/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0955 - accuracy: 0.9627 - val_loss: 0.3019 - val_accuracy: 0.9026\n",
      "Epoch 46/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.1041 - accuracy: 0.9615 - val_loss: 0.3269 - val_accuracy: 0.9026\n",
      "Epoch 47/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0859 - accuracy: 0.9671 - val_loss: 0.3160 - val_accuracy: 0.8979\n",
      "Epoch 48/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.1046 - accuracy: 0.9624 - val_loss: 0.2921 - val_accuracy: 0.9096\n",
      "Epoch 49/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0786 - accuracy: 0.9728 - val_loss: 0.3501 - val_accuracy: 0.8979\n",
      "Epoch 50/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0892 - accuracy: 0.9674 - val_loss: 0.2932 - val_accuracy: 0.9079\n",
      "Epoch 51/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0865 - accuracy: 0.9687 - val_loss: 0.2867 - val_accuracy: 0.9026\n",
      "Epoch 52/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0784 - accuracy: 0.9730 - val_loss: 0.2740 - val_accuracy: 0.9073\n",
      "Epoch 53/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0647 - accuracy: 0.9775 - val_loss: 0.2977 - val_accuracy: 0.8991\n",
      "Epoch 54/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0719 - accuracy: 0.9740 - val_loss: 0.3156 - val_accuracy: 0.9043\n",
      "Epoch 55/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0923 - accuracy: 0.9674 - val_loss: 0.3224 - val_accuracy: 0.9020\n",
      "Epoch 56/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0718 - accuracy: 0.9749 - val_loss: 0.2690 - val_accuracy: 0.9155\n",
      "Epoch 57/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0661 - accuracy: 0.9775 - val_loss: 0.2611 - val_accuracy: 0.9143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0852 - accuracy: 0.9686 - val_loss: 0.2813 - val_accuracy: 0.9137\n",
      "Epoch 59/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0707 - accuracy: 0.9745 - val_loss: 0.3250 - val_accuracy: 0.8991\n",
      "Epoch 60/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0873 - accuracy: 0.9690 - val_loss: 0.2886 - val_accuracy: 0.9126\n",
      "Epoch 61/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0689 - accuracy: 0.9756 - val_loss: 0.2624 - val_accuracy: 0.9102\n",
      "Epoch 62/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.0605 - accuracy: 0.9772 - val_loss: 0.2778 - val_accuracy: 0.9196\n",
      "Epoch 63/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.0700 - accuracy: 0.9737 - val_loss: 0.2669 - val_accuracy: 0.9108\n",
      "Epoch 64/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.0775 - accuracy: 0.9718 - val_loss: 0.2584 - val_accuracy: 0.9231\n",
      "Epoch 65/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0522 - accuracy: 0.9819 - val_loss: 0.2835 - val_accuracy: 0.9126\n",
      "Epoch 66/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0628 - accuracy: 0.9777 - val_loss: 0.3044 - val_accuracy: 0.9131\n",
      "Epoch 67/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.0697 - accuracy: 0.9750 - val_loss: 0.2938 - val_accuracy: 0.9178\n",
      "Epoch 68/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.0673 - accuracy: 0.9749 - val_loss: 0.2499 - val_accuracy: 0.9155\n",
      "Epoch 69/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.0501 - accuracy: 0.9824 - val_loss: 0.2752 - val_accuracy: 0.9149\n",
      "Epoch 70/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0643 - accuracy: 0.9756 - val_loss: 0.3427 - val_accuracy: 0.9032\n",
      "Epoch 71/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.0563 - accuracy: 0.9784 - val_loss: 0.2757 - val_accuracy: 0.9208\n",
      "Epoch 72/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.0478 - accuracy: 0.9817 - val_loss: 0.2604 - val_accuracy: 0.9225\n",
      "Epoch 73/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.0607 - accuracy: 0.9764 - val_loss: 0.2782 - val_accuracy: 0.9208\n",
      "Epoch 74/300\n",
      "213/213 [==============================] - 2s 8ms/step - loss: 0.0549 - accuracy: 0.9818 - val_loss: 0.3361 - val_accuracy: 0.9002\n",
      "Epoch 75/300\n",
      "213/213 [==============================] - 2s 8ms/step - loss: 0.0572 - accuracy: 0.9786 - val_loss: 0.2707 - val_accuracy: 0.9190\n",
      "Epoch 76/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.0737 - accuracy: 0.9733 - val_loss: 0.2918 - val_accuracy: 0.9219\n",
      "Epoch 77/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.0601 - accuracy: 0.9777 - val_loss: 0.2536 - val_accuracy: 0.9249\n",
      "Epoch 78/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.0533 - accuracy: 0.9821 - val_loss: 0.2685 - val_accuracy: 0.9237\n",
      "Epoch 79/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.0525 - accuracy: 0.9815 - val_loss: 0.2754 - val_accuracy: 0.9302\n",
      "Epoch 80/300\n",
      "213/213 [==============================] - 1s 7ms/step - loss: 0.0521 - accuracy: 0.9811 - val_loss: 0.2651 - val_accuracy: 0.9231\n",
      "Epoch 81/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.0508 - accuracy: 0.9815 - val_loss: 0.2940 - val_accuracy: 0.9196\n",
      "Epoch 82/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.0458 - accuracy: 0.9839 - val_loss: 0.2775 - val_accuracy: 0.9202\n",
      "Epoch 83/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.0503 - accuracy: 0.9815 - val_loss: 0.2968 - val_accuracy: 0.9178\n",
      "Epoch 84/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0557 - accuracy: 0.9803 - val_loss: 0.2540 - val_accuracy: 0.9184\n",
      "Epoch 85/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0405 - accuracy: 0.9858 - val_loss: 0.2697 - val_accuracy: 0.9190\n",
      "Epoch 86/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0497 - accuracy: 0.9822 - val_loss: 0.2886 - val_accuracy: 0.9173\n",
      "Epoch 87/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0597 - accuracy: 0.9794 - val_loss: 0.2834 - val_accuracy: 0.9196\n",
      "Epoch 88/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0497 - accuracy: 0.9841 - val_loss: 0.3025 - val_accuracy: 0.9161\n",
      "Epoch 89/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0462 - accuracy: 0.9840 - val_loss: 0.2934 - val_accuracy: 0.9190\n",
      "Epoch 90/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0546 - accuracy: 0.9806 - val_loss: 0.2721 - val_accuracy: 0.9167\n",
      "Epoch 91/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0410 - accuracy: 0.9861 - val_loss: 0.2443 - val_accuracy: 0.9319\n",
      "Epoch 92/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0457 - accuracy: 0.9843 - val_loss: 0.2925 - val_accuracy: 0.9231\n",
      "Epoch 93/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0534 - accuracy: 0.9805 - val_loss: 0.2389 - val_accuracy: 0.9296\n",
      "Epoch 94/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0385 - accuracy: 0.9858 - val_loss: 0.3216 - val_accuracy: 0.9161\n",
      "Epoch 95/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0512 - accuracy: 0.9836 - val_loss: 0.2526 - val_accuracy: 0.9325\n",
      "Epoch 96/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0441 - accuracy: 0.9844 - val_loss: 0.2603 - val_accuracy: 0.9366\n",
      "Epoch 97/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0454 - accuracy: 0.9853 - val_loss: 0.2782 - val_accuracy: 0.9231\n",
      "Epoch 98/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0447 - accuracy: 0.9852 - val_loss: 0.2814 - val_accuracy: 0.9196\n",
      "Epoch 99/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0568 - accuracy: 0.9796 - val_loss: 0.2746 - val_accuracy: 0.9249\n",
      "Epoch 100/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0547 - accuracy: 0.9806 - val_loss: 0.3442 - val_accuracy: 0.9108\n",
      "Epoch 101/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0383 - accuracy: 0.9869 - val_loss: 0.2883 - val_accuracy: 0.9243\n",
      "Epoch 102/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0451 - accuracy: 0.9824 - val_loss: 0.2729 - val_accuracy: 0.9272\n",
      "Epoch 103/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0452 - accuracy: 0.9828 - val_loss: 0.2455 - val_accuracy: 0.9354\n",
      "Epoch 104/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0416 - accuracy: 0.9856 - val_loss: 0.2505 - val_accuracy: 0.9261\n",
      "Epoch 105/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0378 - accuracy: 0.9874 - val_loss: 0.2182 - val_accuracy: 0.9325\n",
      "Epoch 106/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0238 - accuracy: 0.9919 - val_loss: 0.2259 - val_accuracy: 0.9366\n",
      "Epoch 107/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0473 - accuracy: 0.9840 - val_loss: 0.3029 - val_accuracy: 0.9108\n",
      "Epoch 108/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0502 - accuracy: 0.9825 - val_loss: 0.2722 - val_accuracy: 0.9219\n",
      "Epoch 109/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.0606 - accuracy: 0.9771 - val_loss: 0.2787 - val_accuracy: 0.9202\n",
      "Epoch 110/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0411 - accuracy: 0.9843 - val_loss: 0.2670 - val_accuracy: 0.9237\n",
      "Epoch 111/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0367 - accuracy: 0.9866 - val_loss: 0.2315 - val_accuracy: 0.9331\n",
      "Epoch 112/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0430 - accuracy: 0.9855 - val_loss: 0.2776 - val_accuracy: 0.9167\n",
      "Epoch 113/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0463 - accuracy: 0.9850 - val_loss: 0.2192 - val_accuracy: 0.9360\n",
      "Epoch 114/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0411 - accuracy: 0.9863 - val_loss: 0.2680 - val_accuracy: 0.9196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0310 - accuracy: 0.9890 - val_loss: 0.2914 - val_accuracy: 0.9167\n",
      "Epoch 116/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0436 - accuracy: 0.9861 - val_loss: 0.2349 - val_accuracy: 0.9313\n",
      "Epoch 117/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0427 - accuracy: 0.9844 - val_loss: 0.2484 - val_accuracy: 0.9337\n",
      "Epoch 118/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.0413 - accuracy: 0.9856 - val_loss: 0.2535 - val_accuracy: 0.9261\n",
      "Epoch 119/300\n",
      "213/213 [==============================] - 1s 6ms/step - loss: 0.0390 - accuracy: 0.9843 - val_loss: 0.2576 - val_accuracy: 0.9308\n",
      "Epoch 120/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0403 - accuracy: 0.9849 - val_loss: 0.2599 - val_accuracy: 0.9343\n",
      "Epoch 121/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0346 - accuracy: 0.9869 - val_loss: 0.2549 - val_accuracy: 0.9302\n",
      "Epoch 122/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0383 - accuracy: 0.9885 - val_loss: 0.2413 - val_accuracy: 0.9337\n",
      "Epoch 123/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0417 - accuracy: 0.9862 - val_loss: 0.3032 - val_accuracy: 0.9184\n",
      "Epoch 124/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0363 - accuracy: 0.9869 - val_loss: 0.2295 - val_accuracy: 0.9396\n",
      "Epoch 125/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0387 - accuracy: 0.9861 - val_loss: 0.2389 - val_accuracy: 0.9349\n",
      "Epoch 126/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0364 - accuracy: 0.9865 - val_loss: 0.2606 - val_accuracy: 0.9272\n",
      "Epoch 127/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0350 - accuracy: 0.9875 - val_loss: 0.3021 - val_accuracy: 0.9255\n",
      "Epoch 128/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0435 - accuracy: 0.9861 - val_loss: 0.2454 - val_accuracy: 0.9284\n",
      "Epoch 129/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0414 - accuracy: 0.9868 - val_loss: 0.3039 - val_accuracy: 0.9108\n",
      "Epoch 130/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0387 - accuracy: 0.9868 - val_loss: 0.2470 - val_accuracy: 0.9272\n",
      "Epoch 131/300\n",
      "213/213 [==============================] - 1s 5ms/step - loss: 0.0379 - accuracy: 0.9865 - val_loss: 0.2248 - val_accuracy: 0.9419\n",
      "Epoch 132/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0269 - accuracy: 0.9915 - val_loss: 0.2202 - val_accuracy: 0.9384\n",
      "Epoch 133/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0341 - accuracy: 0.9884 - val_loss: 0.2951 - val_accuracy: 0.9266\n",
      "Epoch 134/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0349 - accuracy: 0.9883 - val_loss: 0.2217 - val_accuracy: 0.9378\n",
      "Epoch 135/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0305 - accuracy: 0.9897 - val_loss: 0.2142 - val_accuracy: 0.9437\n",
      "Epoch 136/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0263 - accuracy: 0.9899 - val_loss: 0.2365 - val_accuracy: 0.9354\n",
      "Epoch 137/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0292 - accuracy: 0.9888 - val_loss: 0.2383 - val_accuracy: 0.9378\n",
      "Epoch 138/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0396 - accuracy: 0.9862 - val_loss: 0.2484 - val_accuracy: 0.9284\n",
      "Epoch 139/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0395 - accuracy: 0.9865 - val_loss: 0.2131 - val_accuracy: 0.9437\n",
      "Epoch 140/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0267 - accuracy: 0.9916 - val_loss: 0.2281 - val_accuracy: 0.9372\n",
      "Epoch 141/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0344 - accuracy: 0.9877 - val_loss: 0.2307 - val_accuracy: 0.9378\n",
      "Epoch 142/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0278 - accuracy: 0.9910 - val_loss: 0.2265 - val_accuracy: 0.9343\n",
      "Epoch 143/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.9897 - val_loss: 0.3004 - val_accuracy: 0.9255\n",
      "Epoch 144/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0393 - accuracy: 0.9875 - val_loss: 0.2182 - val_accuracy: 0.9407\n",
      "Epoch 145/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0311 - accuracy: 0.9893 - val_loss: 0.2534 - val_accuracy: 0.9343\n",
      "Epoch 146/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0408 - accuracy: 0.9865 - val_loss: 0.2594 - val_accuracy: 0.9266\n",
      "Epoch 147/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0233 - accuracy: 0.9910 - val_loss: 0.2568 - val_accuracy: 0.9337\n",
      "Epoch 148/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0208 - accuracy: 0.9934 - val_loss: 0.2584 - val_accuracy: 0.9343\n",
      "Epoch 149/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0267 - accuracy: 0.9902 - val_loss: 0.2465 - val_accuracy: 0.9296\n",
      "Epoch 150/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0329 - accuracy: 0.9887 - val_loss: 0.2908 - val_accuracy: 0.9337\n",
      "Epoch 151/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0189 - accuracy: 0.9930 - val_loss: 0.2417 - val_accuracy: 0.9437\n",
      "Epoch 152/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0307 - accuracy: 0.9885 - val_loss: 0.2627 - val_accuracy: 0.9372\n",
      "Epoch 153/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0246 - accuracy: 0.9905 - val_loss: 0.2977 - val_accuracy: 0.9272\n",
      "Epoch 154/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0256 - accuracy: 0.9910 - val_loss: 0.2555 - val_accuracy: 0.9302\n",
      "Epoch 155/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0390 - accuracy: 0.9874 - val_loss: 0.2442 - val_accuracy: 0.9308\n",
      "Epoch 156/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0294 - accuracy: 0.9903 - val_loss: 0.2321 - val_accuracy: 0.9372\n",
      "Epoch 157/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0345 - accuracy: 0.9866 - val_loss: 0.2854 - val_accuracy: 0.9296\n",
      "Epoch 158/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0335 - accuracy: 0.9880 - val_loss: 0.2388 - val_accuracy: 0.9390\n",
      "Epoch 159/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0321 - accuracy: 0.9894 - val_loss: 0.2435 - val_accuracy: 0.9396\n",
      "Epoch 160/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0339 - accuracy: 0.9893 - val_loss: 0.2418 - val_accuracy: 0.9384\n",
      "Epoch 161/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0270 - accuracy: 0.9902 - val_loss: 0.2556 - val_accuracy: 0.9407\n",
      "Epoch 162/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0324 - accuracy: 0.9897 - val_loss: 0.2453 - val_accuracy: 0.9390\n",
      "Epoch 163/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0341 - accuracy: 0.9896 - val_loss: 0.2214 - val_accuracy: 0.9354\n",
      "Epoch 164/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0423 - accuracy: 0.9862 - val_loss: 0.2318 - val_accuracy: 0.9349\n",
      "Epoch 165/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0267 - accuracy: 0.9909 - val_loss: 0.2407 - val_accuracy: 0.9437\n",
      "Epoch 166/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0252 - accuracy: 0.9916 - val_loss: 0.2262 - val_accuracy: 0.9413\n",
      "Epoch 167/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0267 - accuracy: 0.9902 - val_loss: 0.2503 - val_accuracy: 0.9349\n",
      "Epoch 168/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0245 - accuracy: 0.9913 - val_loss: 0.2338 - val_accuracy: 0.9354\n",
      "Epoch 169/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0272 - accuracy: 0.9903 - val_loss: 0.2367 - val_accuracy: 0.9390\n",
      "Epoch 170/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0315 - accuracy: 0.9887 - val_loss: 0.2416 - val_accuracy: 0.9331\n",
      "Epoch 171/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0274 - accuracy: 0.9884 - val_loss: 0.2649 - val_accuracy: 0.9313\n",
      "Epoch 172/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0328 - accuracy: 0.9888 - val_loss: 0.2521 - val_accuracy: 0.9325\n",
      "Epoch 173/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0208 - accuracy: 0.9925 - val_loss: 0.2453 - val_accuracy: 0.9337\n",
      "Epoch 174/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0331 - accuracy: 0.9872 - val_loss: 0.2004 - val_accuracy: 0.9425\n",
      "Epoch 175/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0150 - accuracy: 0.9953 - val_loss: 0.2168 - val_accuracy: 0.9437\n",
      "Epoch 176/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0297 - accuracy: 0.9902 - val_loss: 0.2543 - val_accuracy: 0.9349\n",
      "Epoch 177/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0271 - accuracy: 0.9903 - val_loss: 0.2013 - val_accuracy: 0.9437\n",
      "Epoch 178/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0233 - accuracy: 0.9918 - val_loss: 0.2388 - val_accuracy: 0.9390\n",
      "Epoch 179/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0255 - accuracy: 0.9905 - val_loss: 0.2313 - val_accuracy: 0.9354\n",
      "Epoch 180/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0267 - accuracy: 0.9919 - val_loss: 0.2113 - val_accuracy: 0.9413\n",
      "Epoch 181/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0236 - accuracy: 0.9912 - val_loss: 0.2699 - val_accuracy: 0.9313\n",
      "Epoch 182/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0338 - accuracy: 0.9881 - val_loss: 0.2828 - val_accuracy: 0.9272\n",
      "Epoch 183/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0244 - accuracy: 0.9924 - val_loss: 0.2580 - val_accuracy: 0.9372\n",
      "Epoch 184/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0291 - accuracy: 0.9900 - val_loss: 0.2081 - val_accuracy: 0.9460\n",
      "Epoch 185/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0237 - accuracy: 0.9927 - val_loss: 0.2258 - val_accuracy: 0.9419\n",
      "Epoch 186/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0245 - accuracy: 0.9916 - val_loss: 0.2185 - val_accuracy: 0.9442\n",
      "Epoch 187/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0262 - accuracy: 0.9903 - val_loss: 0.2507 - val_accuracy: 0.9354\n",
      "Epoch 188/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0181 - accuracy: 0.9953 - val_loss: 0.1900 - val_accuracy: 0.9495\n",
      "Epoch 189/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0215 - accuracy: 0.9927 - val_loss: 0.2519 - val_accuracy: 0.9372\n",
      "Epoch 190/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0275 - accuracy: 0.9919 - val_loss: 0.2626 - val_accuracy: 0.9296\n",
      "Epoch 191/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0277 - accuracy: 0.9891 - val_loss: 0.2579 - val_accuracy: 0.9366\n",
      "Epoch 192/300\n",
      "213/213 [==============================] - 1s 4ms/step - loss: 0.0282 - accuracy: 0.9888 - val_loss: 0.2641 - val_accuracy: 0.9354\n",
      "Epoch 193/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0215 - accuracy: 0.9919 - val_loss: 0.2232 - val_accuracy: 0.9460\n",
      "Epoch 194/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0217 - accuracy: 0.9928 - val_loss: 0.2208 - val_accuracy: 0.9413\n",
      "Epoch 195/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0220 - accuracy: 0.9918 - val_loss: 0.2427 - val_accuracy: 0.9437\n",
      "Epoch 196/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0228 - accuracy: 0.9921 - val_loss: 0.2352 - val_accuracy: 0.9360\n",
      "Epoch 197/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0223 - accuracy: 0.9925 - val_loss: 0.2465 - val_accuracy: 0.9325\n",
      "Epoch 198/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0191 - accuracy: 0.9928 - val_loss: 0.2162 - val_accuracy: 0.9454\n",
      "Epoch 199/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0228 - accuracy: 0.9937 - val_loss: 0.2347 - val_accuracy: 0.9442\n",
      "Epoch 200/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0299 - accuracy: 0.9896 - val_loss: 0.2548 - val_accuracy: 0.9396\n",
      "Epoch 201/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0213 - accuracy: 0.9909 - val_loss: 0.2271 - val_accuracy: 0.9466\n",
      "Epoch 202/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0193 - accuracy: 0.9922 - val_loss: 0.2494 - val_accuracy: 0.9372\n",
      "Epoch 203/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0186 - accuracy: 0.9940 - val_loss: 0.2298 - val_accuracy: 0.9407\n",
      "Epoch 204/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0264 - accuracy: 0.9900 - val_loss: 0.2648 - val_accuracy: 0.9360\n",
      "Epoch 205/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0223 - accuracy: 0.9924 - val_loss: 0.2588 - val_accuracy: 0.9354\n",
      "Epoch 206/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0209 - accuracy: 0.9931 - val_loss: 0.2430 - val_accuracy: 0.9396\n",
      "Epoch 207/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0210 - accuracy: 0.9921 - val_loss: 0.2575 - val_accuracy: 0.9401\n",
      "Epoch 208/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0206 - accuracy: 0.9940 - val_loss: 0.2539 - val_accuracy: 0.9384\n",
      "Epoch 209/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0241 - accuracy: 0.9915 - val_loss: 0.2929 - val_accuracy: 0.9308\n",
      "Epoch 210/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0232 - accuracy: 0.9913 - val_loss: 0.2969 - val_accuracy: 0.9308\n",
      "Epoch 211/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0184 - accuracy: 0.9937 - val_loss: 0.2592 - val_accuracy: 0.9466\n",
      "Epoch 212/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0277 - accuracy: 0.9893 - val_loss: 0.2737 - val_accuracy: 0.9308\n",
      "Epoch 213/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0199 - accuracy: 0.9937 - val_loss: 0.2888 - val_accuracy: 0.9284\n",
      "Epoch 214/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0278 - accuracy: 0.9900 - val_loss: 0.2867 - val_accuracy: 0.9272\n",
      "Epoch 215/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0243 - accuracy: 0.9924 - val_loss: 0.2458 - val_accuracy: 0.9349\n",
      "Epoch 216/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0268 - accuracy: 0.9906 - val_loss: 0.2424 - val_accuracy: 0.9401\n",
      "Epoch 217/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0249 - accuracy: 0.9908 - val_loss: 0.2965 - val_accuracy: 0.9249\n",
      "Epoch 218/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0195 - accuracy: 0.9940 - val_loss: 0.2561 - val_accuracy: 0.9343\n",
      "Epoch 219/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0210 - accuracy: 0.9935 - val_loss: 0.2649 - val_accuracy: 0.9360\n",
      "Epoch 220/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0284 - accuracy: 0.9888 - val_loss: 0.2838 - val_accuracy: 0.9296\n",
      "Epoch 221/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0162 - accuracy: 0.9934 - val_loss: 0.2180 - val_accuracy: 0.9448\n",
      "Epoch 222/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0156 - accuracy: 0.9946 - val_loss: 0.2501 - val_accuracy: 0.9372\n",
      "Epoch 223/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0133 - accuracy: 0.9953 - val_loss: 0.2853 - val_accuracy: 0.9313\n",
      "Epoch 224/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0178 - accuracy: 0.9938 - val_loss: 0.2406 - val_accuracy: 0.9431\n",
      "Epoch 225/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0189 - accuracy: 0.9935 - val_loss: 0.2178 - val_accuracy: 0.9437\n",
      "Epoch 226/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0210 - accuracy: 0.9931 - val_loss: 0.2688 - val_accuracy: 0.9349\n",
      "Epoch 227/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0214 - accuracy: 0.9921 - val_loss: 0.2989 - val_accuracy: 0.9302\n",
      "Epoch 228/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 0.9928 - val_loss: 0.2205 - val_accuracy: 0.9378\n",
      "Epoch 229/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0184 - accuracy: 0.9934 - val_loss: 0.2272 - val_accuracy: 0.9419\n",
      "Epoch 230/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0231 - accuracy: 0.9924 - val_loss: 0.2775 - val_accuracy: 0.9337\n",
      "Epoch 231/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0316 - accuracy: 0.9903 - val_loss: 0.2156 - val_accuracy: 0.9472\n",
      "Epoch 232/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0197 - accuracy: 0.9913 - val_loss: 0.2404 - val_accuracy: 0.9372\n",
      "Epoch 233/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0252 - accuracy: 0.9922 - val_loss: 0.2327 - val_accuracy: 0.9419\n",
      "Epoch 234/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0185 - accuracy: 0.9946 - val_loss: 0.2396 - val_accuracy: 0.9396\n",
      "Epoch 235/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0170 - accuracy: 0.9938 - val_loss: 0.2424 - val_accuracy: 0.9396\n",
      "Epoch 236/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0140 - accuracy: 0.9943 - val_loss: 0.2342 - val_accuracy: 0.9431\n",
      "Epoch 237/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 0.9966 - val_loss: 0.2383 - val_accuracy: 0.9448\n",
      "Epoch 238/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0185 - accuracy: 0.9941 - val_loss: 0.2914 - val_accuracy: 0.9308\n",
      "Epoch 239/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0217 - accuracy: 0.9930 - val_loss: 0.2478 - val_accuracy: 0.9454\n",
      "Epoch 240/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0246 - accuracy: 0.9919 - val_loss: 0.2444 - val_accuracy: 0.9337\n",
      "Epoch 241/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 0.9938 - val_loss: 0.2355 - val_accuracy: 0.9401\n",
      "Epoch 242/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0169 - accuracy: 0.9935 - val_loss: 0.2632 - val_accuracy: 0.9337\n",
      "Epoch 243/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0177 - accuracy: 0.9938 - val_loss: 0.2293 - val_accuracy: 0.9419\n",
      "Epoch 244/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0209 - accuracy: 0.9919 - val_loss: 0.2519 - val_accuracy: 0.9390\n",
      "Epoch 245/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0224 - accuracy: 0.9925 - val_loss: 0.2451 - val_accuracy: 0.9372\n",
      "Epoch 246/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0291 - accuracy: 0.9913 - val_loss: 0.2337 - val_accuracy: 0.9366\n",
      "Epoch 247/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0229 - accuracy: 0.9913 - val_loss: 0.2200 - val_accuracy: 0.9431\n",
      "Epoch 248/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0244 - accuracy: 0.9908 - val_loss: 0.2237 - val_accuracy: 0.9437\n",
      "Epoch 249/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0190 - accuracy: 0.9944 - val_loss: 0.2327 - val_accuracy: 0.9396\n",
      "Epoch 250/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0224 - accuracy: 0.9930 - val_loss: 0.2651 - val_accuracy: 0.9337\n",
      "Epoch 251/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0163 - accuracy: 0.9934 - val_loss: 0.1918 - val_accuracy: 0.9437\n",
      "Epoch 252/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0158 - accuracy: 0.9952 - val_loss: 0.2380 - val_accuracy: 0.9419\n",
      "Epoch 253/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0174 - accuracy: 0.9943 - val_loss: 0.2227 - val_accuracy: 0.9425\n",
      "Epoch 254/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0167 - accuracy: 0.9937 - val_loss: 0.2048 - val_accuracy: 0.9442\n",
      "Epoch 255/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0138 - accuracy: 0.9952 - val_loss: 0.2058 - val_accuracy: 0.9442\n",
      "Epoch 256/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0167 - accuracy: 0.9946 - val_loss: 0.2548 - val_accuracy: 0.9354\n",
      "Epoch 257/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0118 - accuracy: 0.9963 - val_loss: 0.2461 - val_accuracy: 0.9396\n",
      "Epoch 258/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0175 - accuracy: 0.9947 - val_loss: 0.2782 - val_accuracy: 0.9378\n",
      "Epoch 259/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0236 - accuracy: 0.9927 - val_loss: 0.2628 - val_accuracy: 0.9360\n",
      "Epoch 260/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 0.9938 - val_loss: 0.2366 - val_accuracy: 0.9501\n",
      "Epoch 261/300\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.99 - 0s 2ms/step - loss: 0.0300 - accuracy: 0.9909 - val_loss: 0.2859 - val_accuracy: 0.9331\n",
      "Epoch 262/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0246 - accuracy: 0.9916 - val_loss: 0.2358 - val_accuracy: 0.9437\n",
      "Epoch 263/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0206 - accuracy: 0.9927 - val_loss: 0.2373 - val_accuracy: 0.9390\n",
      "Epoch 264/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0101 - accuracy: 0.9965 - val_loss: 0.2232 - val_accuracy: 0.9478\n",
      "Epoch 265/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 0.9975 - val_loss: 0.2055 - val_accuracy: 0.9501\n",
      "Epoch 266/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0139 - accuracy: 0.9949 - val_loss: 0.2148 - val_accuracy: 0.9454\n",
      "Epoch 267/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0144 - accuracy: 0.9947 - val_loss: 0.2190 - val_accuracy: 0.9378\n",
      "Epoch 268/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0175 - accuracy: 0.9934 - val_loss: 0.2404 - val_accuracy: 0.9466\n",
      "Epoch 269/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0177 - accuracy: 0.9937 - val_loss: 0.2416 - val_accuracy: 0.9378\n",
      "Epoch 270/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0176 - accuracy: 0.9940 - val_loss: 0.2735 - val_accuracy: 0.9313\n",
      "Epoch 271/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0186 - accuracy: 0.9935 - val_loss: 0.2373 - val_accuracy: 0.9437\n",
      "Epoch 272/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0178 - accuracy: 0.9940 - val_loss: 0.2209 - val_accuracy: 0.9431\n",
      "Epoch 273/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 0.9944 - val_loss: 0.2701 - val_accuracy: 0.9360\n",
      "Epoch 274/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0292 - accuracy: 0.9891 - val_loss: 0.2501 - val_accuracy: 0.9360\n",
      "Epoch 275/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0152 - accuracy: 0.9953 - val_loss: 0.2070 - val_accuracy: 0.9460\n",
      "Epoch 276/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0179 - accuracy: 0.9934 - val_loss: 0.2303 - val_accuracy: 0.9401\n",
      "Epoch 277/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0160 - accuracy: 0.9947 - val_loss: 0.2543 - val_accuracy: 0.9448\n",
      "Epoch 278/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0217 - accuracy: 0.9931 - val_loss: 0.2497 - val_accuracy: 0.9396\n",
      "Epoch 279/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0131 - accuracy: 0.9954 - val_loss: 0.2122 - val_accuracy: 0.9454\n",
      "Epoch 280/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0212 - accuracy: 0.9924 - val_loss: 0.2229 - val_accuracy: 0.9478\n",
      "Epoch 281/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0205 - accuracy: 0.9935 - val_loss: 0.2297 - val_accuracy: 0.9419\n",
      "Epoch 282/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0169 - accuracy: 0.9947 - val_loss: 0.2560 - val_accuracy: 0.9425\n",
      "Epoch 283/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0231 - accuracy: 0.9932 - val_loss: 0.2325 - val_accuracy: 0.9378\n",
      "Epoch 284/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0120 - accuracy: 0.9959 - val_loss: 0.2026 - val_accuracy: 0.9489\n",
      "Epoch 285/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 0.9966 - val_loss: 0.2323 - val_accuracy: 0.9384\n",
      "Epoch 286/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0120 - accuracy: 0.9960 - val_loss: 0.2277 - val_accuracy: 0.9437\n",
      "Epoch 287/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0154 - accuracy: 0.9952 - val_loss: 0.2374 - val_accuracy: 0.9413\n",
      "Epoch 288/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0201 - accuracy: 0.9947 - val_loss: 0.2757 - val_accuracy: 0.9384\n",
      "Epoch 289/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0169 - accuracy: 0.9941 - val_loss: 0.2600 - val_accuracy: 0.9372\n",
      "Epoch 290/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.9912 - val_loss: 0.2537 - val_accuracy: 0.9378\n",
      "Epoch 291/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0140 - accuracy: 0.9946 - val_loss: 0.2219 - val_accuracy: 0.9466\n",
      "Epoch 292/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.0127 - accuracy: 0.9953 - val_loss: 0.2332 - val_accuracy: 0.9419\n",
      "Epoch 293/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0127 - accuracy: 0.9957 - val_loss: 0.2427 - val_accuracy: 0.9396\n",
      "Epoch 294/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.0196 - accuracy: 0.9937 - val_loss: 0.2342 - val_accuracy: 0.9431\n",
      "Epoch 295/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0128 - accuracy: 0.9963 - val_loss: 0.2267 - val_accuracy: 0.9448\n",
      "Epoch 296/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0125 - accuracy: 0.9969 - val_loss: 0.2345 - val_accuracy: 0.9384\n",
      "Epoch 297/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.2400 - val_accuracy: 0.9460\n",
      "Epoch 298/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0180 - accuracy: 0.9944 - val_loss: 0.2273 - val_accuracy: 0.9425\n",
      "Epoch 299/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0202 - accuracy: 0.9934 - val_loss: 0.2527 - val_accuracy: 0.9396\n",
      "Epoch 300/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.0139 - accuracy: 0.9952 - val_loss: 0.2247 - val_accuracy: 0.9478\n",
      "train: \n",
      " [[1503    0    0]\n",
      " [   0 3435    1]\n",
      " [   0    0 1873]]\n",
      "test: \n",
      " [[363  18  10]\n",
      " [ 27 797  11]\n",
      " [ 17   6 455]]\n",
      "test:0.947770\n",
      "train:0.999853\n"
     ]
    }
   ],
   "source": [
    "#sc = StandardScaler(with_mean=True)\n",
    "#feature_sc = sc.fit_transform(feature)\n",
    "#feature2_sc = sc.transform(feature2)\n",
    "#pca = PCA(n_components=160,copy=True)\n",
    "#feature_pca = pca.fit_transform(feature)\n",
    "#feature2_pca = pca.transform(feature2)\n",
    "train,test,sc,pca = train_model(model,feature,np.array(y),False)\n",
    "#rest = test_model(model,feature2,np.array(y2),sc)\n",
    "#acc = [train,valid,test,rest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.726552\n",
      "[[207 152 117]\n",
      " [110 909  58]\n",
      " [ 68  81 441]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7265515632291181"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model,np.array(feature2),np.array(y2),sc,pca,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ind = ((y2==1)|(y2==2)|(y2==6))\n",
    "oc = OneHotEncoder()\n",
    "y_01 = oc.fit_transform(y2[ind,np.newaxis]).toarray()\n",
    "y_pred=model.predict(sc.transform(np.array(feature2)[ind,:]))\n",
    "ind2=np.where(np.argmax(y_pred,axis=1) != np.argmax(y_01,axis=1))\n",
    "#ind2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model/ann2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       ...,\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = np.array((y==0)|(y==1)|(y==2)|(y==6))\n",
    "#feature\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model(model,feature2_sc,np.array(y2))\n",
    "ind = ((y==1)|(y==2)|(y==6))\n",
    "y_01 = y[ind].copy()\n",
    "oc = OneHotEncoder()\n",
    "y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "pred=model.predict(feature_sc[ind,:])\n",
    "np.argmax(pred,axis=1)\n",
    "np.argmax(y_01,axis=1)\n",
    "metrics.confusion_matrix(np.argmax(y_01,axis=1),np.argmax(pred,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc_ann = pd.DataFrame(acc,columns=['dnn1'],index=['train','valid','test','rest data'])\n",
    "#acc_ann_com=pd.concat([acc_ann_com,acc_ann.T])\n",
    "#acc_ann.loc['drop_mDWT',:]=[train,valid,test,rest]\n",
    "acc_ann_com.loc['dnn2',:]=acc\n",
    "acc_ann_com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test feature from Left or Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = layers.Input(shape=feature.loc[:,ind_temp].shape[1:])\n",
    "l1 = layers.Dense(128,activation='elu')(input_)\n",
    "drop1 = layers.Dropout(0.2)(l1)\n",
    "l2 = layers.Dense(64,activation='elu')(drop1)\n",
    "drop2 = layers.Dropout(0.2)(l2)\n",
    "#l3 = layers.Dense(32,activation='elu')(drop2)\n",
    "#drop3 = layers.Dropout(0.2)(l3)\n",
    "#l4 = layers.Dense(16,activation='selu')(l3)\n",
    "#drop4 = layers.Dropout(0.2)(l4)\n",
    "output = layers.Dense(1,activation='sigmoid')(drop2)\n",
    "model = Model(inputs=[input_],outputs=[output])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23125 samples, validate on 5782 samples\n",
      "Epoch 1/200\n",
      "23125/23125 [==============================] - 4s 167us/sample - loss: 1.9657 - accuracy: 0.5308 - val_loss: 1.3537 - val_accuracy: 0.6404\n",
      "Epoch 2/200\n",
      "23125/23125 [==============================] - 3s 109us/sample - loss: 1.3913 - accuracy: 0.6479 - val_loss: 1.1555 - val_accuracy: 0.7008\n",
      "Epoch 3/200\n",
      "23125/23125 [==============================] - 3s 110us/sample - loss: 1.2045 - accuracy: 0.7052 - val_loss: 1.0439 - val_accuracy: 0.7771\n",
      "Epoch 4/200\n",
      "23125/23125 [==============================] - 2s 105us/sample - loss: 1.1076 - accuracy: 0.7394 - val_loss: 0.9467 - val_accuracy: 0.7608\n",
      "Epoch 5/200\n",
      "23125/23125 [==============================] - 2s 86us/sample - loss: 1.0526 - accuracy: 0.7501 - val_loss: 0.9212 - val_accuracy: 0.7508\n",
      "Epoch 6/200\n",
      "23125/23125 [==============================] - 2s 93us/sample - loss: 0.9990 - accuracy: 0.7677 - val_loss: 0.8937 - val_accuracy: 0.7975\n",
      "Epoch 7/200\n",
      "23125/23125 [==============================] - 2s 98us/sample - loss: 0.9561 - accuracy: 0.7806 - val_loss: 0.8888 - val_accuracy: 0.7916\n",
      "Epoch 8/200\n",
      "23125/23125 [==============================] - 2s 85us/sample - loss: 0.9064 - accuracy: 0.8004 - val_loss: 0.8675 - val_accuracy: 0.8127\n",
      "Epoch 9/200\n",
      "23125/23125 [==============================] - 2s 96us/sample - loss: 0.8361 - accuracy: 0.8109 - val_loss: 0.9959 - val_accuracy: 0.8388\n",
      "Epoch 10/200\n",
      "23125/23125 [==============================] - 2s 91us/sample - loss: 0.8394 - accuracy: 0.8178 - val_loss: 0.8477 - val_accuracy: 0.8080\n",
      "Epoch 11/200\n",
      "23125/23125 [==============================] - 2s 89us/sample - loss: 0.8182 - accuracy: 0.8169 - val_loss: 0.8487 - val_accuracy: 0.8193\n",
      "Epoch 12/200\n",
      "23125/23125 [==============================] - 2s 87us/sample - loss: 0.7716 - accuracy: 0.8304 - val_loss: 1.0611 - val_accuracy: 0.8345\n",
      "Epoch 13/200\n",
      "23125/23125 [==============================] - 2s 87us/sample - loss: 0.8293 - accuracy: 0.8183 - val_loss: 0.9155 - val_accuracy: 0.8390\n",
      "Epoch 14/200\n",
      "23125/23125 [==============================] - 2s 82us/sample - loss: 0.8102 - accuracy: 0.8220 - val_loss: 0.8263 - val_accuracy: 0.8255\n",
      "Epoch 15/200\n",
      "23125/23125 [==============================] - 2s 82us/sample - loss: 0.7460 - accuracy: 0.8396 - val_loss: 0.7475 - val_accuracy: 0.8295\n",
      "Epoch 16/200\n",
      "23125/23125 [==============================] - 3s 109us/sample - loss: 0.7118 - accuracy: 0.8471 - val_loss: 0.8684 - val_accuracy: 0.8412\n",
      "Epoch 17/200\n",
      "23125/23125 [==============================] - 3s 135us/sample - loss: 0.7334 - accuracy: 0.8410 - val_loss: 0.8593 - val_accuracy: 0.8258\n",
      "Epoch 18/200\n",
      "23125/23125 [==============================] - 3s 125us/sample - loss: 0.7021 - accuracy: 0.8490 - val_loss: 0.8953 - val_accuracy: 0.8506\n",
      "Epoch 19/200\n",
      "23125/23125 [==============================] - 3s 121us/sample - loss: 0.6847 - accuracy: 0.8514 - val_loss: 0.9428 - val_accuracy: 0.8566\n",
      "Epoch 20/200\n",
      "23125/23125 [==============================] - 3s 133us/sample - loss: 0.7248 - accuracy: 0.8508 - val_loss: 0.9025 - val_accuracy: 0.8392\n",
      "Epoch 21/200\n",
      "23125/23125 [==============================] - 3s 118us/sample - loss: 0.7017 - accuracy: 0.8458 - val_loss: 0.8396 - val_accuracy: 0.8525\n",
      "Epoch 22/200\n",
      "23125/23125 [==============================] - 3s 110us/sample - loss: 0.6808 - accuracy: 0.8589 - val_loss: 0.8343 - val_accuracy: 0.8454\n",
      "Epoch 23/200\n",
      "23125/23125 [==============================] - 3s 111us/sample - loss: 0.6569 - accuracy: 0.8586 - val_loss: 0.9713 - val_accuracy: 0.8646\n",
      "Epoch 24/200\n",
      "23125/23125 [==============================] - 2s 91us/sample - loss: 0.7113 - accuracy: 0.8530 - val_loss: 0.9063 - val_accuracy: 0.8663\n",
      "Epoch 25/200\n",
      "23125/23125 [==============================] - 2s 87us/sample - loss: 0.6886 - accuracy: 0.8540 - val_loss: 1.0114 - val_accuracy: 0.8729\n",
      "Epoch 26/200\n",
      "23125/23125 [==============================] - 2s 83us/sample - loss: 0.6437 - accuracy: 0.8703 - val_loss: 1.0360 - val_accuracy: 0.8737\n",
      "Epoch 27/200\n",
      "23125/23125 [==============================] - 2s 82us/sample - loss: 0.6396 - accuracy: 0.8684 - val_loss: 1.0674 - val_accuracy: 0.8762\n",
      "Epoch 28/200\n",
      "23125/23125 [==============================] - 2s 87us/sample - loss: 0.5848 - accuracy: 0.8806 - val_loss: 1.3498 - val_accuracy: 0.8853\n",
      "Epoch 29/200\n",
      "23125/23125 [==============================] - 2s 79us/sample - loss: 0.6586 - accuracy: 0.8602 - val_loss: 0.9367 - val_accuracy: 0.8565\n",
      "Epoch 30/200\n",
      "23125/23125 [==============================] - 2s 75us/sample - loss: 0.6776 - accuracy: 0.8599 - val_loss: 1.0857 - val_accuracy: 0.8727\n",
      "Epoch 31/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.6196 - accuracy: 0.8752 - val_loss: 1.0133 - val_accuracy: 0.8807\n",
      "Epoch 32/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.6562 - accuracy: 0.8634 - val_loss: 0.8697 - val_accuracy: 0.8708\n",
      "Epoch 33/200\n",
      "23125/23125 [==============================] - 2s 73us/sample - loss: 0.5973 - accuracy: 0.8750 - val_loss: 0.9950 - val_accuracy: 0.8753\n",
      "Epoch 34/200\n",
      "23125/23125 [==============================] - 2s 73us/sample - loss: 0.5699 - accuracy: 0.8852 - val_loss: 0.9541 - val_accuracy: 0.8786\n",
      "Epoch 35/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.5811 - accuracy: 0.8883 - val_loss: 1.2480 - val_accuracy: 0.8967\n",
      "Epoch 36/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.6094 - accuracy: 0.8775 - val_loss: 0.9992 - val_accuracy: 0.8833\n",
      "Epoch 37/200\n",
      "23125/23125 [==============================] - 2s 80us/sample - loss: 0.5736 - accuracy: 0.8909 - val_loss: 1.0015 - val_accuracy: 0.8864\n",
      "Epoch 38/200\n",
      "23125/23125 [==============================] - 2s 77us/sample - loss: 0.5681 - accuracy: 0.8930 - val_loss: 0.8801 - val_accuracy: 0.8717\n",
      "Epoch 39/200\n",
      "23125/23125 [==============================] - 2s 77us/sample - loss: 0.5511 - accuracy: 0.8872 - val_loss: 1.2918 - val_accuracy: 0.8933\n",
      "Epoch 40/200\n",
      "23125/23125 [==============================] - 2s 74us/sample - loss: 0.5846 - accuracy: 0.8878 - val_loss: 1.0802 - val_accuracy: 0.8919\n",
      "Epoch 41/200\n",
      "23125/23125 [==============================] - 2s 83us/sample - loss: 0.5913 - accuracy: 0.8886 - val_loss: 0.7979 - val_accuracy: 0.8649\n",
      "Epoch 42/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.5855 - accuracy: 0.8799 - val_loss: 0.9434 - val_accuracy: 0.8912\n",
      "Epoch 43/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.5320 - accuracy: 0.8915 - val_loss: 1.4377 - val_accuracy: 0.8942\n",
      "Epoch 44/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.5590 - accuracy: 0.8926 - val_loss: 1.2363 - val_accuracy: 0.8945\n",
      "Epoch 45/200\n",
      "23125/23125 [==============================] - 2s 85us/sample - loss: 0.5660 - accuracy: 0.8974 - val_loss: 0.9355 - val_accuracy: 0.8770\n",
      "train: \n",
      " [[13662  1921]\n",
      " [    0  7542]]\n",
      "test: \n",
      " [[3359  562]\n",
      " [  35 1826]]\n",
      "test:0.896749\n",
      "train:0.916930\n",
      "[0 0 0 ... 0 0 0]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "acc:0.248125\n",
      "[[ 430    0]\n",
      " [1303    0]]\n",
      "0.24812463935372187\n",
      "Train on 23125 samples, validate on 5782 samples\n",
      "Epoch 1/200\n",
      "23125/23125 [==============================] - 2s 83us/sample - loss: 1.6699 - accuracy: 0.5641 - val_loss: 1.1863 - val_accuracy: 0.6956\n",
      "Epoch 2/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 1.2186 - accuracy: 0.6931 - val_loss: 1.0834 - val_accuracy: 0.7371\n",
      "Epoch 3/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 1.0666 - accuracy: 0.7425 - val_loss: 1.0060 - val_accuracy: 0.7791\n",
      "Epoch 4/200\n",
      "23125/23125 [==============================] - 2s 73us/sample - loss: 1.0038 - accuracy: 0.7606 - val_loss: 1.0101 - val_accuracy: 0.7757\n",
      "Epoch 5/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 1.0121 - accuracy: 0.7549 - val_loss: 0.9530 - val_accuracy: 0.7598\n",
      "Epoch 6/200\n",
      "23125/23125 [==============================] - 2s 75us/sample - loss: 0.8796 - accuracy: 0.7876 - val_loss: 1.0161 - val_accuracy: 0.8279\n",
      "Epoch 7/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.8478 - accuracy: 0.8073 - val_loss: 0.9871 - val_accuracy: 0.8248\n",
      "Epoch 8/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.8517 - accuracy: 0.8045 - val_loss: 0.8904 - val_accuracy: 0.8210\n",
      "Epoch 9/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.8091 - accuracy: 0.8131 - val_loss: 1.0212 - val_accuracy: 0.8426\n",
      "Epoch 10/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.7750 - accuracy: 0.8260 - val_loss: 1.0699 - val_accuracy: 0.8438\n",
      "Epoch 11/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.7591 - accuracy: 0.8365 - val_loss: 0.9963 - val_accuracy: 0.8513\n",
      "Epoch 12/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.7071 - accuracy: 0.8416 - val_loss: 1.0759 - val_accuracy: 0.8464\n",
      "Epoch 13/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.6955 - accuracy: 0.8444 - val_loss: 1.0100 - val_accuracy: 0.8442\n",
      "Epoch 14/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.7017 - accuracy: 0.8478 - val_loss: 1.0484 - val_accuracy: 0.8539\n",
      "Epoch 15/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.7070 - accuracy: 0.8503 - val_loss: 0.9769 - val_accuracy: 0.8648\n",
      "Epoch 16/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.6993 - accuracy: 0.8527 - val_loss: 0.8596 - val_accuracy: 0.8388\n",
      "Epoch 17/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.6776 - accuracy: 0.8485 - val_loss: 1.0544 - val_accuracy: 0.8644\n",
      "Epoch 18/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.6821 - accuracy: 0.8598 - val_loss: 0.9780 - val_accuracy: 0.8658\n",
      "Epoch 19/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.6573 - accuracy: 0.8584 - val_loss: 1.0270 - val_accuracy: 0.8687\n",
      "Epoch 20/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.6057 - accuracy: 0.8704 - val_loss: 1.1063 - val_accuracy: 0.8692\n",
      "Epoch 21/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.6138 - accuracy: 0.8713 - val_loss: 0.9826 - val_accuracy: 0.8469\n",
      "Epoch 22/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.6390 - accuracy: 0.8652 - val_loss: 1.1678 - val_accuracy: 0.8756\n",
      "Epoch 23/200\n",
      "23125/23125 [==============================] - 2s 75us/sample - loss: 0.6374 - accuracy: 0.8704 - val_loss: 1.1649 - val_accuracy: 0.8637\n",
      "Epoch 24/200\n",
      "23125/23125 [==============================] - 2s 73us/sample - loss: 0.6625 - accuracy: 0.8615 - val_loss: 1.0139 - val_accuracy: 0.8644\n",
      "Epoch 25/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.6115 - accuracy: 0.8678 - val_loss: 0.9688 - val_accuracy: 0.8634\n",
      "Epoch 26/200\n",
      "23125/23125 [==============================] - 2s 74us/sample - loss: 0.5845 - accuracy: 0.8767 - val_loss: 1.3254 - val_accuracy: 0.8826\n",
      "Epoch 27/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.5496 - accuracy: 0.8895 - val_loss: 1.3573 - val_accuracy: 0.8796\n",
      "Epoch 28/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.6670 - accuracy: 0.8630 - val_loss: 0.9269 - val_accuracy: 0.8490\n",
      "Epoch 29/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.5767 - accuracy: 0.8770 - val_loss: 1.3259 - val_accuracy: 0.8819\n",
      "Epoch 30/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.6089 - accuracy: 0.8846 - val_loss: 1.0698 - val_accuracy: 0.8691\n",
      "Epoch 31/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.5645 - accuracy: 0.8845 - val_loss: 1.0752 - val_accuracy: 0.8822\n",
      "Epoch 32/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.6610 - accuracy: 0.8730 - val_loss: 0.9069 - val_accuracy: 0.8675\n",
      "Epoch 33/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.5818 - accuracy: 0.8803 - val_loss: 1.0766 - val_accuracy: 0.8833\n",
      "Epoch 34/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.5978 - accuracy: 0.8772 - val_loss: 1.0967 - val_accuracy: 0.8841\n",
      "Epoch 35/200\n",
      "23125/23125 [==============================] - 2s 83us/sample - loss: 0.5508 - accuracy: 0.8896 - val_loss: 1.0682 - val_accuracy: 0.8744\n",
      "Epoch 36/200\n",
      "23125/23125 [==============================] - 2s 81us/sample - loss: 0.5556 - accuracy: 0.8859 - val_loss: 1.1288 - val_accuracy: 0.8796\n",
      "Epoch 37/200\n",
      "23125/23125 [==============================] - 2s 79us/sample - loss: 0.5555 - accuracy: 0.8897 - val_loss: 1.0734 - val_accuracy: 0.8862\n",
      "Epoch 38/200\n",
      "23125/23125 [==============================] - 2s 73us/sample - loss: 0.5966 - accuracy: 0.8803 - val_loss: 0.9272 - val_accuracy: 0.8686\n",
      "Epoch 39/200\n",
      "23125/23125 [==============================] - 2s 75us/sample - loss: 0.5482 - accuracy: 0.8869 - val_loss: 1.3462 - val_accuracy: 0.8966\n",
      "Epoch 40/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.5505 - accuracy: 0.8925 - val_loss: 1.0365 - val_accuracy: 0.8893\n",
      "Epoch 41/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.5973 - accuracy: 0.8802 - val_loss: 0.9774 - val_accuracy: 0.8852\n",
      "Epoch 42/200\n",
      "23125/23125 [==============================] - 2s 81us/sample - loss: 0.5678 - accuracy: 0.8853 - val_loss: 1.2457 - val_accuracy: 0.8910\n",
      "Epoch 43/200\n",
      "23125/23125 [==============================] - 2s 84us/sample - loss: 0.5707 - accuracy: 0.8997 - val_loss: 1.3619 - val_accuracy: 0.8935\n",
      "Epoch 44/200\n",
      "23125/23125 [==============================] - 2s 75us/sample - loss: 0.5471 - accuracy: 0.8922 - val_loss: 1.0956 - val_accuracy: 0.8872\n",
      "Epoch 45/200\n",
      "23125/23125 [==============================] - 2s 74us/sample - loss: 0.5946 - accuracy: 0.8762 - val_loss: 1.0329 - val_accuracy: 0.8777\n",
      "Epoch 46/200\n",
      "23125/23125 [==============================] - 2s 74us/sample - loss: 0.5659 - accuracy: 0.8837 - val_loss: 0.9916 - val_accuracy: 0.8812\n",
      "Epoch 47/200\n",
      "23125/23125 [==============================] - 2s 74us/sample - loss: 0.5310 - accuracy: 0.8933 - val_loss: 0.9934 - val_accuracy: 0.8860\n",
      "Epoch 48/200\n",
      "23125/23125 [==============================] - 2s 73us/sample - loss: 0.5431 - accuracy: 0.8881 - val_loss: 1.0694 - val_accuracy: 0.8884\n",
      "Epoch 49/200\n",
      "23125/23125 [==============================] - 2s 73us/sample - loss: 0.5159 - accuracy: 0.8993 - val_loss: 1.1309 - val_accuracy: 0.8942\n",
      "train: \n",
      " [[13597  1986]\n",
      " [    0  7542]]\n",
      "test: \n",
      " [[3361  560]\n",
      " [  38 1823]]\n",
      "test:0.896576\n",
      "train:0.914119\n",
      "[0 0 0 ... 0 0 0]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "acc:0.248125\n",
      "[[ 430    0]\n",
      " [1303    0]]\n",
      "0.24812463935372187\n"
     ]
    }
   ],
   "source": [
    "acc={}\n",
    "cols = ['LEFT','RIGHT']\n",
    "\n",
    "#sc = StandardScaler(with_mean=True)\n",
    "\n",
    "for col in cols:\n",
    "    ind_temp=feature.columns.str.contains(col)\n",
    "    #feature_sc = sc.fit_transform(feature.loc[:,ind_temp])\n",
    "    #feature2_sc = sc.transform(feature2.loc[:,ind_temp])\n",
    "    train,test,sc = train_model(model,np.array(feature.loc[:,ind_temp]),np.array(y))\n",
    "    \n",
    "    acc_rest=test_model(model,np.array(feature2.loc[:,ind_temp]),np.array(y2),sc)\n",
    "    #acc[col] = [train,valid,test,acc_rest]\n",
    "    print(acc_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_lr=pd.DataFrame(acc,index=['train','valid','test','rest data'])#.to_csv('./results/acc_lr_ann.csv')\n",
    "#acc_com=pd.concat([acc_ann_com.drop(['dnn','dnn1'],'index'),acc_lr.T])\n",
    "acc_com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test features from 2 of 8 signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test features from 2 of 8 signals\n",
    "\n",
    "acc_2f={}\n",
    "cols = [ 'LEFT_TA','LEFT_TS','LEFT_BF', 'LEFT_RF',\n",
    "        'RIGHT_TA','RIGHT_TS','RIGHT_BF', 'RIGHT_RF']\n",
    "\n",
    "\n",
    "for p in combinations(cols[:4],2):\n",
    "    ind_temp=feature.columns.str.contains(p[0])| feature.columns.str.contains(p[1])\n",
    "    #feature_sc = sc.fit_transform(feature.loc[:,ind_temp])\n",
    "    #feature2_sc = sc.transform(feature2.loc[:,ind_temp])\n",
    "    train,valid,test,sc = train_model(model,np.array(feature.loc[:,ind_temp]),np.array(y))\n",
    "    acc_rest=test_model(model,np.array(feature2.loc[:,ind_temp]),np.array(y2),sc)\n",
    "    acc_2f[p[0]+'_'+p[1]] = [train,valid,test,acc_rest]\n",
    "    print(acc_rest)\n",
    "    \n",
    "for p in combinations(cols[4:],2):\n",
    "    ind_temp=feature.columns.str.contains(p[0])| feature.columns.str.contains(p[1])\n",
    "    #feature_sc = sc.fit_transform(feature.loc[:,ind_temp])\n",
    "    #feature2_sc = sc.transform(feature2.loc[:,ind_temp])\n",
    "    train,valid,test,sc = train_model(model,np.array(feature.loc[:,ind_temp]),np.array(y))\n",
    "    acc_rest=test_model(model,np.array(feature2.loc[:,ind_temp]),np.array(y2),sc)\n",
    "    acc_2f[p[0]+'_'+p[1]] = [train,valid,test,acc_rest]\n",
    "    print(acc_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc_2f_ann = pd.DataFrame(acc_2f,index=['train','valid','test','rest data']).T\n",
    "#acc_com = pd.concat([acc_com,acc_2f_ann])\n",
    "acc_com.to_csv('./results/dropna/acc_com_ann.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(acc_2f,index=['train','valid','test','rest data']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on some of rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_of_rest = ['正常/P940_MSham_B_Walking_trial_6_emg.csv',\n",
    "                '正常/P940_M050_B_Walking_trial_4_emg.csv',\n",
    "                '正常/P812_M100_A_Walking_trial_3_emg.csv',\n",
    "                '正常/P645_M050_A_Walking_trial_3_emg.csv',\n",
    "                '正常/P623_Msham_B_Walking_trial_2_emg.csv',\n",
    "                '正常/P551_M50_B_Walking_trial_6_emg.csv',\n",
    "                'P379_M050_2_OFF_A_FoG_trial_1_emg.csv',\n",
    "                'P551_M050_2_B_FoG_trial_2_emg.csv']\n",
    "#booster = xgb.Booster()\n",
    "#booster.load_model('./model/XGBoost_W256_S64_Left.json')\n",
    "#model = xgb.XGBClassifier()\n",
    "#model._Booster = booster\n",
    "acc = []\n",
    "columns=['LEFT_TA','LEFT_TS','LEFT_BF', 'LEFT_RF']\n",
    "for file in some_of_rest:\n",
    "    path = './data/'+file\n",
    "    feature2,y2 = dp.pipeline_feature(path,width=256,stride=64,scaler=False,\n",
    "                                      threshold_WAMP=threshold_WAMP,\n",
    "                                      threshold_ZC=threshold_ZC,\n",
    "                                      threshold_SSC=threshold_SSC,\n",
    "                                      bins=bins,\n",
    "                                      ranges=HIST_range,\n",
    "                                      show_para=False,\n",
    "                                      filt = 250)\n",
    "    feature2_sc = sc.transform(feature2)\n",
    "    acc += [test_model(model,feature2_sc,y2)]\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(feature2_sc)\n",
    "metrics.accuracy_score(y2,y_pred>0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:1 (16583, 8)\n",
      "i:2 (4489, 8)\n",
      "i:3 (35121, 8)\n",
      "i:4 (31027, 8)\n",
      "i:5 (30783, 8)\n",
      "i:6 (159539, 8)\n",
      "i:7 (225455, 8)\n",
      "i:8 (362273, 8)\n",
      "i:9 (28868, 8)\n",
      "i:10 (20878, 8)\n",
      "i:11 (25322, 8)\n",
      "i:12 (24242, 8)\n",
      "i:13 (21728, 8)\n",
      "i:14 (27916, 8)\n",
      "i:15 (27146, 8)\n",
      "i:16 (25947, 8)\n",
      "i:17 (26203, 8)\n",
      "i:18 (89500, 8)\n",
      "i:19 (70282, 8)\n",
      "i:20 (79595, 8)\n",
      "i:21 (73280, 8)\n",
      "Duration: 4.539805\n"
     ]
    }
   ],
   "source": [
    "files = np.array(df.columns)\n",
    "X = []\n",
    "Y = []\n",
    "#sc = StandardScaler(with_mean=False)\n",
    "i = 0\n",
    "start = time.time()\n",
    "for file in files:\n",
    "    emg_data = pd.read_csv('./data/'+file)\n",
    "    #emg_data = emg_data.fillna({'LEFT_TA':emg_data.LEFT_TA.mean(),\n",
    "    #                            'LEFT_TS':emg_data.LEFT_TS.mean(),\n",
    "    #                            'LEFT_BF':emg_data.LEFT_BF.mean(),\n",
    "    #                            'LEFT_RF':emg_data.LEFT_RF.mean(),\n",
    "    #                            'RIGHT_TA':emg_data.RIGHT_TA.mean(),\n",
    "    #                            'RIGHT_TS':emg_data.RIGHT_TS.mean(),\n",
    "    #                            'RIGHT_BF':emg_data.RIGHT_BF.mean(),\n",
    "    #                            'RIGHT_RF':emg_data.RIGHT_RF.mean()})\n",
    "    #emg_data = emg_data[emg_data.Label1 == emg_data.Label2].reset_index(drop=True)\n",
    "    emg_data = emg_data.dropna().reset_index(drop=True)\n",
    "    x = np.array(emg_data.iloc[:,3:])\n",
    "    y = np.array(emg_data.Label2)\n",
    "    #x,y = dp.generate_window_slide_data(emg_data,width=width,stride=stride,scaler=False,same_label=True)\n",
    "    X += x.tolist()\n",
    "    Y += y.tolist()\n",
    "    i += 1\n",
    "    print('i:%d'%i,x.shape)\n",
    "end = time.time()\n",
    "duration = end-start\n",
    "print('Duration: %f'%(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,feature,y,binary=True,file=None):\n",
    "    if binary:\n",
    "        ind = ((y==0)|(y==1)|(y==2)|(y==3)|(y==4)|(y==6))\n",
    "        ind1 = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "        y_01 = y.copy()\n",
    "        #ind1 = ((y_01==1)|(y_01==2)|(y_01==6))\n",
    "        y_01[ind1] = 1\n",
    "        oc = OneHotEncoder()\n",
    "        y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "        cw = None#{0:1,1:5}\n",
    "    else:\n",
    "        ind = ((y==1)|(y==2)|(y==6))\n",
    "        #ind = ((y==1)|(y==6))\n",
    "        y_01 = y[ind].copy()\n",
    "        oc = OneHotEncoder()\n",
    "        y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "        cw = None#{0:5,1:1,2:1}#{0:2,1:1,2:10,3:2}\n",
    "    x_full,x_test,y_full,y_test = train_test_split(np.array(feature)[ind,:],y_01,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=123,\n",
    "                                                   shuffle=True)\n",
    "    x_train,x_valid,y_train,y_valid = train_test_split(x_full,y_full,\n",
    "                                                       test_size=0.25,\n",
    "                                                       random_state=555,\n",
    "                                                       shuffle=True)\n",
    "    \n",
    "    #sm = BorderlineSMOTE(random_state=50,kind='borderline-1')\n",
    "    #sm = SMOTE(random_state=50)\n",
    "    #print(y_full.shape)\n",
    "    #x_full,y_full = sm.fit_resample(x_full,y_full)\n",
    "    #print(y_full_n.shape)\n",
    "    x_full = np.reshape(x_full,(-1,128*8))\n",
    "    sc = StandardScaler(with_mean=False)\n",
    "    #sc = MinMaxScaler((0,1))\n",
    "    x_train = sc.fit_transform(x_full)\n",
    "    x_train = x_train.reshape((-1,128,8))\n",
    "    #pca = PCA(n_components=150)\n",
    "    #x_train = pca.fit_transform(x_train)\n",
    "    #x_valid = sc.transform(x_valid)\n",
    "    x_test = np.reshape(x_test,(-1,128*8))\n",
    "    x_test = sc.transform(x_test)\n",
    "    x_test = x_test.reshape((-1,128,8))\n",
    "    #x_test = pca.transform(x_test)\n",
    "    #x_train = x_full\n",
    "    \n",
    "    early_stopping = callbacks.EarlyStopping(patience = 10,\n",
    "                                             monitor = 'val_accuracy', \n",
    "                                             restore_best_weights=True)\n",
    "    history = model.fit(x_train,y_full,validation_data=[x_test,y_test],\n",
    "                        epochs=300,batch_size=500,class_weight=cw,\n",
    "                        callbacks=[early_stopping],\n",
    "                        shuffle=True)\n",
    "    \n",
    "    if binary:\n",
    "        y_pred_t=model.predict(x_test)\n",
    "        test = metrics.accuracy_score(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1))\n",
    "        #test = metrics.accuracy_score(y_test,y_pred_t>0.5)\n",
    "        \n",
    "        #y_pred_v=model.predict(x_valid)\n",
    "        #valid = metrics.accuracy_score(y_valid,np.argmax(y_pred_v,axis=1))\n",
    "        y_pred_ta=model.predict(x_train)\n",
    "        train = metrics.accuracy_score(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1))\n",
    "        #train = metrics.accuracy_score(y_full,y_pred_ta>0.5)\n",
    "        \n",
    "        print('train: \\n',metrics.confusion_matrix(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1)))\n",
    "        #print('valid: \\n',metrics.confusion_matrix(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1)))\n",
    "        print('test: \\n',metrics.confusion_matrix(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1)))\n",
    "        \n",
    "        #print('train: \\n',metrics.confusion_matrix(y_full,y_pred_ta>0.5))\n",
    "        #print('test: \\n',metrics.confusion_matrix(y_test,y_pred_t>0.5))\n",
    "\n",
    "    else:\n",
    "        y_pred_t=model.predict(x_test)\n",
    "        test = metrics.accuracy_score(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1))\n",
    "        #y_pred_v=model.predict(x_valid)\n",
    "        #valid = metrics.accuracy_score(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1))\n",
    "        y_pred_ta=model.predict(x_train)\n",
    "        train = metrics.accuracy_score(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1))        \n",
    "        print('train: \\n',metrics.confusion_matrix(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1)))\n",
    "        #print('valid: \\n',metrics.confusion_matrix(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1)))\n",
    "        print('test: \\n',metrics.confusion_matrix(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1)))\n",
    "    print('test:%f'%test)\n",
    "    #print('valid:%f'%valid)\n",
    "    print('train:%f'%train)\n",
    "    if file != None:\n",
    "        model.save_model(file)\n",
    "    return train,test,sc#,pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_re = feature_sc.reshape((-1,X.shape[1],1))\n",
    "x_full,x_test,y_full,y_test = train_test_split(np.array(feature_re),np.array(y_01),test_size=0.2,random_state=123)\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(x_full,y_full,test_size=0.2,random_state=555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = layers.Input(shape=[128,8])\n",
    "lstm1 = layers.LSTM(50,return_sequences=True)(input_)\n",
    "drop1 = layers.Dropout(0.2)(lstm1)\n",
    "lstm2 = layers.LSTM(50,return_sequences=True)(drop1)\n",
    "drop2 = layers.Dropout(0.2)(lstm2)\n",
    "#lstm3 = layers.LSTM(50,return_sequences=True)(drop2)\n",
    "#drop3 = layers.Dropout(0.2)(lstm3)\n",
    "lstm4 = layers.LSTM(50)(drop2)\n",
    "drop4 = layers.Dropout(0.2)(lstm4)\n",
    "dense1 = layers.Dense(64,activation='relu')(drop4)\n",
    "drop5 = layers.Dropout(0.2)(dense1)\n",
    "dense2 = layers.Dense(32,activation='relu')(drop5)\n",
    "output = layers.Dense(3,activation='softmax')(dense2)\n",
    "model = Model(inputs=[input_],outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stopping = callbacks.EarlyStopping(patience = 20,\n",
    "                                         monitor = 'val_accuracy', \n",
    "                                         restore_best_weights=True)\n",
    "history = model.fit(x_train,y_train,validation_data=[x_valid,y_valid],\n",
    "                    epochs=100,batch_size=500,\n",
    "                   callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8872 samples, validate on 2219 samples\n",
      "Epoch 1/300\n",
      "8872/8872 [==============================] - 88s 10ms/sample - loss: 0.9841 - accuracy: 0.4876 - val_loss: 0.8857 - val_accuracy: 0.5002\n",
      "Epoch 2/300\n",
      "8872/8872 [==============================] - 82s 9ms/sample - loss: 0.8572 - accuracy: 0.5571 - val_loss: 0.8288 - val_accuracy: 0.6471\n",
      "Epoch 3/300\n",
      "8872/8872 [==============================] - 81s 9ms/sample - loss: 0.8051 - accuracy: 0.6443 - val_loss: 0.7573 - val_accuracy: 0.6670\n",
      "Epoch 4/300\n",
      "8872/8872 [==============================] - 81s 9ms/sample - loss: 0.7433 - accuracy: 0.6677 - val_loss: 0.7142 - val_accuracy: 0.6791\n",
      "Epoch 5/300\n",
      "8872/8872 [==============================] - 81s 9ms/sample - loss: 0.7212 - accuracy: 0.6787 - val_loss: 0.6996 - val_accuracy: 0.6886\n",
      "Epoch 6/300\n",
      "8872/8872 [==============================] - 87s 10ms/sample - loss: 0.7071 - accuracy: 0.6878 - val_loss: 0.6952 - val_accuracy: 0.7008\n",
      "Epoch 7/300\n",
      "8872/8872 [==============================] - 81s 9ms/sample - loss: 0.6882 - accuracy: 0.6948 - val_loss: 0.6657 - val_accuracy: 0.7012\n",
      "Epoch 8/300\n",
      "8872/8872 [==============================] - 81s 9ms/sample - loss: 0.6646 - accuracy: 0.7073 - val_loss: 0.6449 - val_accuracy: 0.7265\n",
      "Epoch 9/300\n",
      "8872/8872 [==============================] - 84s 9ms/sample - loss: 0.6623 - accuracy: 0.7076 - val_loss: 0.6351 - val_accuracy: 0.7206\n",
      "Epoch 10/300\n",
      "8872/8872 [==============================] - 81s 9ms/sample - loss: 0.6435 - accuracy: 0.7187 - val_loss: 0.6351 - val_accuracy: 0.7274\n",
      "Epoch 11/300\n",
      "8872/8872 [==============================] - 85s 10ms/sample - loss: 0.6427 - accuracy: 0.7163 - val_loss: 0.6309 - val_accuracy: 0.7260\n",
      "Epoch 12/300\n",
      "8872/8872 [==============================] - 82s 9ms/sample - loss: 0.6384 - accuracy: 0.7187 - val_loss: 0.6255 - val_accuracy: 0.7256\n",
      "Epoch 13/300\n",
      "8872/8872 [==============================] - 87s 10ms/sample - loss: 0.6292 - accuracy: 0.7271 - val_loss: 0.6225 - val_accuracy: 0.7165\n",
      "Epoch 14/300\n",
      "8872/8872 [==============================] - 83s 9ms/sample - loss: 0.6213 - accuracy: 0.7299 - val_loss: 0.6190 - val_accuracy: 0.7210\n",
      "Epoch 15/300\n",
      "3000/8872 [=========>....................] - ETA: 57s - loss: 0.6155 - accuracy: 0.7276 WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-6c283b32f09c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-d3cca95f67ba>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, feature, y, binary, file)\u001b[0m\n\u001b[0;32m     50\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                         shuffle=True)\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train,test,sc = train_model(model,np.array(x),np.array(y),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature2_re = feature2_sc.reshape((-1,feature.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_matrix=np.array([[0,1,1,1],\n",
    "                  [1,0,1,1],\n",
    "                  [10,100,0,10],\n",
    "                  [1.,1.,1,0]])\n",
    "cost_matrix=np.array([[0,2.],\n",
    "                  [5.,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "P0 = 55080/(55080+8181)\n",
    "P1 = 8181/(55080+8181)\n",
    "P = [P0,P1]\n",
    "cost_vec = np.zeros(2)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        if i == j:\n",
    "            continue\n",
    "        cost_vec[i]+=1/(1-P[i])*P[j]*cost_matrix[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 5.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1567100, shape=(4, 4), dtype=float64, numpy=\n",
       "array([[  0.,   1.,  10.,   1.],\n",
       "       [  1.,   0., 100.,   1.],\n",
       "       [  1.,   1.,   0.,   1.],\n",
       "       [  1.,   1.,  10.,   0.]])>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [2.],\n",
       "       [5.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(cost_matrix,(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
