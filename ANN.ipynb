{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import data_processing as dp\n",
    "from scipy import signal\n",
    "from scipy.stats import skew\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('.\\data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 = '.\\data\\正常\\G11_Walking_trial_4_emg.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv('./processed data/featurePcwtf_W256_S64_WS32_DWTLmax_dropna_nm.csv')\n",
    "\n",
    "# read file name of data with various Labels\n",
    "df = pd.read_csv('./useful_data_label.csv',index_col=0) \n",
    "drop = 'P379_M050_2_OFF_A_FoG_trial_2_emg.csv'\n",
    "drop2= 'P812_M050_2_B_FoG_trial_2_emg.csv'\n",
    "ind_drop = (df.columns!=drop) | (df.columns!=drop2)\n",
    "# read file name of data with only label 0\n",
    "df2 = pd.read_csv('./unuseful_data_label.csv',index_col=0)\n",
    "# read some of the data with only label 0\n",
    "df3 = pd.read_csv('./data/file_name.txt',header=None)\n",
    "\n",
    "files = np.concatenate([np.array(df.columns[ind_drop]),np.array(df3.loc[:,0])])\n",
    "#ind = Data.File.isin(files)\n",
    "ind = (Data.File != drop) & (Data.File != drop2)\n",
    "Data_sel = Data[ind]\n",
    "Data_rest = Data[~ind]\n",
    "#ind2 = Data_rest.File == drop\n",
    "#Data_rest = Data_rest[ind2]\n",
    "#Data_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_col = ['_IEMG','_MAV','_SSI','_VAR','_RMS',\n",
    "               '_WL','_ZC','_SSC','_WAMP','_skew','_Acti','_AR','_HIST','_MDF']\n",
    "\n",
    "feature_all = Data_sel.iloc[:,1:-1]\n",
    "#ind_temp1 = feature_all.columns.str.contains('_mDWT')\n",
    "#ind_temp2 = feature_all.columns.str.contains('_Coe')\n",
    "#ind_temp3 = feature_all.columns.str.contains('_Scale')\n",
    "#ind_temp4 = feature_all.columns.str.contains('_HIST')\n",
    "#ind_temp = ind_temp1|ind_temp2|ind_temp3|ind_temp4\n",
    "#ind_temp = feature_all.columns.str.contains('mDWT')\n",
    "#feature = feature_all.loc[:,~ind_temp]\n",
    "feature = Data_sel.iloc[:,1:-1]\n",
    "#feature = Data_sel.iloc[:,:-2]\n",
    "y = Data_sel.Label\n",
    "#feature2_all = Data_rest.iloc[:,1:-1]\n",
    "#feature2 = feature2_all.loc[:,ind_temp]\n",
    "feature2 = Data_rest.iloc[:,1:-1]\n",
    "#feature2 = Data_rest.iloc[:,:-2]\n",
    "y2 = Data_rest.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './processed data/data_set_after_window_withoutSC.hdf5'\n",
    "with h5py.File(path,'r') as f:\n",
    "    x = f['cwt_data'][...]\n",
    "    y = f['label2'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "imp.reload(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "player = ctypes.windll.kernel32\n",
    "player.Beep(1000,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LEFT_TA_IEMG</th>\n",
       "      <th>LEFT_TS_IEMG</th>\n",
       "      <th>LEFT_BF_IEMG</th>\n",
       "      <th>LEFT_RF_IEMG</th>\n",
       "      <th>RIGHT_TA_IEMG</th>\n",
       "      <th>RIGHT_TS_IEMG</th>\n",
       "      <th>RIGHT_BF_IEMG</th>\n",
       "      <th>RIGHT_RF_IEMG</th>\n",
       "      <th>LEFT_TA_SSI</th>\n",
       "      <th>LEFT_TS_SSI</th>\n",
       "      <th>...</th>\n",
       "      <th>RIGHT_TS_Mean_Scale</th>\n",
       "      <th>RIGHT_TS_Median_Scale</th>\n",
       "      <th>RIGHT_BF_Mean_Coe</th>\n",
       "      <th>RIGHT_BF_Min_Coe</th>\n",
       "      <th>RIGHT_BF_Mean_Scale</th>\n",
       "      <th>RIGHT_BF_Median_Scale</th>\n",
       "      <th>RIGHT_RF_Mean_Coe</th>\n",
       "      <th>RIGHT_RF_Min_Coe</th>\n",
       "      <th>RIGHT_RF_Mean_Scale</th>\n",
       "      <th>RIGHT_RF_Median_Scale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10948</th>\n",
       "      <td>47.382390</td>\n",
       "      <td>46.594215</td>\n",
       "      <td>35.291904</td>\n",
       "      <td>116.145730</td>\n",
       "      <td>45.856050</td>\n",
       "      <td>123.335860</td>\n",
       "      <td>31.193258</td>\n",
       "      <td>51.369167</td>\n",
       "      <td>15.538249</td>\n",
       "      <td>18.513243</td>\n",
       "      <td>...</td>\n",
       "      <td>13.632590</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.177132</td>\n",
       "      <td>0.117279</td>\n",
       "      <td>17.253480</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.264891</td>\n",
       "      <td>0.133763</td>\n",
       "      <td>18.165906</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10949</th>\n",
       "      <td>44.828053</td>\n",
       "      <td>39.516617</td>\n",
       "      <td>34.159218</td>\n",
       "      <td>113.909065</td>\n",
       "      <td>42.923042</td>\n",
       "      <td>128.338090</td>\n",
       "      <td>31.143126</td>\n",
       "      <td>56.215000</td>\n",
       "      <td>14.999877</td>\n",
       "      <td>14.308270</td>\n",
       "      <td>...</td>\n",
       "      <td>14.116727</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.182103</td>\n",
       "      <td>0.117114</td>\n",
       "      <td>17.220130</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.224715</td>\n",
       "      <td>0.147266</td>\n",
       "      <td>17.052881</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10950</th>\n",
       "      <td>43.025867</td>\n",
       "      <td>37.990456</td>\n",
       "      <td>29.838589</td>\n",
       "      <td>114.139595</td>\n",
       "      <td>42.155900</td>\n",
       "      <td>129.607850</td>\n",
       "      <td>30.133968</td>\n",
       "      <td>55.165016</td>\n",
       "      <td>14.461066</td>\n",
       "      <td>13.801164</td>\n",
       "      <td>...</td>\n",
       "      <td>14.749943</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.158897</td>\n",
       "      <td>0.114445</td>\n",
       "      <td>16.446790</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.260783</td>\n",
       "      <td>0.131384</td>\n",
       "      <td>18.002591</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10951</th>\n",
       "      <td>44.651604</td>\n",
       "      <td>40.892870</td>\n",
       "      <td>31.411098</td>\n",
       "      <td>104.653760</td>\n",
       "      <td>45.591217</td>\n",
       "      <td>132.558000</td>\n",
       "      <td>31.221636</td>\n",
       "      <td>56.788517</td>\n",
       "      <td>15.174104</td>\n",
       "      <td>14.831537</td>\n",
       "      <td>...</td>\n",
       "      <td>14.029997</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.176271</td>\n",
       "      <td>0.121978</td>\n",
       "      <td>16.845654</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.326726</td>\n",
       "      <td>0.137582</td>\n",
       "      <td>19.130992</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952</th>\n",
       "      <td>43.412860</td>\n",
       "      <td>39.094160</td>\n",
       "      <td>30.660444</td>\n",
       "      <td>107.645220</td>\n",
       "      <td>47.048786</td>\n",
       "      <td>133.734630</td>\n",
       "      <td>29.432373</td>\n",
       "      <td>50.962555</td>\n",
       "      <td>14.925791</td>\n",
       "      <td>14.161764</td>\n",
       "      <td>...</td>\n",
       "      <td>14.439348</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.150912</td>\n",
       "      <td>0.114144</td>\n",
       "      <td>16.054218</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.272874</td>\n",
       "      <td>0.127858</td>\n",
       "      <td>16.135058</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16101</th>\n",
       "      <td>70.580040</td>\n",
       "      <td>75.913590</td>\n",
       "      <td>126.304886</td>\n",
       "      <td>28.200338</td>\n",
       "      <td>86.091930</td>\n",
       "      <td>38.523070</td>\n",
       "      <td>25.148800</td>\n",
       "      <td>41.124783</td>\n",
       "      <td>41.334454</td>\n",
       "      <td>46.245580</td>\n",
       "      <td>...</td>\n",
       "      <td>15.942846</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.187388</td>\n",
       "      <td>0.081706</td>\n",
       "      <td>16.524947</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.272937</td>\n",
       "      <td>0.102111</td>\n",
       "      <td>16.223742</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16102</th>\n",
       "      <td>76.214836</td>\n",
       "      <td>73.919020</td>\n",
       "      <td>130.151630</td>\n",
       "      <td>27.541174</td>\n",
       "      <td>83.605705</td>\n",
       "      <td>39.387753</td>\n",
       "      <td>23.524101</td>\n",
       "      <td>32.228546</td>\n",
       "      <td>47.416060</td>\n",
       "      <td>45.405724</td>\n",
       "      <td>...</td>\n",
       "      <td>16.790883</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.182730</td>\n",
       "      <td>0.075469</td>\n",
       "      <td>16.943298</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.197590</td>\n",
       "      <td>0.086402</td>\n",
       "      <td>16.462787</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16103</th>\n",
       "      <td>55.280030</td>\n",
       "      <td>103.922540</td>\n",
       "      <td>128.093640</td>\n",
       "      <td>27.076658</td>\n",
       "      <td>83.045140</td>\n",
       "      <td>33.136265</td>\n",
       "      <td>23.160860</td>\n",
       "      <td>29.875586</td>\n",
       "      <td>27.816164</td>\n",
       "      <td>73.014786</td>\n",
       "      <td>...</td>\n",
       "      <td>17.634540</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.191178</td>\n",
       "      <td>0.080705</td>\n",
       "      <td>17.288791</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.167673</td>\n",
       "      <td>0.075130</td>\n",
       "      <td>14.920290</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16104</th>\n",
       "      <td>78.838950</td>\n",
       "      <td>81.898060</td>\n",
       "      <td>130.907060</td>\n",
       "      <td>30.604612</td>\n",
       "      <td>73.035170</td>\n",
       "      <td>33.454340</td>\n",
       "      <td>23.480703</td>\n",
       "      <td>28.396082</td>\n",
       "      <td>48.587643</td>\n",
       "      <td>54.006077</td>\n",
       "      <td>...</td>\n",
       "      <td>18.773659</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.168605</td>\n",
       "      <td>0.086141</td>\n",
       "      <td>17.410496</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.176414</td>\n",
       "      <td>0.073928</td>\n",
       "      <td>16.442348</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16105</th>\n",
       "      <td>66.678080</td>\n",
       "      <td>86.774530</td>\n",
       "      <td>135.485320</td>\n",
       "      <td>28.700045</td>\n",
       "      <td>79.775130</td>\n",
       "      <td>31.042190</td>\n",
       "      <td>21.129416</td>\n",
       "      <td>28.895012</td>\n",
       "      <td>38.156773</td>\n",
       "      <td>57.230396</td>\n",
       "      <td>...</td>\n",
       "      <td>15.732634</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.108201</td>\n",
       "      <td>0.076624</td>\n",
       "      <td>16.305438</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.209978</td>\n",
       "      <td>0.072147</td>\n",
       "      <td>17.113561</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>931 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       LEFT_TA_IEMG  LEFT_TS_IEMG  LEFT_BF_IEMG  LEFT_RF_IEMG  RIGHT_TA_IEMG  \\\n",
       "10948     47.382390     46.594215     35.291904    116.145730      45.856050   \n",
       "10949     44.828053     39.516617     34.159218    113.909065      42.923042   \n",
       "10950     43.025867     37.990456     29.838589    114.139595      42.155900   \n",
       "10951     44.651604     40.892870     31.411098    104.653760      45.591217   \n",
       "10952     43.412860     39.094160     30.660444    107.645220      47.048786   \n",
       "...             ...           ...           ...           ...            ...   \n",
       "16101     70.580040     75.913590    126.304886     28.200338      86.091930   \n",
       "16102     76.214836     73.919020    130.151630     27.541174      83.605705   \n",
       "16103     55.280030    103.922540    128.093640     27.076658      83.045140   \n",
       "16104     78.838950     81.898060    130.907060     30.604612      73.035170   \n",
       "16105     66.678080     86.774530    135.485320     28.700045      79.775130   \n",
       "\n",
       "       RIGHT_TS_IEMG  RIGHT_BF_IEMG  RIGHT_RF_IEMG  LEFT_TA_SSI  LEFT_TS_SSI  \\\n",
       "10948     123.335860      31.193258      51.369167    15.538249    18.513243   \n",
       "10949     128.338090      31.143126      56.215000    14.999877    14.308270   \n",
       "10950     129.607850      30.133968      55.165016    14.461066    13.801164   \n",
       "10951     132.558000      31.221636      56.788517    15.174104    14.831537   \n",
       "10952     133.734630      29.432373      50.962555    14.925791    14.161764   \n",
       "...              ...            ...            ...          ...          ...   \n",
       "16101      38.523070      25.148800      41.124783    41.334454    46.245580   \n",
       "16102      39.387753      23.524101      32.228546    47.416060    45.405724   \n",
       "16103      33.136265      23.160860      29.875586    27.816164    73.014786   \n",
       "16104      33.454340      23.480703      28.396082    48.587643    54.006077   \n",
       "16105      31.042190      21.129416      28.895012    38.156773    57.230396   \n",
       "\n",
       "       ...  RIGHT_TS_Mean_Scale  RIGHT_TS_Median_Scale  RIGHT_BF_Mean_Coe  \\\n",
       "10948  ...            13.632590                   10.0           0.177132   \n",
       "10949  ...            14.116727                   11.0           0.182103   \n",
       "10950  ...            14.749943                   12.0           0.158897   \n",
       "10951  ...            14.029997                   11.0           0.176271   \n",
       "10952  ...            14.439348                   12.0           0.150912   \n",
       "...    ...                  ...                    ...                ...   \n",
       "16101  ...            15.942846                   14.0           0.187388   \n",
       "16102  ...            16.790883                   15.0           0.182730   \n",
       "16103  ...            17.634540                   16.0           0.191178   \n",
       "16104  ...            18.773659                   18.0           0.168605   \n",
       "16105  ...            15.732634                   13.0           0.108201   \n",
       "\n",
       "       RIGHT_BF_Min_Coe  RIGHT_BF_Mean_Scale  RIGHT_BF_Median_Scale  \\\n",
       "10948          0.117279            17.253480                   16.0   \n",
       "10949          0.117114            17.220130                   16.0   \n",
       "10950          0.114445            16.446790                   15.0   \n",
       "10951          0.121978            16.845654                   15.0   \n",
       "10952          0.114144            16.054218                   14.0   \n",
       "...                 ...                  ...                    ...   \n",
       "16101          0.081706            16.524947                   14.0   \n",
       "16102          0.075469            16.943298                   15.0   \n",
       "16103          0.080705            17.288791                   16.0   \n",
       "16104          0.086141            17.410496                   16.0   \n",
       "16105          0.076624            16.305438                   14.0   \n",
       "\n",
       "       RIGHT_RF_Mean_Coe  RIGHT_RF_Min_Coe  RIGHT_RF_Mean_Scale  \\\n",
       "10948           0.264891          0.133763            18.165906   \n",
       "10949           0.224715          0.147266            17.052881   \n",
       "10950           0.260783          0.131384            18.002591   \n",
       "10951           0.326726          0.137582            19.130992   \n",
       "10952           0.272874          0.127858            16.135058   \n",
       "...                  ...               ...                  ...   \n",
       "16101           0.272937          0.102111            16.223742   \n",
       "16102           0.197590          0.086402            16.462787   \n",
       "16103           0.167673          0.075130            14.920290   \n",
       "16104           0.176414          0.073928            16.442348   \n",
       "16105           0.209978          0.072147            17.113561   \n",
       "\n",
       "       RIGHT_RF_Median_Scale  \n",
       "10948                   17.0  \n",
       "10949                   15.0  \n",
       "10950                   17.0  \n",
       "10951                   19.0  \n",
       "10952                   13.0  \n",
       "...                      ...  \n",
       "16101                   14.0  \n",
       "16102                   14.0  \n",
       "16103                   12.0  \n",
       "16104                   14.0  \n",
       "16105                   15.0  \n",
       "\n",
       "[931 rows x 256 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 256\n",
    "threshold_WAMP = 30\n",
    "threshold_ZC = 0\n",
    "threshold_SSC = 1\n",
    "bins=9\n",
    "bound = 70\n",
    "HIST_range = (-bound,bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './processed data/data_set_after_window_S64_withoutSC_allPa.hdf5'\n",
    "with h5py.File(path,'r') as f:\n",
    "    x = f['cwt_data'][...]\n",
    "    y = f['label2'][...]\n",
    "feature = dp.generate_feature(x,threshold_WAMP=threshold_WAMP,\n",
    "                              threshold_ZC=threshold_ZC,\n",
    "                              threshold_SSC=threshold_SSC,\n",
    "                              bins=bins,ranges=HIST_range)\n",
    "#feature2 = dp.generate_feature(x2)\n",
    "player.Beep(1000,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./processed data/nfeatures_W256_S64_WAMP30.hdf5','r') as f:\n",
    "    feature = f['features'][...]\n",
    "    y = f['labels'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature2,y2 = dp.pipeline_feature(path2,width=256,stride=64,\n",
    "                                  scaler=False,\n",
    "                                  threshold_WAMP=threshold_WAMP,\n",
    "                                  threshold_ZC=threshold_ZC,\n",
    "                                  threshold_SSC=threshold_SSC,\n",
    "                                  bins=bins,ranges=HIST_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./processed data/nfeatures_rest_W256_S64.hdf5','r') as f:\n",
    "    feature2 = f['features'][...]\n",
    "    y2 = f['labels'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler(with_mean=True)\n",
    "feature_sc = sc.fit_transform(feature)\n",
    "feature2_sc = sc.transform(feature2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,Model,callbacks,regularizers\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE,BorderlineSMOTE,ADASYN,SVMSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,feature,y,binary=True,file=None):\n",
    "    if binary:\n",
    "        ind = ((y==0)|(y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "        ind1 = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "        y_01 = y.copy()\n",
    "        y_01[ind1] = 1\n",
    "        oc = OneHotEncoder()\n",
    "        y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "        cw = None#{0:1,1:5}\n",
    "    else:\n",
    "        ind = ((y==1)|(y==2)|(y==6))\n",
    "        #ind = ((y==1)|(y==6))\n",
    "        y_01 = y[ind].copy()\n",
    "        oc = OneHotEncoder()\n",
    "        y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "        cw = None#{0:5,1:1,2:1}#{0:2,1:1,2:10,3:2}\n",
    "    x_full,x_test,y_full,y_test = train_test_split(np.array(feature)[ind,:],y_01,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=345,\n",
    "                                                   shuffle=True)\n",
    "    x_train,x_valid,y_train,y_valid = train_test_split(x_full,y_full,\n",
    "                                                       test_size=0.25,\n",
    "                                                       random_state=555,\n",
    "                                                       shuffle=True)\n",
    "    \n",
    "    #sm = BorderlineSMOTE(random_state=50,kind='borderline-1')\n",
    "    #sm = SMOTE(random_state=50)\n",
    "    #print(y_full.shape)\n",
    "    #x_full,y_full = sm.fit_resample(x_full,y_full)\n",
    "    #print(y_full_n.shape)\n",
    "    sc = StandardScaler(with_mean=True)\n",
    "    x_train = sc.fit_transform(x_full)\n",
    "    #pca = PCA(n_components=200)\n",
    "    #x_train = pca.fit_transform(x_train)\n",
    "    #x_valid = sc.transform(x_valid)\n",
    "    x_test = sc.transform(x_test)\n",
    "    #x_test = pca.transform(x_test)\n",
    "    #x_train = x_full\n",
    "    \n",
    "    early_stopping = callbacks.EarlyStopping(patience = 10,\n",
    "                                             monitor = 'val_accuracy', \n",
    "                                             restore_best_weights=True)\n",
    "    history = model.fit(x_train,y_full,validation_data=[x_test,y_test],\n",
    "                        epochs=200,batch_size=32,class_weight=cw,\n",
    "                        callbacks=[early_stopping],\n",
    "                        shuffle=True)\n",
    "    \n",
    "    if binary:\n",
    "        y_pred_t=model.predict(x_test)\n",
    "        test = metrics.accuracy_score(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1))\n",
    "        #test = metrics.accuracy_score(y_test,y_pred_t>0.5)\n",
    "        \n",
    "        #y_pred_v=model.predict(x_valid)\n",
    "        #valid = metrics.accuracy_score(y_valid,np.argmax(y_pred_v,axis=1))\n",
    "        y_pred_ta=model.predict(x_train)\n",
    "        train = metrics.accuracy_score(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1))\n",
    "        #train = metrics.accuracy_score(y_full_n,y_pred_ta>0.5)\n",
    "        \n",
    "        print('train: \\n',metrics.confusion_matrix(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1)))\n",
    "        #print('valid: \\n',metrics.confusion_matrix(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1)))\n",
    "        print('test: \\n',metrics.confusion_matrix(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1)))\n",
    "        \n",
    "        #print('train: \\n',metrics.confusion_matrix(y_full_n,y_pred_ta>0.5))\n",
    "        #print('test: \\n',metrics.confusion_matrix(y_test,y_pred_t>0.5))\n",
    "\n",
    "    else:\n",
    "        y_pred_t=model.predict(x_test)\n",
    "        test = metrics.accuracy_score(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1))\n",
    "        #y_pred_v=model.predict(x_valid)\n",
    "        #valid = metrics.accuracy_score(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1))\n",
    "        y_pred_ta=model.predict(x_train)\n",
    "        train = metrics.accuracy_score(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1))        \n",
    "        print('train: \\n',metrics.confusion_matrix(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1)))\n",
    "        #print('valid: \\n',metrics.confusion_matrix(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1)))\n",
    "        print('test: \\n',metrics.confusion_matrix(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1)))\n",
    "    print('test:%f'%test)\n",
    "    #print('valid:%f'%valid)\n",
    "    print('train:%f'%train)\n",
    "    if file != None:\n",
    "        model.save_model(file)\n",
    "    return train,test,sc\n",
    "\n",
    "def test_model(model,feature,y,sc):\n",
    "    ind = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "    #ind = ((y==0)|(y==1)|(y==2)|(y==6))\n",
    "    y_01 = y.copy()\n",
    "    y_01[ind] = 1\n",
    "    #print(set(y_01))\n",
    "    oh_ec = OneHotEncoder()\n",
    "    y_01 = oh_ec.fit_transform(y_01[:,np.newaxis]).toarray()\n",
    "    #print(y_01)\n",
    "    y_pred=model.predict(sc.transform(feature))\n",
    "    test = metrics.accuracy_score(np.argmax(y_01,axis=1),np.argmax(y_pred,axis=1))\n",
    "    #test = metrics.accuracy_score(y_01,y_pred>0.5)\n",
    "    \n",
    "    print('acc:%f'%test)\n",
    "    print(metrics.confusion_matrix(np.argmax(y_01,axis=1),np.argmax(y_pred,axis=1)))\n",
    "    #print(metrics.confusion_matrix(y_01,y_pred>0.5))\n",
    "    return test\n",
    "\n",
    "def sparse_cost_sensitive_loss (y_true,y_pred):\n",
    "    #cost_matrix = tf.constant([[0,1.,1,1.],\n",
    "    #              [2,0,5,5],\n",
    "    #              [1,1,0,1],\n",
    "    #              [1.,2.,1,0]])\n",
    "    #cost_matrix = tf.constant([[0,5.,1],\n",
    "    #              [1,0,1],\n",
    "    #              [1.5,2.,0]])\n",
    "    cost_matrix = tf.constant([[0,2.],\n",
    "                  [5.,0]])\n",
    "    batch_cost_matrix = tf.nn.embedding_lookup(cost_matrix, tf.argmax(y_true,axis=1))\n",
    "    eps = 1e-6\n",
    "    probability = tf.clip_by_value(y_pred, eps, 1-eps)\n",
    "    cost_values = tf.math.log(1-probability)*batch_cost_matrix\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(cost_values, axis=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = ((y==1)|(y==2)|(y==3)|(y==6))\n",
    "#ind = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "ind_f = [0,1,6,42,46,57,62]\n",
    "#y_01 = y[ind].copy()\n",
    "#y_01[y_01==1]=0\n",
    "#y_01[y_01==2]=1\n",
    "#y_01[y_01==3]=2\n",
    "#y_01[y_01==6]=3\n",
    "y_01 = y[ind].copy()\n",
    "#y_01[ind] = 1\n",
    "oh_ec = OneHotEncoder()\n",
    "y_oh = oh_ec.fit_transform(y_01[:,np.newaxis]).toarray()\n",
    "x_full,x_test,y_full,y_test = train_test_split(feature.loc[ind,:],y_01,test_size=0.2,random_state=123,shuffle=False)\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(x_full,y_full,test_size=0.2,random_state=555,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = layers.Input(shape=feature.shape[1:])\n",
    "l1 = layers.Dense(128,activation='elu',\n",
    "                  #kernel_initializer='lecun_normal',\n",
    "                  #kernel_regularizer = regularizers.l2(0.001),\n",
    "                 )(input_)\n",
    "drop1 = layers.Dropout(0.2)(l1)\n",
    "l2 = layers.Dense(64,activation='elu',\n",
    "                  #kernel_initializer='lecun_normal',\n",
    "                  #kernel_regularizer = regularizers.l2(0.001),\n",
    "                 )(drop1)\n",
    "drop2 = layers.Dropout(0.2)(l2)\n",
    "l3 = layers.Dense(32,activation='elu',\n",
    "                  #kernel_initializer='lecun_normal',\n",
    "                  #kernel_regularizer = regularizers.l2(0.001),\n",
    "                 )(drop2)\n",
    "drop3 = layers.Dropout(0.2)(l3)\n",
    "l4 = layers.Dense(16,activation='elu')(drop3)\n",
    "drop4 = layers.Dropout(0.2)(l4)\n",
    "#l5 = layers.Dense(8,activation='relu')(drop4)\n",
    "#drop5 = layers.Dropout(0.5)(l5)\n",
    "output = layers.Dense(2,activation='softmax')(drop4)\n",
    "model = Model(inputs=[input_],outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=sparse_cost_sensitive_loss,optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = callbacks.EarlyStopping(patience = 20,\n",
    "                                         monitor = 'val_accuracy', \n",
    "                                         restore_best_weights=True)\n",
    "history = model.fit(x_train,y_train,validation_data=[x_valid,y_valid],\n",
    "                    epochs=200,batch_size=50,\n",
    "                   callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 66288 samples, validate on 16573 samples\n",
      "Epoch 1/200\n",
      "66288/66288 [==============================] - 7s 103us/sample - loss: 0.4466 - accuracy: 0.9404 - val_loss: 0.2340 - val_accuracy: 0.9690\n",
      "Epoch 2/200\n",
      "66288/66288 [==============================] - 6s 93us/sample - loss: 0.2736 - accuracy: 0.9650 - val_loss: 0.1943 - val_accuracy: 0.9736\n",
      "Epoch 3/200\n",
      "66288/66288 [==============================] - 6s 87us/sample - loss: 0.2347 - accuracy: 0.9693 - val_loss: 0.1767 - val_accuracy: 0.9764\n",
      "Epoch 4/200\n",
      "66288/66288 [==============================] - 6s 86us/sample - loss: 0.2087 - accuracy: 0.9726 - val_loss: 0.1600 - val_accuracy: 0.9777\n",
      "Epoch 5/200\n",
      "66288/66288 [==============================] - 6s 86us/sample - loss: 0.1946 - accuracy: 0.9741 - val_loss: 0.1602 - val_accuracy: 0.9770\n",
      "Epoch 6/200\n",
      "66288/66288 [==============================] - 6s 95us/sample - loss: 0.1794 - accuracy: 0.9762 - val_loss: 0.1543 - val_accuracy: 0.9774\n",
      "Epoch 7/200\n",
      "66288/66288 [==============================] - 7s 110us/sample - loss: 0.1627 - accuracy: 0.9782 - val_loss: 0.1403 - val_accuracy: 0.9817\n",
      "Epoch 8/200\n",
      "66288/66288 [==============================] - 6s 96us/sample - loss: 0.1553 - accuracy: 0.9794 - val_loss: 0.1493 - val_accuracy: 0.9791\n",
      "Epoch 9/200\n",
      "66288/66288 [==============================] - 8s 115us/sample - loss: 0.1459 - accuracy: 0.9801 - val_loss: 0.1394 - val_accuracy: 0.9817\n",
      "Epoch 10/200\n",
      "66288/66288 [==============================] - 7s 106us/sample - loss: 0.1382 - accuracy: 0.9807 - val_loss: 0.1416 - val_accuracy: 0.9811\n",
      "Epoch 11/200\n",
      "66288/66288 [==============================] - 7s 110us/sample - loss: 0.1358 - accuracy: 0.9811 - val_loss: 0.1256 - val_accuracy: 0.9812\n",
      "Epoch 12/200\n",
      "66288/66288 [==============================] - 8s 114us/sample - loss: 0.1242 - accuracy: 0.9828 - val_loss: 0.1238 - val_accuracy: 0.9826\n",
      "Epoch 13/200\n",
      "66288/66288 [==============================] - 8s 116us/sample - loss: 0.1242 - accuracy: 0.9831 - val_loss: 0.1153 - val_accuracy: 0.9848\n",
      "Epoch 14/200\n",
      "66288/66288 [==============================] - 8s 115us/sample - loss: 0.1150 - accuracy: 0.9839 - val_loss: 0.1130 - val_accuracy: 0.9841\n",
      "Epoch 15/200\n",
      "66288/66288 [==============================] - 7s 105us/sample - loss: 0.1110 - accuracy: 0.9846 - val_loss: 0.1292 - val_accuracy: 0.9851\n",
      "Epoch 16/200\n",
      "66288/66288 [==============================] - 7s 110us/sample - loss: 0.1060 - accuracy: 0.9855 - val_loss: 0.1268 - val_accuracy: 0.9843\n",
      "Epoch 17/200\n",
      "66288/66288 [==============================] - 7s 108us/sample - loss: 0.1052 - accuracy: 0.9853 - val_loss: 0.1184 - val_accuracy: 0.9854\n",
      "Epoch 18/200\n",
      "66288/66288 [==============================] - 6s 96us/sample - loss: 0.0993 - accuracy: 0.9867 - val_loss: 0.1456 - val_accuracy: 0.9861\n",
      "Epoch 19/200\n",
      "66288/66288 [==============================] - 6s 96us/sample - loss: 0.0976 - accuracy: 0.9867 - val_loss: 0.1370 - val_accuracy: 0.9865\n",
      "Epoch 20/200\n",
      "66288/66288 [==============================] - 7s 102us/sample - loss: 0.0942 - accuracy: 0.9871 - val_loss: 0.1227 - val_accuracy: 0.9853\n",
      "Epoch 21/200\n",
      "66288/66288 [==============================] - 6s 95us/sample - loss: 0.0871 - accuracy: 0.9875 - val_loss: 0.1246 - val_accuracy: 0.9873\n",
      "Epoch 22/200\n",
      "66288/66288 [==============================] - 6s 95us/sample - loss: 0.0888 - accuracy: 0.9875 - val_loss: 0.1072 - val_accuracy: 0.9846\n",
      "Epoch 23/200\n",
      "66288/66288 [==============================] - 6s 89us/sample - loss: 0.0860 - accuracy: 0.9877 - val_loss: 0.1145 - val_accuracy: 0.9854\n",
      "Epoch 24/200\n",
      "66288/66288 [==============================] - 6s 89us/sample - loss: 0.0759 - accuracy: 0.9894 - val_loss: 0.1021 - val_accuracy: 0.9854\n",
      "Epoch 25/200\n",
      "66288/66288 [==============================] - 6s 89us/sample - loss: 0.0791 - accuracy: 0.9886 - val_loss: 0.1102 - val_accuracy: 0.9870\n",
      "Epoch 26/200\n",
      "66288/66288 [==============================] - 6s 90us/sample - loss: 0.0730 - accuracy: 0.9901 - val_loss: 0.0995 - val_accuracy: 0.9874\n",
      "Epoch 27/200\n",
      "66288/66288 [==============================] - 6s 91us/sample - loss: 0.0750 - accuracy: 0.9900 - val_loss: 0.1012 - val_accuracy: 0.9868\n",
      "Epoch 28/200\n",
      "66288/66288 [==============================] - 7s 102us/sample - loss: 0.0725 - accuracy: 0.9901 - val_loss: 0.1188 - val_accuracy: 0.9877\n",
      "Epoch 29/200\n",
      "66288/66288 [==============================] - 7s 99us/sample - loss: 0.0705 - accuracy: 0.9902 - val_loss: 0.0962 - val_accuracy: 0.9884\n",
      "Epoch 30/200\n",
      "66288/66288 [==============================] - 6s 97us/sample - loss: 0.0665 - accuracy: 0.9907 - val_loss: 0.1339 - val_accuracy: 0.9880\n",
      "Epoch 31/200\n",
      "66288/66288 [==============================] - 6s 96us/sample - loss: 0.0690 - accuracy: 0.9908 - val_loss: 0.1166 - val_accuracy: 0.9881\n",
      "Epoch 32/200\n",
      "66288/66288 [==============================] - 6s 95us/sample - loss: 0.0615 - accuracy: 0.9915 - val_loss: 0.1079 - val_accuracy: 0.9876\n",
      "Epoch 33/200\n",
      "66288/66288 [==============================] - 6s 90us/sample - loss: 0.0634 - accuracy: 0.9917 - val_loss: 0.0962 - val_accuracy: 0.9892\n",
      "Epoch 34/200\n",
      "66288/66288 [==============================] - 8s 116us/sample - loss: 0.0565 - accuracy: 0.9920 - val_loss: 0.1026 - val_accuracy: 0.9880\n",
      "Epoch 35/200\n",
      "66288/66288 [==============================] - 6s 90us/sample - loss: 0.0607 - accuracy: 0.9918 - val_loss: 0.0946 - val_accuracy: 0.9893\n",
      "Epoch 36/200\n",
      "66288/66288 [==============================] - 6s 86us/sample - loss: 0.0594 - accuracy: 0.9922 - val_loss: 0.0987 - val_accuracy: 0.9893\n",
      "Epoch 37/200\n",
      "66288/66288 [==============================] - 6s 85us/sample - loss: 0.0566 - accuracy: 0.9924 - val_loss: 0.1208 - val_accuracy: 0.9898\n",
      "Epoch 38/200\n",
      "66288/66288 [==============================] - 6s 86us/sample - loss: 0.0505 - accuracy: 0.9929 - val_loss: 0.1193 - val_accuracy: 0.9903\n",
      "Epoch 39/200\n",
      "66288/66288 [==============================] - 6s 88us/sample - loss: 0.0512 - accuracy: 0.9931 - val_loss: 0.1022 - val_accuracy: 0.9886\n",
      "Epoch 40/200\n",
      "66288/66288 [==============================] - 6s 90us/sample - loss: 0.0496 - accuracy: 0.9930 - val_loss: 0.1208 - val_accuracy: 0.9897\n",
      "Epoch 41/200\n",
      "66288/66288 [==============================] - 6s 92us/sample - loss: 0.0505 - accuracy: 0.9932 - val_loss: 0.1108 - val_accuracy: 0.9902\n",
      "Epoch 42/200\n",
      "66288/66288 [==============================] - 6s 94us/sample - loss: 0.0509 - accuracy: 0.9932 - val_loss: 0.0893 - val_accuracy: 0.9900\n",
      "Epoch 43/200\n",
      "66288/66288 [==============================] - 6s 90us/sample - loss: 0.0470 - accuracy: 0.9938 - val_loss: 0.0969 - val_accuracy: 0.9892\n",
      "Epoch 44/200\n",
      "66288/66288 [==============================] - 6s 92us/sample - loss: 0.0463 - accuracy: 0.9941 - val_loss: 0.1042 - val_accuracy: 0.9898\n",
      "Epoch 45/200\n",
      "66288/66288 [==============================] - 6s 92us/sample - loss: 0.0442 - accuracy: 0.9946 - val_loss: 0.1192 - val_accuracy: 0.9889\n",
      "Epoch 46/200\n",
      "66288/66288 [==============================] - 6s 96us/sample - loss: 0.0453 - accuracy: 0.9942 - val_loss: 0.1150 - val_accuracy: 0.9905\n",
      "Epoch 47/200\n",
      "66288/66288 [==============================] - 6s 91us/sample - loss: 0.0511 - accuracy: 0.9936 - val_loss: 0.0998 - val_accuracy: 0.9901\n",
      "Epoch 48/200\n",
      "66288/66288 [==============================] - 6s 90us/sample - loss: 0.0440 - accuracy: 0.9940 - val_loss: 0.0917 - val_accuracy: 0.9896\n",
      "Epoch 49/200\n",
      "66288/66288 [==============================] - 6s 92us/sample - loss: 0.0404 - accuracy: 0.9945 - val_loss: 0.1046 - val_accuracy: 0.9900\n",
      "Epoch 50/200\n",
      "66288/66288 [==============================] - 6s 95us/sample - loss: 0.0409 - accuracy: 0.9946 - val_loss: 0.0984 - val_accuracy: 0.9905\n",
      "Epoch 51/200\n",
      "66288/66288 [==============================] - 6s 91us/sample - loss: 0.0414 - accuracy: 0.9946 - val_loss: 0.1111 - val_accuracy: 0.9886\n",
      "Epoch 52/200\n",
      "66288/66288 [==============================] - 6s 91us/sample - loss: 0.0412 - accuracy: 0.9947 - val_loss: 0.1129 - val_accuracy: 0.9900\n",
      "Epoch 53/200\n",
      "66288/66288 [==============================] - 7s 98us/sample - loss: 0.0423 - accuracy: 0.9945 - val_loss: 0.1082 - val_accuracy: 0.9900\n",
      "Epoch 54/200\n",
      "66288/66288 [==============================] - 6s 96us/sample - loss: 0.0407 - accuracy: 0.9950 - val_loss: 0.0883 - val_accuracy: 0.9898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/200\n",
      "66288/66288 [==============================] - 6s 90us/sample - loss: 0.0392 - accuracy: 0.9954 - val_loss: 0.1117 - val_accuracy: 0.9902\n",
      "Epoch 56/200\n",
      "66288/66288 [==============================] - 6s 86us/sample - loss: 0.0368 - accuracy: 0.9952 - val_loss: 0.1217 - val_accuracy: 0.9900\n",
      "train: \n",
      " [[55390   115]\n",
      " [    0 10783]]\n",
      "test: \n",
      " [[13714   110]\n",
      " [   47  2702]]\n",
      "test:0.990527\n",
      "train:0.998265\n"
     ]
    }
   ],
   "source": [
    "#sc = StandardScaler(with_mean=True)\n",
    "#feature_sc = sc.fit_transform(feature)\n",
    "#feature2_sc = sc.transform(feature2)\n",
    "#pca = PCA(n_components=160,copy=True)\n",
    "#feature_pca = pca.fit_transform(feature)\n",
    "#feature2_pca = pca.transform(feature2)\n",
    "train,test,sc = train_model(model,feature,np.array(y),True)\n",
    "#rest = test_model(model,feature2,np.array(y2),sc)\n",
    "#acc = [train,valid,test,rest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.864530\n",
      "[[535 115]\n",
      " [ 71 652]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.864530225782957"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model,np.array(feature2),np.array(y2),sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    460\n",
       "2    430\n",
       "1     33\n",
       "3      8\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y2).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model(model,feature2_sc,np.array(y2))\n",
    "ind = ((y==1)|(y==2)|(y==6))\n",
    "y_01 = y[ind].copy()\n",
    "oc = OneHotEncoder()\n",
    "y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "pred=model.predict(feature_sc[ind,:])\n",
    "np.argmax(pred,axis=1)\n",
    "np.argmax(y_01,axis=1)\n",
    "metrics.confusion_matrix(np.argmax(y_01,axis=1),np.argmax(pred,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc_ann = pd.DataFrame(acc,columns=['dnn1'],index=['train','valid','test','rest data'])\n",
    "#acc_ann_com=pd.concat([acc_ann_com,acc_ann.T])\n",
    "#acc_ann.loc['drop_mDWT',:]=[train,valid,test,rest]\n",
    "acc_ann_com.loc['dnn2',:]=acc\n",
    "acc_ann_com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test feature from Left or Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = layers.Input(shape=feature.loc[:,ind_temp].shape[1:])\n",
    "l1 = layers.Dense(128,activation='elu')(input_)\n",
    "drop1 = layers.Dropout(0.2)(l1)\n",
    "l2 = layers.Dense(64,activation='elu')(drop1)\n",
    "drop2 = layers.Dropout(0.2)(l2)\n",
    "#l3 = layers.Dense(32,activation='elu')(drop2)\n",
    "#drop3 = layers.Dropout(0.2)(l3)\n",
    "#l4 = layers.Dense(16,activation='selu')(l3)\n",
    "#drop4 = layers.Dropout(0.2)(l4)\n",
    "output = layers.Dense(1,activation='sigmoid')(drop2)\n",
    "model = Model(inputs=[input_],outputs=[output])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23125 samples, validate on 5782 samples\n",
      "Epoch 1/200\n",
      "23125/23125 [==============================] - 4s 167us/sample - loss: 1.9657 - accuracy: 0.5308 - val_loss: 1.3537 - val_accuracy: 0.6404\n",
      "Epoch 2/200\n",
      "23125/23125 [==============================] - 3s 109us/sample - loss: 1.3913 - accuracy: 0.6479 - val_loss: 1.1555 - val_accuracy: 0.7008\n",
      "Epoch 3/200\n",
      "23125/23125 [==============================] - 3s 110us/sample - loss: 1.2045 - accuracy: 0.7052 - val_loss: 1.0439 - val_accuracy: 0.7771\n",
      "Epoch 4/200\n",
      "23125/23125 [==============================] - 2s 105us/sample - loss: 1.1076 - accuracy: 0.7394 - val_loss: 0.9467 - val_accuracy: 0.7608\n",
      "Epoch 5/200\n",
      "23125/23125 [==============================] - 2s 86us/sample - loss: 1.0526 - accuracy: 0.7501 - val_loss: 0.9212 - val_accuracy: 0.7508\n",
      "Epoch 6/200\n",
      "23125/23125 [==============================] - 2s 93us/sample - loss: 0.9990 - accuracy: 0.7677 - val_loss: 0.8937 - val_accuracy: 0.7975\n",
      "Epoch 7/200\n",
      "23125/23125 [==============================] - 2s 98us/sample - loss: 0.9561 - accuracy: 0.7806 - val_loss: 0.8888 - val_accuracy: 0.7916\n",
      "Epoch 8/200\n",
      "23125/23125 [==============================] - 2s 85us/sample - loss: 0.9064 - accuracy: 0.8004 - val_loss: 0.8675 - val_accuracy: 0.8127\n",
      "Epoch 9/200\n",
      "23125/23125 [==============================] - 2s 96us/sample - loss: 0.8361 - accuracy: 0.8109 - val_loss: 0.9959 - val_accuracy: 0.8388\n",
      "Epoch 10/200\n",
      "23125/23125 [==============================] - 2s 91us/sample - loss: 0.8394 - accuracy: 0.8178 - val_loss: 0.8477 - val_accuracy: 0.8080\n",
      "Epoch 11/200\n",
      "23125/23125 [==============================] - 2s 89us/sample - loss: 0.8182 - accuracy: 0.8169 - val_loss: 0.8487 - val_accuracy: 0.8193\n",
      "Epoch 12/200\n",
      "23125/23125 [==============================] - 2s 87us/sample - loss: 0.7716 - accuracy: 0.8304 - val_loss: 1.0611 - val_accuracy: 0.8345\n",
      "Epoch 13/200\n",
      "23125/23125 [==============================] - 2s 87us/sample - loss: 0.8293 - accuracy: 0.8183 - val_loss: 0.9155 - val_accuracy: 0.8390\n",
      "Epoch 14/200\n",
      "23125/23125 [==============================] - 2s 82us/sample - loss: 0.8102 - accuracy: 0.8220 - val_loss: 0.8263 - val_accuracy: 0.8255\n",
      "Epoch 15/200\n",
      "23125/23125 [==============================] - 2s 82us/sample - loss: 0.7460 - accuracy: 0.8396 - val_loss: 0.7475 - val_accuracy: 0.8295\n",
      "Epoch 16/200\n",
      "23125/23125 [==============================] - 3s 109us/sample - loss: 0.7118 - accuracy: 0.8471 - val_loss: 0.8684 - val_accuracy: 0.8412\n",
      "Epoch 17/200\n",
      "23125/23125 [==============================] - 3s 135us/sample - loss: 0.7334 - accuracy: 0.8410 - val_loss: 0.8593 - val_accuracy: 0.8258\n",
      "Epoch 18/200\n",
      "23125/23125 [==============================] - 3s 125us/sample - loss: 0.7021 - accuracy: 0.8490 - val_loss: 0.8953 - val_accuracy: 0.8506\n",
      "Epoch 19/200\n",
      "23125/23125 [==============================] - 3s 121us/sample - loss: 0.6847 - accuracy: 0.8514 - val_loss: 0.9428 - val_accuracy: 0.8566\n",
      "Epoch 20/200\n",
      "23125/23125 [==============================] - 3s 133us/sample - loss: 0.7248 - accuracy: 0.8508 - val_loss: 0.9025 - val_accuracy: 0.8392\n",
      "Epoch 21/200\n",
      "23125/23125 [==============================] - 3s 118us/sample - loss: 0.7017 - accuracy: 0.8458 - val_loss: 0.8396 - val_accuracy: 0.8525\n",
      "Epoch 22/200\n",
      "23125/23125 [==============================] - 3s 110us/sample - loss: 0.6808 - accuracy: 0.8589 - val_loss: 0.8343 - val_accuracy: 0.8454\n",
      "Epoch 23/200\n",
      "23125/23125 [==============================] - 3s 111us/sample - loss: 0.6569 - accuracy: 0.8586 - val_loss: 0.9713 - val_accuracy: 0.8646\n",
      "Epoch 24/200\n",
      "23125/23125 [==============================] - 2s 91us/sample - loss: 0.7113 - accuracy: 0.8530 - val_loss: 0.9063 - val_accuracy: 0.8663\n",
      "Epoch 25/200\n",
      "23125/23125 [==============================] - 2s 87us/sample - loss: 0.6886 - accuracy: 0.8540 - val_loss: 1.0114 - val_accuracy: 0.8729\n",
      "Epoch 26/200\n",
      "23125/23125 [==============================] - 2s 83us/sample - loss: 0.6437 - accuracy: 0.8703 - val_loss: 1.0360 - val_accuracy: 0.8737\n",
      "Epoch 27/200\n",
      "23125/23125 [==============================] - 2s 82us/sample - loss: 0.6396 - accuracy: 0.8684 - val_loss: 1.0674 - val_accuracy: 0.8762\n",
      "Epoch 28/200\n",
      "23125/23125 [==============================] - 2s 87us/sample - loss: 0.5848 - accuracy: 0.8806 - val_loss: 1.3498 - val_accuracy: 0.8853\n",
      "Epoch 29/200\n",
      "23125/23125 [==============================] - 2s 79us/sample - loss: 0.6586 - accuracy: 0.8602 - val_loss: 0.9367 - val_accuracy: 0.8565\n",
      "Epoch 30/200\n",
      "23125/23125 [==============================] - 2s 75us/sample - loss: 0.6776 - accuracy: 0.8599 - val_loss: 1.0857 - val_accuracy: 0.8727\n",
      "Epoch 31/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.6196 - accuracy: 0.8752 - val_loss: 1.0133 - val_accuracy: 0.8807\n",
      "Epoch 32/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.6562 - accuracy: 0.8634 - val_loss: 0.8697 - val_accuracy: 0.8708\n",
      "Epoch 33/200\n",
      "23125/23125 [==============================] - 2s 73us/sample - loss: 0.5973 - accuracy: 0.8750 - val_loss: 0.9950 - val_accuracy: 0.8753\n",
      "Epoch 34/200\n",
      "23125/23125 [==============================] - 2s 73us/sample - loss: 0.5699 - accuracy: 0.8852 - val_loss: 0.9541 - val_accuracy: 0.8786\n",
      "Epoch 35/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.5811 - accuracy: 0.8883 - val_loss: 1.2480 - val_accuracy: 0.8967\n",
      "Epoch 36/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.6094 - accuracy: 0.8775 - val_loss: 0.9992 - val_accuracy: 0.8833\n",
      "Epoch 37/200\n",
      "23125/23125 [==============================] - 2s 80us/sample - loss: 0.5736 - accuracy: 0.8909 - val_loss: 1.0015 - val_accuracy: 0.8864\n",
      "Epoch 38/200\n",
      "23125/23125 [==============================] - 2s 77us/sample - loss: 0.5681 - accuracy: 0.8930 - val_loss: 0.8801 - val_accuracy: 0.8717\n",
      "Epoch 39/200\n",
      "23125/23125 [==============================] - 2s 77us/sample - loss: 0.5511 - accuracy: 0.8872 - val_loss: 1.2918 - val_accuracy: 0.8933\n",
      "Epoch 40/200\n",
      "23125/23125 [==============================] - 2s 74us/sample - loss: 0.5846 - accuracy: 0.8878 - val_loss: 1.0802 - val_accuracy: 0.8919\n",
      "Epoch 41/200\n",
      "23125/23125 [==============================] - 2s 83us/sample - loss: 0.5913 - accuracy: 0.8886 - val_loss: 0.7979 - val_accuracy: 0.8649\n",
      "Epoch 42/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.5855 - accuracy: 0.8799 - val_loss: 0.9434 - val_accuracy: 0.8912\n",
      "Epoch 43/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.5320 - accuracy: 0.8915 - val_loss: 1.4377 - val_accuracy: 0.8942\n",
      "Epoch 44/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.5590 - accuracy: 0.8926 - val_loss: 1.2363 - val_accuracy: 0.8945\n",
      "Epoch 45/200\n",
      "23125/23125 [==============================] - 2s 85us/sample - loss: 0.5660 - accuracy: 0.8974 - val_loss: 0.9355 - val_accuracy: 0.8770\n",
      "train: \n",
      " [[13662  1921]\n",
      " [    0  7542]]\n",
      "test: \n",
      " [[3359  562]\n",
      " [  35 1826]]\n",
      "test:0.896749\n",
      "train:0.916930\n",
      "[0 0 0 ... 0 0 0]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "acc:0.248125\n",
      "[[ 430    0]\n",
      " [1303    0]]\n",
      "0.24812463935372187\n",
      "Train on 23125 samples, validate on 5782 samples\n",
      "Epoch 1/200\n",
      "23125/23125 [==============================] - 2s 83us/sample - loss: 1.6699 - accuracy: 0.5641 - val_loss: 1.1863 - val_accuracy: 0.6956\n",
      "Epoch 2/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 1.2186 - accuracy: 0.6931 - val_loss: 1.0834 - val_accuracy: 0.7371\n",
      "Epoch 3/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 1.0666 - accuracy: 0.7425 - val_loss: 1.0060 - val_accuracy: 0.7791\n",
      "Epoch 4/200\n",
      "23125/23125 [==============================] - 2s 73us/sample - loss: 1.0038 - accuracy: 0.7606 - val_loss: 1.0101 - val_accuracy: 0.7757\n",
      "Epoch 5/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 1.0121 - accuracy: 0.7549 - val_loss: 0.9530 - val_accuracy: 0.7598\n",
      "Epoch 6/200\n",
      "23125/23125 [==============================] - 2s 75us/sample - loss: 0.8796 - accuracy: 0.7876 - val_loss: 1.0161 - val_accuracy: 0.8279\n",
      "Epoch 7/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.8478 - accuracy: 0.8073 - val_loss: 0.9871 - val_accuracy: 0.8248\n",
      "Epoch 8/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.8517 - accuracy: 0.8045 - val_loss: 0.8904 - val_accuracy: 0.8210\n",
      "Epoch 9/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.8091 - accuracy: 0.8131 - val_loss: 1.0212 - val_accuracy: 0.8426\n",
      "Epoch 10/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.7750 - accuracy: 0.8260 - val_loss: 1.0699 - val_accuracy: 0.8438\n",
      "Epoch 11/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.7591 - accuracy: 0.8365 - val_loss: 0.9963 - val_accuracy: 0.8513\n",
      "Epoch 12/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.7071 - accuracy: 0.8416 - val_loss: 1.0759 - val_accuracy: 0.8464\n",
      "Epoch 13/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.6955 - accuracy: 0.8444 - val_loss: 1.0100 - val_accuracy: 0.8442\n",
      "Epoch 14/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.7017 - accuracy: 0.8478 - val_loss: 1.0484 - val_accuracy: 0.8539\n",
      "Epoch 15/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.7070 - accuracy: 0.8503 - val_loss: 0.9769 - val_accuracy: 0.8648\n",
      "Epoch 16/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.6993 - accuracy: 0.8527 - val_loss: 0.8596 - val_accuracy: 0.8388\n",
      "Epoch 17/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.6776 - accuracy: 0.8485 - val_loss: 1.0544 - val_accuracy: 0.8644\n",
      "Epoch 18/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.6821 - accuracy: 0.8598 - val_loss: 0.9780 - val_accuracy: 0.8658\n",
      "Epoch 19/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.6573 - accuracy: 0.8584 - val_loss: 1.0270 - val_accuracy: 0.8687\n",
      "Epoch 20/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.6057 - accuracy: 0.8704 - val_loss: 1.1063 - val_accuracy: 0.8692\n",
      "Epoch 21/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.6138 - accuracy: 0.8713 - val_loss: 0.9826 - val_accuracy: 0.8469\n",
      "Epoch 22/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.6390 - accuracy: 0.8652 - val_loss: 1.1678 - val_accuracy: 0.8756\n",
      "Epoch 23/200\n",
      "23125/23125 [==============================] - 2s 75us/sample - loss: 0.6374 - accuracy: 0.8704 - val_loss: 1.1649 - val_accuracy: 0.8637\n",
      "Epoch 24/200\n",
      "23125/23125 [==============================] - 2s 73us/sample - loss: 0.6625 - accuracy: 0.8615 - val_loss: 1.0139 - val_accuracy: 0.8644\n",
      "Epoch 25/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.6115 - accuracy: 0.8678 - val_loss: 0.9688 - val_accuracy: 0.8634\n",
      "Epoch 26/200\n",
      "23125/23125 [==============================] - 2s 74us/sample - loss: 0.5845 - accuracy: 0.8767 - val_loss: 1.3254 - val_accuracy: 0.8826\n",
      "Epoch 27/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.5496 - accuracy: 0.8895 - val_loss: 1.3573 - val_accuracy: 0.8796\n",
      "Epoch 28/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.6670 - accuracy: 0.8630 - val_loss: 0.9269 - val_accuracy: 0.8490\n",
      "Epoch 29/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.5767 - accuracy: 0.8770 - val_loss: 1.3259 - val_accuracy: 0.8819\n",
      "Epoch 30/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.6089 - accuracy: 0.8846 - val_loss: 1.0698 - val_accuracy: 0.8691\n",
      "Epoch 31/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.5645 - accuracy: 0.8845 - val_loss: 1.0752 - val_accuracy: 0.8822\n",
      "Epoch 32/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.6610 - accuracy: 0.8730 - val_loss: 0.9069 - val_accuracy: 0.8675\n",
      "Epoch 33/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.5818 - accuracy: 0.8803 - val_loss: 1.0766 - val_accuracy: 0.8833\n",
      "Epoch 34/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.5978 - accuracy: 0.8772 - val_loss: 1.0967 - val_accuracy: 0.8841\n",
      "Epoch 35/200\n",
      "23125/23125 [==============================] - 2s 83us/sample - loss: 0.5508 - accuracy: 0.8896 - val_loss: 1.0682 - val_accuracy: 0.8744\n",
      "Epoch 36/200\n",
      "23125/23125 [==============================] - 2s 81us/sample - loss: 0.5556 - accuracy: 0.8859 - val_loss: 1.1288 - val_accuracy: 0.8796\n",
      "Epoch 37/200\n",
      "23125/23125 [==============================] - 2s 79us/sample - loss: 0.5555 - accuracy: 0.8897 - val_loss: 1.0734 - val_accuracy: 0.8862\n",
      "Epoch 38/200\n",
      "23125/23125 [==============================] - 2s 73us/sample - loss: 0.5966 - accuracy: 0.8803 - val_loss: 0.9272 - val_accuracy: 0.8686\n",
      "Epoch 39/200\n",
      "23125/23125 [==============================] - 2s 75us/sample - loss: 0.5482 - accuracy: 0.8869 - val_loss: 1.3462 - val_accuracy: 0.8966\n",
      "Epoch 40/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.5505 - accuracy: 0.8925 - val_loss: 1.0365 - val_accuracy: 0.8893\n",
      "Epoch 41/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.5973 - accuracy: 0.8802 - val_loss: 0.9774 - val_accuracy: 0.8852\n",
      "Epoch 42/200\n",
      "23125/23125 [==============================] - 2s 81us/sample - loss: 0.5678 - accuracy: 0.8853 - val_loss: 1.2457 - val_accuracy: 0.8910\n",
      "Epoch 43/200\n",
      "23125/23125 [==============================] - 2s 84us/sample - loss: 0.5707 - accuracy: 0.8997 - val_loss: 1.3619 - val_accuracy: 0.8935\n",
      "Epoch 44/200\n",
      "23125/23125 [==============================] - 2s 75us/sample - loss: 0.5471 - accuracy: 0.8922 - val_loss: 1.0956 - val_accuracy: 0.8872\n",
      "Epoch 45/200\n",
      "23125/23125 [==============================] - 2s 74us/sample - loss: 0.5946 - accuracy: 0.8762 - val_loss: 1.0329 - val_accuracy: 0.8777\n",
      "Epoch 46/200\n",
      "23125/23125 [==============================] - 2s 74us/sample - loss: 0.5659 - accuracy: 0.8837 - val_loss: 0.9916 - val_accuracy: 0.8812\n",
      "Epoch 47/200\n",
      "23125/23125 [==============================] - 2s 74us/sample - loss: 0.5310 - accuracy: 0.8933 - val_loss: 0.9934 - val_accuracy: 0.8860\n",
      "Epoch 48/200\n",
      "23125/23125 [==============================] - 2s 73us/sample - loss: 0.5431 - accuracy: 0.8881 - val_loss: 1.0694 - val_accuracy: 0.8884\n",
      "Epoch 49/200\n",
      "23125/23125 [==============================] - 2s 73us/sample - loss: 0.5159 - accuracy: 0.8993 - val_loss: 1.1309 - val_accuracy: 0.8942\n",
      "train: \n",
      " [[13597  1986]\n",
      " [    0  7542]]\n",
      "test: \n",
      " [[3361  560]\n",
      " [  38 1823]]\n",
      "test:0.896576\n",
      "train:0.914119\n",
      "[0 0 0 ... 0 0 0]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "acc:0.248125\n",
      "[[ 430    0]\n",
      " [1303    0]]\n",
      "0.24812463935372187\n"
     ]
    }
   ],
   "source": [
    "acc={}\n",
    "cols = ['LEFT','RIGHT']\n",
    "\n",
    "#sc = StandardScaler(with_mean=True)\n",
    "\n",
    "for col in cols:\n",
    "    ind_temp=feature.columns.str.contains(col)\n",
    "    #feature_sc = sc.fit_transform(feature.loc[:,ind_temp])\n",
    "    #feature2_sc = sc.transform(feature2.loc[:,ind_temp])\n",
    "    train,test,sc = train_model(model,np.array(feature.loc[:,ind_temp]),np.array(y))\n",
    "    \n",
    "    acc_rest=test_model(model,np.array(feature2.loc[:,ind_temp]),np.array(y2),sc)\n",
    "    #acc[col] = [train,valid,test,acc_rest]\n",
    "    print(acc_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_lr=pd.DataFrame(acc,index=['train','valid','test','rest data'])#.to_csv('./results/acc_lr_ann.csv')\n",
    "#acc_com=pd.concat([acc_ann_com.drop(['dnn','dnn1'],'index'),acc_lr.T])\n",
    "acc_com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test features from 2 of 8 signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test features from 2 of 8 signals\n",
    "\n",
    "acc_2f={}\n",
    "cols = [ 'LEFT_TA','LEFT_TS','LEFT_BF', 'LEFT_RF',\n",
    "        'RIGHT_TA','RIGHT_TS','RIGHT_BF', 'RIGHT_RF']\n",
    "\n",
    "\n",
    "for p in combinations(cols[:4],2):\n",
    "    ind_temp=feature.columns.str.contains(p[0])| feature.columns.str.contains(p[1])\n",
    "    #feature_sc = sc.fit_transform(feature.loc[:,ind_temp])\n",
    "    #feature2_sc = sc.transform(feature2.loc[:,ind_temp])\n",
    "    train,valid,test,sc = train_model(model,np.array(feature.loc[:,ind_temp]),np.array(y))\n",
    "    acc_rest=test_model(model,np.array(feature2.loc[:,ind_temp]),np.array(y2),sc)\n",
    "    acc_2f[p[0]+'_'+p[1]] = [train,valid,test,acc_rest]\n",
    "    print(acc_rest)\n",
    "    \n",
    "for p in combinations(cols[4:],2):\n",
    "    ind_temp=feature.columns.str.contains(p[0])| feature.columns.str.contains(p[1])\n",
    "    #feature_sc = sc.fit_transform(feature.loc[:,ind_temp])\n",
    "    #feature2_sc = sc.transform(feature2.loc[:,ind_temp])\n",
    "    train,valid,test,sc = train_model(model,np.array(feature.loc[:,ind_temp]),np.array(y))\n",
    "    acc_rest=test_model(model,np.array(feature2.loc[:,ind_temp]),np.array(y2),sc)\n",
    "    acc_2f[p[0]+'_'+p[1]] = [train,valid,test,acc_rest]\n",
    "    print(acc_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc_2f_ann = pd.DataFrame(acc_2f,index=['train','valid','test','rest data']).T\n",
    "#acc_com = pd.concat([acc_com,acc_2f_ann])\n",
    "acc_com.to_csv('./results/dropna/acc_com_ann.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(acc_2f,index=['train','valid','test','rest data']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on some of rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_of_rest = ['正常/P940_MSham_B_Walking_trial_6_emg.csv',\n",
    "                '正常/P940_M050_B_Walking_trial_4_emg.csv',\n",
    "                '正常/P812_M100_A_Walking_trial_3_emg.csv',\n",
    "                '正常/P645_M050_A_Walking_trial_3_emg.csv',\n",
    "                '正常/P623_Msham_B_Walking_trial_2_emg.csv',\n",
    "                '正常/P551_M50_B_Walking_trial_6_emg.csv',\n",
    "                'P379_M050_2_OFF_A_FoG_trial_1_emg.csv',\n",
    "                'P551_M050_2_B_FoG_trial_2_emg.csv']\n",
    "#booster = xgb.Booster()\n",
    "#booster.load_model('./model/XGBoost_W256_S64_Left.json')\n",
    "#model = xgb.XGBClassifier()\n",
    "#model._Booster = booster\n",
    "acc = []\n",
    "columns=['LEFT_TA','LEFT_TS','LEFT_BF', 'LEFT_RF']\n",
    "for file in some_of_rest:\n",
    "    path = './data/'+file\n",
    "    feature2,y2 = dp.pipeline_feature(path,width=256,stride=64,scaler=False,\n",
    "                                      threshold_WAMP=threshold_WAMP,\n",
    "                                      threshold_ZC=threshold_ZC,\n",
    "                                      threshold_SSC=threshold_SSC,\n",
    "                                      bins=bins,\n",
    "                                      ranges=HIST_range,\n",
    "                                      show_para=False,\n",
    "                                      filt = 250)\n",
    "    feature2_sc = sc.transform(feature2)\n",
    "    acc += [test_model(model,feature2_sc,y2)]\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(feature2_sc)\n",
    "metrics.accuracy_score(y2,y_pred>0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_re = feature_sc.reshape((-1,feature.shape[1],1))\n",
    "x_full,x_test,y_full,y_test = train_test_split(np.array(feature_re),np.array(y_01),test_size=0.2,random_state=123)\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(x_full,y_full,test_size=0.2,random_state=555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = layers.Input(shape=[feature.shape[1],1])\n",
    "lstm1 = layers.GRU(100)(input_)\n",
    "#lstm2 = layers.LSTM(20)(lstm1)\n",
    "output = layers.Dense(1,activation='sigmoid')(lstm1)\n",
    "model = Model(inputs=[input_],outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stopping = callbacks.EarlyStopping(patience = 20,\n",
    "                                         monitor = 'val_accuracy', \n",
    "                                         restore_best_weights=True)\n",
    "history = model.fit(x_train,y_train,validation_data=[x_valid,y_valid],\n",
    "                    epochs=100,batch_size=500,\n",
    "                   callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature2_re = feature2_sc.reshape((-1,feature.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_matrix=np.array([[0,1,1,1],\n",
    "                  [1,0,1,1],\n",
    "                  [10,100,0,10],\n",
    "                  [1.,1.,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.constant(cost_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.00e+00, 2.00e+00, 1.10e+02, 2.00e+00],\n",
       "       [2.00e+00, 3.00e+00, 2.00e+01, 2.00e+00],\n",
       "       [1.10e+02, 2.00e+01, 1.02e+04, 1.10e+02],\n",
       "       [2.00e+00, 2.00e+00, 1.10e+02, 3.00e+00]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t@tf.transpose(t)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1567100, shape=(4, 4), dtype=float64, numpy=\n",
       "array([[  0.,   1.,  10.,   1.],\n",
       "       [  1.,   0., 100.,   1.],\n",
       "       [  1.,   1.,   0.,   1.],\n",
       "       [  1.,   1.,  10.,   0.]])>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
