{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import data_processing as dp\n",
    "from scipy import signal\n",
    "from scipy.stats import skew\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('.\\data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 = '.\\data\\正常\\G11_Walking_trial_4_emg.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv('./processed data/featurePcwtf_W256_S64_dropna_samelabel.csv')\n",
    "\n",
    "# read file name of data with various Labels\n",
    "df = pd.read_csv('./useful_data_label.csv',index_col=0) \n",
    "drop = 'P551_M050_2_B_FoG_trial_2_emg.csv'\n",
    "ind_drop = df.columns!=drop\n",
    "# read file name of data with only label 0\n",
    "df2 = pd.read_csv('./unuseful_data_label.csv',index_col=0)\n",
    "# read some of the data with only label 0\n",
    "df3 = pd.read_csv('./data/file_name.txt',header=None)\n",
    "\n",
    "files = np.concatenate([np.array(df.columns),np.array(df3.loc[:,0])])\n",
    "ind = Data.File.isin(files)\n",
    "Data_sel = Data[ind]\n",
    "Data_rest = Data[~ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_col = ['_IEMG','_MAV','_SSI','_VAR','_RMS',\n",
    "               '_WL','_ZC','_SSC','_WAMP','_skew','_Acti','_AR','_HIST','_MDF']\n",
    "\n",
    "feature_all = Data_sel.iloc[:,1:-1]\n",
    "#ind_temp1 = feature_all.columns.str.contains('MAV')\n",
    "#ind_temp2 = feature_all.columns.str.contains('RMS')\n",
    "#ind_temp3 = feature_all.columns.str.contains('VAR')\n",
    "#ind_temp = ind_temp1|ind_temp2|ind_temp3\n",
    "ind_temp = feature_all.columns.str.contains('mDWT')\n",
    "feature = feature_all.loc[:,~ind_temp]\n",
    "#feature = Data_sel.iloc[:,1:-1]\n",
    "#feature = Data_sel.iloc[:,:-2]\n",
    "y = Data_sel.Label\n",
    "feature2_all = Data_rest.iloc[:,1:-1]\n",
    "feature2 = feature2_all.loc[:,~ind_temp]\n",
    "#feature2 = Data_rest.iloc[:,1:-1]\n",
    "#feature2 = Data_rest.iloc[:,:-2]\n",
    "y2 = Data_rest.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './processed data/data_set_after_window_withoutSC.hdf5'\n",
    "with h5py.File(path,'r') as f:\n",
    "    x = f['cwt_data'][...]\n",
    "    y = f['label2'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "imp.reload(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "player = ctypes.windll.kernel32\n",
    "player.Beep(1000,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 256\n",
    "threshold_WAMP = 30\n",
    "threshold_ZC = 0\n",
    "threshold_SSC = 1\n",
    "bins=9\n",
    "bound = 70\n",
    "HIST_range = (-bound,bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './processed data/data_set_after_window_S64_withoutSC_allPa.hdf5'\n",
    "with h5py.File(path,'r') as f:\n",
    "    x = f['cwt_data'][...]\n",
    "    y = f['label2'][...]\n",
    "feature = dp.generate_feature(x,threshold_WAMP=threshold_WAMP,\n",
    "                              threshold_ZC=threshold_ZC,\n",
    "                              threshold_SSC=threshold_SSC,\n",
    "                              bins=bins,ranges=HIST_range)\n",
    "#feature2 = dp.generate_feature(x2)\n",
    "player.Beep(1000,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./processed data/nfeatures_W256_S64_WAMP30.hdf5','r') as f:\n",
    "    feature = f['features'][...]\n",
    "    y = f['labels'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature2,y2 = dp.pipeline_feature(path2,width=256,stride=64,\n",
    "                                  scaler=False,\n",
    "                                  threshold_WAMP=threshold_WAMP,\n",
    "                                  threshold_ZC=threshold_ZC,\n",
    "                                  threshold_SSC=threshold_SSC,\n",
    "                                  bins=bins,ranges=HIST_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./processed data/nfeatures_rest_W256_S64.hdf5','r') as f:\n",
    "    feature2 = f['features'][...]\n",
    "    y2 = f['labels'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler(with_mean=True)\n",
    "feature_sc = sc.fit_transform(feature)\n",
    "feature2_sc = sc.transform(feature2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers,Model,callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,feature,y,binary=True,file=None):\n",
    "    if binary:\n",
    "        ind = ((y==0)|(y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "        ind1 = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "        y_01 = y.copy()\n",
    "        y_01[ind1] = 1\n",
    "        cw = None\n",
    "    else:\n",
    "        ind = ((y==1)|(y==2)|(y==6))\n",
    "        y_01 = y[ind].copy()\n",
    "        oc = OneHotEncoder()\n",
    "        y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "        cw = None#{0:1,1:2,2:1}\n",
    "    x_full,x_test,y_full,y_test = train_test_split(np.array(feature)[ind,:],y_01,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=123,\n",
    "                                                   shuffle=True)\n",
    "    x_train,x_valid,y_train,y_valid = train_test_split(x_full,y_full,\n",
    "                                                       test_size=0.2,\n",
    "                                                       random_state=555,\n",
    "                                                       shuffle=True)\n",
    "    \n",
    "    sc = StandardScaler(with_mean=True)\n",
    "    x_train = sc.fit_transform(x_train)\n",
    "    x_valid = sc.transform(x_valid)\n",
    "    x_test = sc.transform(x_test)\n",
    "    \n",
    "    early_stopping = callbacks.EarlyStopping(patience = 10,\n",
    "                                             monitor = 'val_accuracy', \n",
    "                                             restore_best_weights=True)\n",
    "    history = model.fit(x_train,y_train,validation_data=[x_valid,y_valid],\n",
    "                        epochs=200,batch_size=50,class_weight=cw,\n",
    "                        callbacks=[early_stopping])\n",
    "    \n",
    "    if binary:\n",
    "        y_pred_t=model.predict(x_test)\n",
    "        test = metrics.accuracy_score(y_test,y_pred_t>0.5)\n",
    "        y_pred_v=model.predict(x_valid)\n",
    "        valid = metrics.accuracy_score(y_valid,y_pred_v>0.5)\n",
    "        y_pred_ta=model.predict(x_train)\n",
    "        train = metrics.accuracy_score(y_train,y_pred_ta>0.5)\n",
    "    else:\n",
    "        y_pred_t=model.predict(x_test)\n",
    "        test = metrics.accuracy_score(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1))\n",
    "        y_pred_v=model.predict(x_valid)\n",
    "        valid = metrics.accuracy_score(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1))\n",
    "        y_pred_ta=model.predict(x_train)\n",
    "        train = metrics.accuracy_score(np.argmax(y_train,axis=1),np.argmax(y_pred_ta,axis=1))        \n",
    "        print('train: \\n',metrics.confusion_matrix(np.argmax(y_train,axis=1),np.argmax(y_pred_ta,axis=1)))\n",
    "        print('valid: \\n',metrics.confusion_matrix(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1)))\n",
    "        print('test: \\n',metrics.confusion_matrix(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1)))\n",
    "    print('test:%f'%test)\n",
    "    print('valid:%f'%valid)\n",
    "    print('train:%f'%train)\n",
    "    if file != None:\n",
    "        model.save_model(file)\n",
    "    return train,valid,test,sc\n",
    "\n",
    "def test_model(model,feature,y,sc):\n",
    "    ind = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "    y_01 = y.copy()\n",
    "    y_01[ind] = 1\n",
    "    y_pred=model.predict(sc.transform(feature))\n",
    "    test = metrics.accuracy_score(y_01,y_pred>0.5)\n",
    "    print('acc:%f'%test)\n",
    "    return test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ind = ((y==1)|(y==2)|(y==3)|(y==6))\n",
    "ind = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "ind_f = [0,1,6,42,46,57,62]\n",
    "#y_01 = y[ind].copy()\n",
    "#y_01[y_01==1]=0\n",
    "#y_01[y_01==2]=1\n",
    "#y_01[y_01==3]=2\n",
    "#y_01[y_01==6]=3\n",
    "y_01 = y.copy()\n",
    "y_01[ind] = 1\n",
    "oh_ec = OneHotEncoder()\n",
    "y_oh = oh_ec.fit_transform(y_01[:,np.newaxis]).toarray()\n",
    "x_full,x_test,y_full,y_test = train_test_split(feature_sc,y_01,test_size=0.2,random_state=123,shuffle=False)\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(x_full,y_full,test_size=0.2,random_state=555,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = layers.Input(shape=feature.shape[1:])\n",
    "l1 = layers.Dense(128,activation='elu')(input_)\n",
    "drop1 = layers.Dropout(0.2)(l1)\n",
    "l2 = layers.Dense(64,activation='elu')(drop1)\n",
    "drop2 = layers.Dropout(0.2)(l2)\n",
    "l3 = layers.Dense(32,activation='elu')(drop2)\n",
    "drop3 = layers.Dropout(0.2)(l3)\n",
    "#l4 = layers.Dense(16,activation='selu')(l3)\n",
    "#drop4 = layers.Dropout(0.2)(l4)\n",
    "output = layers.Dense(3,activation='softmax')(drop3)\n",
    "model = Model(inputs=[input_],outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = callbacks.EarlyStopping(patience = 20,\n",
    "                                         monitor = 'val_accuracy', \n",
    "                                         restore_best_weights=True)\n",
    "history = model.fit(x_train,y_train,validation_data=[x_valid,y_valid],\n",
    "                    epochs=200,batch_size=50,\n",
    "                   callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6821 samples, validate on 1706 samples\n",
      "Epoch 1/200\n",
      "6821/6821 [==============================] - 1s 146us/sample - loss: 0.6663 - accuracy: 0.7213 - val_loss: 0.5271 - val_accuracy: 0.7667\n",
      "Epoch 2/200\n",
      "6821/6821 [==============================] - 0s 60us/sample - loss: 0.5497 - accuracy: 0.7643 - val_loss: 0.4933 - val_accuracy: 0.7907\n",
      "Epoch 3/200\n",
      "6821/6821 [==============================] - 0s 63us/sample - loss: 0.4908 - accuracy: 0.7939 - val_loss: 0.4745 - val_accuracy: 0.7819\n",
      "Epoch 4/200\n",
      "6821/6821 [==============================] - 0s 63us/sample - loss: 0.4685 - accuracy: 0.8011 - val_loss: 0.4612 - val_accuracy: 0.7925\n",
      "Epoch 5/200\n",
      "6821/6821 [==============================] - 0s 56us/sample - loss: 0.4506 - accuracy: 0.8035 - val_loss: 0.4426 - val_accuracy: 0.8007\n",
      "Epoch 6/200\n",
      "6821/6821 [==============================] - 0s 58us/sample - loss: 0.4396 - accuracy: 0.8131 - val_loss: 0.4304 - val_accuracy: 0.8066\n",
      "Epoch 7/200\n",
      "6821/6821 [==============================] - 0s 57us/sample - loss: 0.4145 - accuracy: 0.8247 - val_loss: 0.4094 - val_accuracy: 0.8124\n",
      "Epoch 8/200\n",
      "6821/6821 [==============================] - 0s 62us/sample - loss: 0.4036 - accuracy: 0.8318 - val_loss: 0.3892 - val_accuracy: 0.8247\n",
      "Epoch 9/200\n",
      "6821/6821 [==============================] - 0s 56us/sample - loss: 0.3896 - accuracy: 0.8355 - val_loss: 0.4288 - val_accuracy: 0.7966\n",
      "Epoch 10/200\n",
      "6821/6821 [==============================] - 0s 55us/sample - loss: 0.3622 - accuracy: 0.8497 - val_loss: 0.3960 - val_accuracy: 0.8329\n",
      "Epoch 11/200\n",
      "6821/6821 [==============================] - 0s 57us/sample - loss: 0.3495 - accuracy: 0.8547 - val_loss: 0.3903 - val_accuracy: 0.8288\n",
      "Epoch 12/200\n",
      "6821/6821 [==============================] - 0s 56us/sample - loss: 0.3461 - accuracy: 0.8568 - val_loss: 0.3621 - val_accuracy: 0.8423\n",
      "Epoch 13/200\n",
      "6821/6821 [==============================] - 0s 61us/sample - loss: 0.3338 - accuracy: 0.8651 - val_loss: 0.3496 - val_accuracy: 0.8453\n",
      "Epoch 14/200\n",
      "6821/6821 [==============================] - 0s 56us/sample - loss: 0.3181 - accuracy: 0.8685 - val_loss: 0.3516 - val_accuracy: 0.8394\n",
      "Epoch 15/200\n",
      "6821/6821 [==============================] - 0s 57us/sample - loss: 0.3161 - accuracy: 0.8663 - val_loss: 0.3583 - val_accuracy: 0.8464\n",
      "Epoch 16/200\n",
      "6821/6821 [==============================] - 0s 60us/sample - loss: 0.2995 - accuracy: 0.8776 - val_loss: 0.3488 - val_accuracy: 0.8470\n",
      "Epoch 17/200\n",
      "6821/6821 [==============================] - 0s 56us/sample - loss: 0.3059 - accuracy: 0.8717 - val_loss: 0.3274 - val_accuracy: 0.8546\n",
      "Epoch 18/200\n",
      "6821/6821 [==============================] - 0s 60us/sample - loss: 0.2801 - accuracy: 0.8824 - val_loss: 0.3500 - val_accuracy: 0.8576\n",
      "Epoch 19/200\n",
      "6821/6821 [==============================] - 0s 63us/sample - loss: 0.2746 - accuracy: 0.8900 - val_loss: 0.3232 - val_accuracy: 0.8705\n",
      "Epoch 20/200\n",
      "6821/6821 [==============================] - 0s 57us/sample - loss: 0.2705 - accuracy: 0.8896 - val_loss: 0.3121 - val_accuracy: 0.8705\n",
      "Epoch 21/200\n",
      "6821/6821 [==============================] - 0s 57us/sample - loss: 0.2758 - accuracy: 0.8848 - val_loss: 0.3223 - val_accuracy: 0.8570\n",
      "Epoch 22/200\n",
      "6821/6821 [==============================] - 0s 56us/sample - loss: 0.2610 - accuracy: 0.8994 - val_loss: 0.3124 - val_accuracy: 0.8664\n",
      "Epoch 23/200\n",
      "6821/6821 [==============================] - 0s 59us/sample - loss: 0.2513 - accuracy: 0.8974 - val_loss: 0.3051 - val_accuracy: 0.8681\n",
      "Epoch 24/200\n",
      "6821/6821 [==============================] - 0s 57us/sample - loss: 0.2397 - accuracy: 0.9028 - val_loss: 0.2993 - val_accuracy: 0.8792\n",
      "Epoch 25/200\n",
      "6821/6821 [==============================] - 0s 56us/sample - loss: 0.2340 - accuracy: 0.9100 - val_loss: 0.2984 - val_accuracy: 0.8804\n",
      "Epoch 26/200\n",
      "6821/6821 [==============================] - 0s 56us/sample - loss: 0.2252 - accuracy: 0.9104 - val_loss: 0.2997 - val_accuracy: 0.8775\n",
      "Epoch 27/200\n",
      "6821/6821 [==============================] - 0s 56us/sample - loss: 0.2127 - accuracy: 0.9156 - val_loss: 0.3017 - val_accuracy: 0.8787\n",
      "Epoch 28/200\n",
      "6821/6821 [==============================] - 0s 63us/sample - loss: 0.2240 - accuracy: 0.9094 - val_loss: 0.2925 - val_accuracy: 0.8810\n",
      "Epoch 29/200\n",
      "6821/6821 [==============================] - 0s 57us/sample - loss: 0.2126 - accuracy: 0.9197 - val_loss: 0.2934 - val_accuracy: 0.8857\n",
      "Epoch 30/200\n",
      "6821/6821 [==============================] - 0s 60us/sample - loss: 0.2055 - accuracy: 0.9202 - val_loss: 0.2886 - val_accuracy: 0.8875\n",
      "Epoch 31/200\n",
      "6821/6821 [==============================] - 0s 56us/sample - loss: 0.2028 - accuracy: 0.9223 - val_loss: 0.2961 - val_accuracy: 0.8834\n",
      "Epoch 32/200\n",
      "6821/6821 [==============================] - 0s 56us/sample - loss: 0.1934 - accuracy: 0.9242 - val_loss: 0.2667 - val_accuracy: 0.9004\n",
      "Epoch 33/200\n",
      "6821/6821 [==============================] - 0s 59us/sample - loss: 0.1895 - accuracy: 0.9233 - val_loss: 0.3016 - val_accuracy: 0.8863\n",
      "Epoch 34/200\n",
      "6821/6821 [==============================] - 0s 57us/sample - loss: 0.1858 - accuracy: 0.9279 - val_loss: 0.3122 - val_accuracy: 0.8898\n",
      "Epoch 35/200\n",
      "6821/6821 [==============================] - 0s 56us/sample - loss: 0.1816 - accuracy: 0.9277 - val_loss: 0.3011 - val_accuracy: 0.8845\n",
      "Epoch 36/200\n",
      "6821/6821 [==============================] - 0s 58us/sample - loss: 0.1800 - accuracy: 0.9320 - val_loss: 0.2844 - val_accuracy: 0.8939\n",
      "Epoch 37/200\n",
      "6821/6821 [==============================] - 0s 58us/sample - loss: 0.1772 - accuracy: 0.9318 - val_loss: 0.3059 - val_accuracy: 0.8828\n",
      "Epoch 38/200\n",
      "6821/6821 [==============================] - 0s 61us/sample - loss: 0.1665 - accuracy: 0.9342 - val_loss: 0.2827 - val_accuracy: 0.8927\n",
      "Epoch 39/200\n",
      "6821/6821 [==============================] - 0s 56us/sample - loss: 0.1660 - accuracy: 0.9343 - val_loss: 0.2833 - val_accuracy: 0.8962\n",
      "Epoch 40/200\n",
      "6821/6821 [==============================] - 0s 57us/sample - loss: 0.1551 - accuracy: 0.9393 - val_loss: 0.2839 - val_accuracy: 0.9009\n",
      "Epoch 41/200\n",
      "6821/6821 [==============================] - 0s 57us/sample - loss: 0.1534 - accuracy: 0.9421 - val_loss: 0.2827 - val_accuracy: 0.8962\n",
      "Epoch 42/200\n",
      "6821/6821 [==============================] - 0s 60us/sample - loss: 0.1682 - accuracy: 0.9362 - val_loss: 0.2903 - val_accuracy: 0.8904\n",
      "Epoch 43/200\n",
      "6821/6821 [==============================] - 0s 59us/sample - loss: 0.1525 - accuracy: 0.9383 - val_loss: 0.2832 - val_accuracy: 0.8951\n",
      "Epoch 44/200\n",
      "6821/6821 [==============================] - 0s 57us/sample - loss: 0.1569 - accuracy: 0.9389 - val_loss: 0.2519 - val_accuracy: 0.9097\n",
      "Epoch 45/200\n",
      "6821/6821 [==============================] - 0s 56us/sample - loss: 0.1355 - accuracy: 0.9460 - val_loss: 0.2802 - val_accuracy: 0.8957\n",
      "Epoch 46/200\n",
      "6821/6821 [==============================] - 0s 56us/sample - loss: 0.1424 - accuracy: 0.9463 - val_loss: 0.2783 - val_accuracy: 0.9068\n",
      "Epoch 47/200\n",
      "6821/6821 [==============================] - 0s 57us/sample - loss: 0.1365 - accuracy: 0.9481 - val_loss: 0.2754 - val_accuracy: 0.9045\n",
      "Epoch 48/200\n",
      "6821/6821 [==============================] - 0s 60us/sample - loss: 0.1392 - accuracy: 0.9485 - val_loss: 0.2893 - val_accuracy: 0.8962\n",
      "Epoch 49/200\n",
      "6821/6821 [==============================] - 0s 56us/sample - loss: 0.1307 - accuracy: 0.9485 - val_loss: 0.3196 - val_accuracy: 0.8939\n",
      "Epoch 50/200\n",
      "6821/6821 [==============================] - 0s 56us/sample - loss: 0.1320 - accuracy: 0.9477 - val_loss: 0.2759 - val_accuracy: 0.9027\n",
      "Epoch 51/200\n",
      "6821/6821 [==============================] - 0s 56us/sample - loss: 0.1305 - accuracy: 0.9502 - val_loss: 0.2915 - val_accuracy: 0.9021\n",
      "Epoch 52/200\n",
      "6821/6821 [==============================] - 0s 58us/sample - loss: 0.1304 - accuracy: 0.9516 - val_loss: 0.2796 - val_accuracy: 0.9074\n",
      "Epoch 53/200\n",
      "6821/6821 [==============================] - 0s 64us/sample - loss: 0.1266 - accuracy: 0.9537 - val_loss: 0.2913 - val_accuracy: 0.9004\n",
      "Epoch 54/200\n",
      "6821/6821 [==============================] - 0s 71us/sample - loss: 0.1236 - accuracy: 0.9538 - val_loss: 0.3013 - val_accuracy: 0.8998\n",
      "train: \n",
      " [[1502   29   13]\n",
      " [  25 3376    9]\n",
      " [   8    9 1850]]\n",
      "valid: \n",
      " [[291  43  28]\n",
      " [ 37 813  10]\n",
      " [ 21  15 448]]\n",
      "test: \n",
      " [[ 367   70   27]\n",
      " [  39 1022   17]\n",
      " [  34   28  528]]\n",
      "test:0.899156\n",
      "valid:0.909730\n",
      "train:0.986366\n"
     ]
    }
   ],
   "source": [
    "#sc = StandardScaler(with_mean=True)\n",
    "#feature_sc = sc.fit_transform(feature)\n",
    "#feature2_sc = sc.transform(feature2)\n",
    "#pca = PCA(n_components=160,copy=True)\n",
    "#feature_pca = pca.fit_transform(feature)\n",
    "#feature2_pca = pca.transform(feature2)\n",
    "train,valid,test,sc = train_model(model,feature,np.array(y),False)\n",
    "#rest = test_model(model,feature2,np.array(y2),sc)\n",
    "#acc = [train,valid,test,rest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model,feature2_sc,np.array(y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model(model,feature2_sc,np.array(y2))\n",
    "ind = ((y==1)|(y==2)|(y==6))\n",
    "y_01 = y[ind].copy()\n",
    "oc = OneHotEncoder()\n",
    "y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "pred=model.predict(feature_sc[ind,:])\n",
    "np.argmax(pred,axis=1)\n",
    "np.argmax(y_01,axis=1)\n",
    "metrics.confusion_matrix(np.argmax(y_01,axis=1),np.argmax(pred,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc_ann = pd.DataFrame(acc,columns=['dnn1'],index=['train','valid','test','rest data'])\n",
    "#acc_ann_com=pd.concat([acc_ann_com,acc_ann.T])\n",
    "#acc_ann.loc['drop_mDWT',:]=[train,valid,test,rest]\n",
    "acc_ann_com.loc['dnn2',:]=acc\n",
    "acc_ann_com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test feature from Left or Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = layers.Input(shape=feature.loc[:,ind_temp].shape[1:])\n",
    "l1 = layers.Dense(128,activation='elu')(input_)\n",
    "drop1 = layers.Dropout(0.2)(l1)\n",
    "l2 = layers.Dense(64,activation='elu')(drop1)\n",
    "drop2 = layers.Dropout(0.2)(l2)\n",
    "#l3 = layers.Dense(32,activation='elu')(drop2)\n",
    "#drop3 = layers.Dropout(0.2)(l3)\n",
    "#l4 = layers.Dense(16,activation='selu')(l3)\n",
    "#drop4 = layers.Dropout(0.2)(l4)\n",
    "output = layers.Dense(1,activation='sigmoid')(drop2)\n",
    "model = Model(inputs=[input_],outputs=[output])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc={}\n",
    "cols = ['LEFT','RIGHT']\n",
    "\n",
    "#sc = StandardScaler(with_mean=True)\n",
    "\n",
    "for col in cols:\n",
    "    ind_temp=feature.columns.str.contains(col)\n",
    "    #feature_sc = sc.fit_transform(feature.loc[:,ind_temp])\n",
    "    #feature2_sc = sc.transform(feature2.loc[:,ind_temp])\n",
    "    train,valid,test,sc = train_model(model,np.array(feature.loc[:,ind_temp]),np.array(y))\n",
    "    \n",
    "    acc_rest=test_model(model,np.array(feature2.loc[:,ind_temp]),np.array(y2),sc)\n",
    "    acc[col] = [train,valid,test,acc_rest]\n",
    "    print(acc_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_lr=pd.DataFrame(acc,index=['train','valid','test','rest data'])#.to_csv('./results/acc_lr_ann.csv')\n",
    "#acc_com=pd.concat([acc_ann_com.drop(['dnn','dnn1'],'index'),acc_lr.T])\n",
    "acc_com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test features from 2 of 8 signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test features from 2 of 8 signals\n",
    "\n",
    "acc_2f={}\n",
    "cols = [ 'LEFT_TA','LEFT_TS','LEFT_BF', 'LEFT_RF',\n",
    "        'RIGHT_TA','RIGHT_TS','RIGHT_BF', 'RIGHT_RF']\n",
    "\n",
    "\n",
    "for p in combinations(cols[:4],2):\n",
    "    ind_temp=feature.columns.str.contains(p[0])| feature.columns.str.contains(p[1])\n",
    "    #feature_sc = sc.fit_transform(feature.loc[:,ind_temp])\n",
    "    #feature2_sc = sc.transform(feature2.loc[:,ind_temp])\n",
    "    train,valid,test,sc = train_model(model,np.array(feature.loc[:,ind_temp]),np.array(y))\n",
    "    acc_rest=test_model(model,np.array(feature2.loc[:,ind_temp]),np.array(y2),sc)\n",
    "    acc_2f[p[0]+'_'+p[1]] = [train,valid,test,acc_rest]\n",
    "    print(acc_rest)\n",
    "    \n",
    "for p in combinations(cols[4:],2):\n",
    "    ind_temp=feature.columns.str.contains(p[0])| feature.columns.str.contains(p[1])\n",
    "    #feature_sc = sc.fit_transform(feature.loc[:,ind_temp])\n",
    "    #feature2_sc = sc.transform(feature2.loc[:,ind_temp])\n",
    "    train,valid,test,sc = train_model(model,np.array(feature.loc[:,ind_temp]),np.array(y))\n",
    "    acc_rest=test_model(model,np.array(feature2.loc[:,ind_temp]),np.array(y2),sc)\n",
    "    acc_2f[p[0]+'_'+p[1]] = [train,valid,test,acc_rest]\n",
    "    print(acc_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc_2f_ann = pd.DataFrame(acc_2f,index=['train','valid','test','rest data']).T\n",
    "#acc_com = pd.concat([acc_com,acc_2f_ann])\n",
    "acc_com.to_csv('./results/dropna/acc_com_ann.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(acc_2f,index=['train','valid','test','rest data']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on some of rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_of_rest = ['正常/P940_MSham_B_Walking_trial_6_emg.csv',\n",
    "                '正常/P940_M050_B_Walking_trial_4_emg.csv',\n",
    "                '正常/P812_M100_A_Walking_trial_3_emg.csv',\n",
    "                '正常/P645_M050_A_Walking_trial_3_emg.csv',\n",
    "                '正常/P623_Msham_B_Walking_trial_2_emg.csv',\n",
    "                '正常/P551_M50_B_Walking_trial_6_emg.csv',\n",
    "                'P379_M050_2_OFF_A_FoG_trial_1_emg.csv',\n",
    "                'P551_M050_2_B_FoG_trial_2_emg.csv']\n",
    "#booster = xgb.Booster()\n",
    "#booster.load_model('./model/XGBoost_W256_S64_Left.json')\n",
    "#model = xgb.XGBClassifier()\n",
    "#model._Booster = booster\n",
    "acc = []\n",
    "columns=['LEFT_TA','LEFT_TS','LEFT_BF', 'LEFT_RF']\n",
    "for file in some_of_rest:\n",
    "    path = './data/'+file\n",
    "    feature2,y2 = dp.pipeline_feature(path,width=256,stride=64,scaler=False,\n",
    "                                      threshold_WAMP=threshold_WAMP,\n",
    "                                      threshold_ZC=threshold_ZC,\n",
    "                                      threshold_SSC=threshold_SSC,\n",
    "                                      bins=bins,\n",
    "                                      ranges=HIST_range,\n",
    "                                      show_para=False,\n",
    "                                      filt = 250)\n",
    "    feature2_sc = sc.transform(feature2)\n",
    "    acc += [test_model(model,feature2_sc,y2)]\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(feature2_sc)\n",
    "metrics.accuracy_score(y2,y_pred>0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_re = feature_sc.reshape((-1,feature.shape[1],1))\n",
    "x_full,x_test,y_full,y_test = train_test_split(np.array(feature_re),np.array(y_01),test_size=0.2,random_state=123)\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(x_full,y_full,test_size=0.2,random_state=555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = layers.Input(shape=[feature.shape[1],1])\n",
    "lstm1 = layers.GRU(100)(input_)\n",
    "#lstm2 = layers.LSTM(20)(lstm1)\n",
    "output = layers.Dense(1,activation='sigmoid')(lstm1)\n",
    "model = Model(inputs=[input_],outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stopping = callbacks.EarlyStopping(patience = 20,\n",
    "                                         monitor = 'val_accuracy', \n",
    "                                         restore_best_weights=True)\n",
    "history = model.fit(x_train,y_train,validation_data=[x_valid,y_valid],\n",
    "                    epochs=100,batch_size=500,\n",
    "                   callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature2_re = feature2_sc.reshape((-1,feature.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
