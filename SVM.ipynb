{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder,normalize\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "import data_processing as dp\n",
    "from scipy import signal\n",
    "from scipy.stats import skew,pearsonr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "from itertools import combinations\n",
    "from sklearn.feature_selection import SelectKBest,f_classif,chi2,mutual_info_classif,VarianceThreshold,RFE,SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\data\\G04_FoG_trial_1_emg.csv\n",
      ".\\data\\G04_FoG_trial_2_emg.csv\n",
      ".\\data\\G06_FoG_trial_1_emg.csv\n",
      ".\\data\\G06_FoG_trial_2_emg.csv\n",
      ".\\data\\G06_FoG_trial_3_emg.csv\n",
      ".\\data\\G07_Freezing_Trial1_trial_1_emg.csv\n",
      ".\\data\\G08_FoG_1_trial_1_emg.csv\n",
      ".\\data\\G08_FoG_2_trial_1_emg.csv\n",
      ".\\data\\G11_FoG_trial_1_emg.csv\n",
      ".\\data\\G11_FoG_trial_2_emg.csv\n",
      ".\\data\\P379_M050_2_OFF_A_FoG_trial_1_emg.csv\n",
      ".\\data\\P379_M050_2_OFF_A_FoG_trial_2_emg.csv\n",
      ".\\data\\P379_M050_2_OFF_A_FoG_trial_3_emg.csv\n",
      ".\\data\\P379_M050_2_OFF_B_FoG_trial_1_emg.csv\n",
      ".\\data\\P379_M050_2_OFF_B_FoG_trial_2_emg.csv\n",
      ".\\data\\P379_M050_2_OFF_B_FoG_trial_3_emg.csv\n",
      ".\\data\\P551_M050_2_A_FoG_trial_1_emg.csv\n",
      ".\\data\\P551_M050_2_B_FoG_trial_1_emg.csv\n",
      ".\\data\\P551_M050_2_B_FoG_trial_2_emg.csv\n",
      ".\\data\\P812_M050_2_B_FoG_trial_1_emg.csv\n",
      ".\\data\\P812_M050_2_B_FoG_trial_2_emg.csv\n",
      ".\\data\\其他\\labels.txt\n",
      ".\\data\\其他\\P812_M050_2_B_FoG_trial1_annotation.csv\n",
      ".\\data\\其他\\P812_M050_2_B_FoG_trials.mat\n",
      ".\\data\\其他\\P812_M050_2_B_FoG_trial_1_out_left_foot.csv\n",
      ".\\data\\其他\\P812_M050_2_B_FoG_trial_1_out_lower_left_foot.csv\n",
      ".\\data\\其他\\P812_M050_2_B_FoG_trial_1_out_lower_right_foot.csv\n",
      ".\\data\\其他\\P812_M050_2_B_FoG_trial_1_out_right_foot.csv\n",
      ".\\data\\其他\\P812_M050_2_B_FoG_trial_2_out_left_foot.csv\n",
      ".\\data\\其他\\P812_M050_2_B_FoG_trial_2_out_lower_left_foot.csv\n",
      ".\\data\\其他\\P812_M050_2_B_FoG_trial_2_out_lower_right_foot.csv\n",
      ".\\data\\其他\\P812_M050_2_B_FoG_trial_2_out_right_foot.csv\n",
      ".\\data\\其他\\P812_M50_2_B_FoG_trial2_annotation.csv\n",
      ".\\data\\正常\\G02_Walking_trial_1_emg.csv\n",
      ".\\data\\正常\\G03_Walking_trial_1_emg.csv\n",
      ".\\data\\正常\\G03_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\G05_Walking_struct_fixed_trial_1_emg.csv\n",
      ".\\data\\正常\\G05_Walking_struct_fixed_trial_2_emg.csv\n",
      ".\\data\\正常\\G05_Walking_struct_fixed_trial_3_emg.csv\n",
      ".\\data\\正常\\G09_FoG_trial_1_emg.csv\n",
      ".\\data\\正常\\G09_FoG_trial_2_emg.csv\n",
      ".\\data\\正常\\G09_FoG_trial_3_emg.csv\n",
      ".\\data\\正常\\G09_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\G09_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\G09_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\G11_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\G11_Walking_trial_4_emg.csv\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('.\\data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emg_data = pd.read_csv('.\\data\\P812_M050_2_B_FoG_trial_1_emg.csv')\n",
    "emg_data2 = pd.read_csv('.\\data\\P812_M050_2_B_FoG_trial_2_emg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time        0\n",
       "Label1      0\n",
       "Label2      0\n",
       "LEFT_TA     0\n",
       "LEFT_TS     0\n",
       "LEFT_BF     0\n",
       "LEFT_RF     0\n",
       "RIGHT_TA    0\n",
       "RIGHT_TS    0\n",
       "RIGHT_BF    0\n",
       "RIGHT_RF    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emg_data = emg_data.fillna({'LEFT_BF':emg_data.LEFT_BF.mean(),\n",
    "                           'LEFT_RF':emg_data.LEFT_RF.mean(),\n",
    "                           'RIGHT_TA':emg_data.RIGHT_TA.mean(),\n",
    "                           'RIGHT_TS':emg_data.RIGHT_TS.mean(),\n",
    "                           'RIGHT_BF':emg_data.RIGHT_BF.mean(),\n",
    "                           'RIGHT_RF':emg_data.RIGHT_RF.mean()})\n",
    "emg_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv('./processed data/featurePcwtf_W256_S64_WS32_DWTLmax_dropna_samelabel.csv')\n",
    "\n",
    "# read file name of data with various Labels\n",
    "df = pd.read_csv('./useful_data_label.csv',index_col=0) \n",
    "drop = 'P551_M050_2_B_FoG_trial_2_emg.csv'\n",
    "ind_drop = df.columns!=drop\n",
    "\n",
    "# read file name of data with only label 0\n",
    "df2 = pd.read_csv('./unuseful_data_label.csv',index_col=0)\n",
    "# read some of the data with only label 0\n",
    "df3 = pd.read_csv('./data/file_name.txt',header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = np.concatenate([np.array(df.columns),np.array(df3.loc[:,0])])\n",
    "ind = Data.File.isin(files)\n",
    "Data_sel = Data[ind]\n",
    "Data_rest = Data[~ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_col = ['_IEMG','_MAV','_SSI','_VAR','_RMS',\n",
    "               '_WL','_ZC','_SSC','_WAMP','_skew',\n",
    "               '_Acti','_AR','_HIST','_MDF','_MNF','_mDWT']\n",
    "\n",
    "feature_all = Data_sel.iloc[:,1:-1]\n",
    "#ind_temp1 = feature_all.columns.str.contains('_IEMG')\n",
    "#ind_temp2 = feature_all.columns.str.contains('_skew')\n",
    "#ind_temp3 = feature_all.columns.str.contains('_mDWT')\n",
    "#ind_temp = ind_temp2|ind_temp3\n",
    "ind_temp = feature_all.columns.str.contains('_mDWT')\n",
    "feature = feature_all.loc[:,~ind_temp]\n",
    "#feature = Data_sel.iloc[:,1:-1]\n",
    "y = Data_sel.iloc[:,0]\n",
    "feature2_all = Data_rest.iloc[:,1:-1]\n",
    "feature2 = feature2_all.loc[:,~ind_temp]\n",
    "#feature2 = Data_rest.iloc[:,1:-1]\n",
    "y2 = Data_rest.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = generate_window_slide_data(emg_data)\n",
    "x2,y2 = generate_window_slide_data(emg_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './processed data/data_set_after_window_withoutSC.hdf5'\n",
    "with h5py.File(path,'r') as f:\n",
    "    x = f['cwt_data'][...]\n",
    "    y = f['label2'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold_WAMP:2.0, threshold_ZC:0.0, threshold_SSC:0.0,bins:9,ranges:(-70,70)\n",
      "IEMG,MAV,SSI,VAR,RMS,WL,ZC,SSC,WAMP,skew,Acti,AR\n"
     ]
    }
   ],
   "source": [
    "feature = dp.generate_feature(x,2,0,bins=9,ranges=(-70,70))\n",
    "#feature2 = dp.generate_feature(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler(with_mean=False)\n",
    "feature_sc = sc.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ctypes\n",
    "player = ctypes.windll.kernel32\n",
    "player.Beep(1000,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file name of data with various Labels\n",
    "df = pd.read_csv('./useful_data_label.csv',index_col=0) \n",
    "#drop = 'G08_FoG_1_trial_1_emg.csv'\n",
    "#ind_drop = df.columns!=drop\n",
    "\n",
    "# read file name of data with only label 0\n",
    "df2 = pd.read_csv('./unuseful_data_label.csv',index_col=0)\n",
    "# read some of the data with only label 0\n",
    "df3 = pd.read_csv('./data/file_name.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/174: G06_FoG_trial_1_emg.csv\n",
      "4/174: G06_FoG_trial_2_emg.csv\n",
      "5/174: G06_FoG_trial_3_emg.csv\n",
      "6/174: G07_Freezing_Trial1_trial_1_emg.csv\n",
      "7/174: G08_FoG_1_trial_1_emg.csv\n",
      "8/174: G08_FoG_2_trial_1_emg.csv\n",
      "9/174: G11_FoG_trial_1_emg.csv\n",
      "10/174: G11_FoG_trial_2_emg.csv\n",
      "11/174: P379_M050_2_OFF_A_FoG_trial_1_emg.csv\n",
      "12/174: P379_M050_2_OFF_A_FoG_trial_2_emg.csv\n",
      "13/174: P379_M050_2_OFF_A_FoG_trial_3_emg.csv\n",
      "14/174: P379_M050_2_OFF_B_FoG_trial_1_emg.csv\n",
      "15/174: P379_M050_2_OFF_B_FoG_trial_2_emg.csv\n",
      "16/174: P379_M050_2_OFF_B_FoG_trial_3_emg.csv\n",
      "17/174: P551_M050_2_A_FoG_trial_1_emg.csv\n",
      "18/174: P551_M050_2_B_FoG_trial_1_emg.csv\n",
      "19/174: P551_M050_2_B_FoG_trial_2_emg.csv\n",
      "20/174: P812_M050_2_B_FoG_trial_1_emg.csv\n",
      "21/174: P812_M050_2_B_FoG_trial_2_emg.csv\n",
      "22/174: 正常/G02_Walking_trial_1_emg.csv\n",
      "23/174: 正常/G03_Walking_trial_1_emg.csv\n",
      "24/174: 正常/G03_Walking_trial_2_emg.csv\n",
      "25/174: 正常/G05_Walking_struct_fixed_trial_1_emg.csv\n",
      "26/174: 正常/G05_Walking_struct_fixed_trial_2_emg.csv\n",
      "27/174: 正常/G05_Walking_struct_fixed_trial_3_emg.csv\n",
      "28/174: 正常/G09_FoG_trial_1_emg.csv\n",
      "29/174: 正常/G09_FoG_trial_2_emg.csv\n",
      "30/174: 正常/G09_FoG_trial_3_emg.csv\n",
      "31/174: 正常/G09_Walking_trial_2_emg.csv\n",
      "32/174: 正常/G09_Walking_trial_4_emg.csv\n",
      "33/174: 正常/G09_Walking_trial_6_emg.csv\n",
      "34/174: 正常/G11_Walking_trial_2_emg.csv\n",
      "35/174: 正常/G11_Walking_trial_4_emg.csv\n",
      "36/174: 正常/P231_M050_A_Walking_trial_2_emg.csv\n",
      "37/174: 正常/P231_M050_A_Walking_trial_4_emg.csv\n",
      "38/174: 正常/P231_M050_A_Walking_trial_6_emg.csv\n",
      "39/174: 正常/P231_M050_B_Walking_trial_2_emg.csv\n",
      "40/174: 正常/P231_M050_B_Walking_trial_4_emg.csv\n",
      "41/174: 正常/P231_M050_B_Walking_trial_6_emg.csv\n",
      "42/174: 正常/P231_M100_2_A_FoG_trial_3_emg.csv\n",
      "43/174: 正常/P231_M100_2_A_Walking_trial_4_emg.csv\n",
      "44/174: 正常/P231_M100_2_A_Walking_trial_6_emg.csv\n",
      "45/174: 正常/P231_M100_ON_A_Walking_trial_2_emg.csv\n",
      "46/174: 正常/P231_M100_ON_A_Walking_trial_4_emg.csv\n",
      "47/174: 正常/P231_M100_ON_A_Walking_trial_6_emg.csv\n",
      "48/174: 正常/P231_Msham_A_Walking_trial_2_emg.csv\n",
      "49/174: 正常/P231_Msham_A_Walking_trial_6_emg.csv\n",
      "50/174: 正常/P231_Msham_B_Walking_trial_2_emg.csv\n",
      "51/174: 正常/P351_M050_2_A_FoG_trial_1_emg.csv\n",
      "52/174: 正常/P351_M050_2_A_FoG_trial_2_emg.csv\n",
      "53/174: 正常/P351_M050_2_A_FoG_trial_3_emg.csv\n",
      "54/174: 正常/P351_M050_2_A_Walking_trial_2_emg.csv\n",
      "55/174: 正常/P351_M050_2_A_Walking_trial_4_emg.csv\n",
      "56/174: 正常/P351_M050_2_A_Walking_trial_6_emg.csv\n",
      "57/174: 正常/P351_M050_2_B_FoG_trial_1_emg.csv\n",
      "58/174: 正常/P351_M050_2_B_FoG_trial_2_emg.csv\n",
      "59/174: 正常/P351_M050_2_B_FoG_trial_3_emg.csv\n",
      "60/174: 正常/P351_M050_2_B_Walking_trial_2_emg.csv\n",
      "61/174: 正常/P351_M050_2_B_Walking_trial_4_emg.csv\n",
      "62/174: 正常/P351_M050_2_B_Walking_trial_6_emg.csv\n",
      "63/174: 正常/P351_M050_A_FoG_trial_1_emg.csv\n",
      "64/174: 正常/P351_M050_A_FoG_trial_2_emg.csv\n",
      "65/174: 正常/P351_M050_A_FoG_trial_3_emg.csv\n",
      "66/174: 正常/P351_M050_A_Walking_trial_2_emg.csv\n",
      "67/174: 正常/P351_M050_A_Walking_trial_4_emg.csv\n",
      "68/174: 正常/P351_M050_B_FoG_trial_1_emg.csv\n",
      "69/174: 正常/P351_M050_B_FoG_trial_2_emg.csv\n",
      "70/174: 正常/P351_M050_B_FoG_trial_3_emg.csv\n",
      "71/174: 正常/P351_M050_B_Walking_trial_2_emg.csv\n",
      "72/174: 正常/P351_M050_B_Walking_trial_4_emg.csv\n",
      "73/174: 正常/P351_M050_B_Walking_trial_6_emg.csv\n",
      "74/174: 正常/P351_Msham_A_FoG_trial_1_emg.csv\n",
      "75/174: 正常/P351_Msham_A_FoG_trial_2_emg.csv\n",
      "76/174: 正常/P351_Msham_A_FoG_trial_3_emg.csv\n",
      "77/174: 正常/P351_Msham_A_Walking_trial_2_emg.csv\n",
      "78/174: 正常/P351_Msham_A_Walking_trial_4_emg.csv\n",
      "79/174: 正常/P351_Msham_A_Walking_trial_6_emg.csv\n",
      "80/174: 正常/P351_Msham_B_FoG_trial_1_emg.csv\n",
      "81/174: 正常/P351_Msham_B_FoG_trial_2_emg.csv\n",
      "82/174: 正常/P351_Msham_B_FoG_trial_3_emg.csv\n",
      "83/174: 正常/P351_Msham_B_Walking_trial_2_emg.csv\n",
      "84/174: 正常/P351_Msham_B_Walking_trial_4_emg.csv\n",
      "85/174: 正常/P351_Msham_B_Walking_trial_6_emg.csv\n",
      "86/174: 正常/P379_M050_A_Walking_trial_2_emg.csv\n",
      "87/174: 正常/P379_M050_A_Walking_trial_3_emg.csv\n",
      "88/174: 正常/P379_M050_B_Walking_trial_2_emg.csv\n",
      "89/174: 正常/P379_Msham_B_Walking_trial_6_emg.csv\n",
      "90/174: 正常/P533_M050_A_Walking_trial_1_emg.csv\n",
      "91/174: 正常/P533_M050_A_Walking_trial_2_emg.csv\n",
      "92/174: 正常/P533_M050_B_Walking_trial_2_emg.csv\n",
      "93/174: 正常/P533_M050_B_Walking_trial_3_emg.csv\n",
      "94/174: 正常/P533_M100_A_Walking_trial_2_emg.csv\n",
      "95/174: 正常/P533_M100_B_Walking_trial_4_emg.csv\n",
      "96/174: 正常/P551_M50_B_Walking_trial_6_emg.csv\n",
      "97/174: 正常/P623_M050_2_A_Walking_trial_2_emg.csv\n",
      "98/174: 正常/P623_M050_2_A_Walking_trial_4_emg.csv\n",
      "99/174: 正常/P623_M050_2_A_Walking_trial_6_emg.csv\n",
      "100/174: 正常/P623_M050_2_B_Walking_trial_2_emg.csv\n",
      "101/174: 正常/P623_M050_2_B_Walking_trial_6_emg.csv\n",
      "102/174: 正常/P623_M050_A_Walking_trial_4_emg.csv\n",
      "103/174: 正常/P623_M100_A_Walking_trial_4_emg.csv\n",
      "104/174: 正常/P623_M100_B_Walking_trial_4_emg.csv\n",
      "105/174: 正常/P623_Msham_A_Walking_trial_4_emg.csv\n",
      "106/174: 正常/P623_Msham_A_Walking_trial_6_emg.csv\n",
      "107/174: 正常/P623_Msham_B_Walking_trial_2_emg.csv\n",
      "108/174: 正常/P623_Msham_B_Walking_trial_4_emg.csv\n",
      "109/174: 正常/P645_M050_A_Walking_trial_2_emg.csv\n",
      "110/174: 正常/P645_M050_A_Walking_trial_3_emg.csv\n",
      "111/174: 正常/P645_M050_B_Walking_trial_2_emg.csv\n",
      "112/174: 正常/P645_M050_B_Walking_trial_3_emg.csv\n",
      "113/174: 正常/P812_M050_2_A_FoG_trial_1_emg.csv\n",
      "114/174: 正常/P812_M050_2_A_FoG_trial_3_emg.csv\n",
      "115/174: 正常/P812_M050_2_A_Walking_trial_2_emg.csv\n",
      "116/174: 正常/P812_M050_2_A_Walking_trial_3_emg.csv\n",
      "117/174: 正常/P812_M050_2_B_Walking_1_trial_4_emg.csv\n",
      "118/174: 正常/P812_M050_A_FoG_trial_1_emg.csv\n",
      "119/174: 正常/P812_M050_A_FoG_trial_2_emg.csv\n",
      "120/174: 正常/P812_M050_A_FoG_trial_3_emg.csv\n",
      "121/174: 正常/P812_M050_A_Walking_trial_1_emg.csv\n",
      "122/174: 正常/P812_M050_A_Walking_trial_2_emg.csv\n",
      "123/174: 正常/P812_M050_B_FoG_trial_1_emg.csv\n",
      "124/174: 正常/P812_M050_B_FoG_trial_2_emg.csv\n",
      "125/174: 正常/P812_M050_B_FoG_trial_3_emg.csv\n",
      "126/174: 正常/P812_M050_B_Walking_trial_1_emg.csv\n",
      "127/174: 正常/P812_M050_B_Walking_trial_2_emg.csv\n",
      "128/174: 正常/P812_M100_A_FoG_trial_1_emg.csv\n",
      "129/174: 正常/P812_M100_A_Walking_trial_3_emg.csv\n",
      "130/174: 正常/P812_M100_B_FoG_trial_1_emg.csv\n",
      "131/174: 正常/P812_M100_B_FoG_trial_3_emg.csv\n",
      "132/174: 正常/P812_M100_B_Walking2_trial_1_emg.csv\n",
      "133/174: 正常/P812_M100_B_Walking2_trial_2_emg.csv\n",
      "134/174: 正常/P876_M100_B_FoG_trial_1_emg.csv\n",
      "135/174: 正常/P876_M100_B_FoG_trial_2_emg.csv\n",
      "136/174: 正常/P876_M100_B_FoG_trial_3_emg.csv\n",
      "137/174: 正常/P876_M100_B_Walking_trial_4_emg.csv\n",
      "138/174: 正常/P876_M100_B_Walking_trial_6_emg.csv\n",
      "139/174: 正常/P940_M050_2_A_FoG_trial_3_emg.csv\n",
      "140/174: 正常/P940_M050_2_A_FoG_trial_4_emg.csv\n",
      "141/174: 正常/P940_M050_2_A_Walking_trial_2_emg.csv\n",
      "142/174: 正常/P940_M050_2_B_FoG_trial_1_emg.csv\n",
      "143/174: 正常/P940_M050_2_B_Walking_trial_2_emg.csv\n",
      "144/174: 正常/P940_M050_2_B_Walking_trial_4_emg.csv\n",
      "145/174: 正常/P940_M050_2_B_Walking_trial_6_emg.csv\n",
      "146/174: 正常/P940_M050_A_FoG_trial_2_emg.csv\n",
      "147/174: 正常/P940_M050_A_FoG_trial_3_emg.csv\n",
      "148/174: 正常/P940_M050_A_Walking_trial_2_emg.csv\n",
      "149/174: 正常/P940_M050_A_Walking_trial_4_emg.csv\n",
      "150/174: 正常/P940_M050_A_Walking_trial_6_emg.csv\n",
      "151/174: 正常/P940_M050_B_FoG_trial_1_emg.csv\n",
      "152/174: 正常/P940_M050_B_FoG_trial_2_emg.csv\n",
      "153/174: 正常/P940_M050_B_FoG_trial_3_emg.csv\n",
      "154/174: 正常/P940_M050_B_Walking_trial_2_emg.csv\n",
      "155/174: 正常/P940_M050_B_Walking_trial_4_emg.csv\n",
      "156/174: 正常/P940_M050_B_Walking_trial_6_emg.csv\n",
      "157/174: 正常/P940_M100_A_FoG_trial_1_emg.csv\n",
      "158/174: 正常/P940_M100_A_FoG_trial_2_emg.csv\n",
      "159/174: 正常/P940_M100_A_FoG_trial_3_emg.csv\n",
      "160/174: 正常/P940_M100_A_Walking_trial_2_emg.csv\n",
      "161/174: 正常/P940_M100_A_Walking_trial_4_emg.csv\n",
      "162/174: 正常/P940_M100_A_Walking_trial_6_emg.csv\n",
      "163/174: 正常/P940_M100_B_FoG_trial_2_emg.csv\n",
      "164/174: 正常/P940_M100_B_FoG_trial_3_emg.csv\n",
      "165/174: 正常/P940_M100_B_Walking_2_trial_2_emg.csv\n",
      "166/174: 正常/P940_M100_B_Walking_2_trial_6_emg.csv\n",
      "167/174: 正常/P940_MSham_A_FoG_trial_1_emg.csv\n",
      "168/174: 正常/P940_MSham_A_FoG_trial_3_emg.csv\n",
      "169/174: 正常/P940_MSham_A_Walking_trial_2_emg.csv\n",
      "170/174: 正常/P940_MSham_A_Walking_trial_4_emg.csv\n",
      "171/174: 正常/P940_MSham_A_Walking_trial_6_emg.csv\n",
      "172/174: 正常/P940_MSham_B_Walking_trial_2_emg.csv\n",
      "173/174: 正常/P940_MSham_B_Walking_trial_4_emg.csv\n",
      "174/174: 正常/P940_MSham_B_Walking_trial_6_emg.csv\n",
      "Duration: 1566.829508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data and labels of df2 or df3\n",
    "sc = StandardScaler(with_mean = True)\n",
    "#sc = MinMaxScaler()\n",
    "ind = df2.iloc[1].isna()\n",
    "files = np.concatenate([np.array(df.columns),np.array('正常/'+df2.columns[ind])])\n",
    "#files = np.array(df.columns[[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]])#[[2,5,6,7,8,10,11,12,13,16,17,18,19,20]])\n",
    "N = len(files)\n",
    "#sc = StandardScaler(with_mean=False)\n",
    "width = 1024\n",
    "stride = 512\n",
    "start = time.time()\n",
    "i = 0\n",
    "X = []\n",
    "Y = []\n",
    "X2 = []\n",
    "Y2 = []\n",
    "X3 = []\n",
    "Y3 = []\n",
    "F = []\n",
    "F2 = []\n",
    "F3 = []\n",
    "for file in files:\n",
    "    i += 1\n",
    "    if file.find('G04')==0:\n",
    "        continue\n",
    "    emg_data = pd.read_csv('./data/'+file)\n",
    "    #emg_data.iloc[:,3:] = \n",
    "    emg_data = emg_data.dropna().reset_index(drop=True)\n",
    "    #emg_data.iloc[:,3:]=normalize(emg_data.iloc[:,3:],axis=0)\n",
    "    #emg_data.iloc[:,3:] = sc.fit_transform(emg_data.iloc[:,3:])\n",
    "    #for j in (0,1,4,5):\n",
    "        #ind = abs(zscore(emg_data.iloc[:,j+3]))>10\n",
    "        #emg_data=emg_data.loc[~ind,:]\n",
    "        #ind_p = zscore(emg_data.iloc[:,j+3])>10\n",
    "        #ind_n = zscore(emg_data.iloc[:,j+3])<-10\n",
    "        #emg_data.loc[ind_p,emg_data.columns[3+j]] = emg_data.loc[~ind_p,emg_data.columns[3+j]].max()\n",
    "        #emg_data.loc[ind_n,emg_data.columns[3+j]] = emg_data.loc[~ind_n,emg_data.columns[3+j]].min()\n",
    "    fn = 300\n",
    "    wn=2*fn/1000\n",
    "    fn1 = 400\n",
    "    wn1 = 2*fn1/1000\n",
    "    #fs = 1000.0  # Sample frequency (Hz)\n",
    "    #f0 = 50  # Frequency to be removed from signal (Hz)\n",
    "    #Q = 100.0  # Quality factor\n",
    "    # Design notch filter\n",
    "    #b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "    #b, a = signal.butter(4, [wn,wn1], 'bandpass')\n",
    "    b, a = signal.butter(4, [wn], 'lowpass')\n",
    "    #b, a = signal.butter(4, [wn], 'highpass')\n",
    "    #for j in ['LEFT_TA','LEFT_TS','LEFT_BF','LEFT_RF','RIGHT_TA','RIGHT_TS','RIGHT_BF','RIGHT_RF']:\n",
    "        #emg_data.loc[:,j] = signal.filtfilt(b, a, emg_data.loc[:,j])\n",
    "        #emg_data.loc[:,j] = signal.filtfilt(b1, a1, emg_data.loc[:,j])\n",
    "    \"\"\"if file==df.columns[5]:\n",
    "        print(file)\n",
    "        fs = 1000.0  # Sample frequency (Hz)\n",
    "        f0 = 72  # Frequency to be removed from signal (Hz)\n",
    "        Q = 50.0  # Quality factor\n",
    "        # Design notch filter\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TA'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TA'])\n",
    "        f0 = 75  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TS'])\n",
    "        f0 = 13.2  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'RIGHT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'RIGHT_TS'])\n",
    "    if file==df.columns[6]:\n",
    "        print(file)\n",
    "        fs = 1000.0  # Sample frequency (Hz)\n",
    "        f0 = 40  # Frequency to be removed from signal (Hz)\n",
    "        Q = 100.0  # Quality factor\n",
    "        # Design notch filter\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TA'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TA'])\n",
    "        f0 = 26.5  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TS'])\n",
    "        f0 = 13.2  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TS'])\n",
    "        f0 = 48  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        #emg_data.loc[:,'LEFT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TS'])\n",
    "        f0 = 50  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'RIGHT_TA'] = signal.filtfilt(b1, a1, emg_data.loc[:,'RIGHT_TA'])\n",
    "    if file==df.columns[7]:\n",
    "        print(file)\n",
    "        fs = 1000.0  # Sample frequency (Hz)\n",
    "        f0 = 13.2  # Frequency to be removed from signal (Hz)\n",
    "        Q = 50.0  # Quality factor\n",
    "        # Design notch filter\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TS'])\n",
    "        emg_data.loc[:,'RIGHT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'RIGHT_TS'])\n",
    "        f0 = 26.5  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TS'])\n",
    "        f0 = 50  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TS'])\n",
    "    \"\"\"\n",
    "    #emg_data.iloc[:,3:] = sc.fit_transform(emg_data.iloc[:,3:])\n",
    "    #emg_data.iloc[:,3:]=normalize(emg_data.iloc[:,3:],axis=0)\n",
    "    x_raw,y = dp.generate_window_slide_data_time_continue(emg_data,width=width,\n",
    "                                        stride=stride,\n",
    "                                        scaler=True,\n",
    "                                        same_label=True)\n",
    "    #x=np.abs(x)\n",
    "    #x=dp.lowpass_filter(x,300)\n",
    "    #x=dp.mean_smooth(x)\n",
    "    shape = x_raw.shape\n",
    "    x = np.zeros(shape)\n",
    "    #x = x_raw\n",
    "    for n in range(shape[0]):\n",
    "        x[n,:,:] = dp.detrend(x_raw[n,:,:],50)\n",
    "        for c in range(shape[2]):\n",
    "            x[n,:,c] = signal.filtfilt(b,a,x[n,:,c])\n",
    "    \n",
    "    ind1 = []\n",
    "    ind2 = []\n",
    "    ind3 = []\n",
    "    l = len(y)\n",
    "    for j in set(y):\n",
    "        ind = np.where(y == j)[0].tolist()\n",
    "        l_t = len(ind)\n",
    "        #if (j==1)&((file==df.columns[5])|(file==df.columns[17])):\n",
    "            #continue\n",
    "#         if file == files[6]:\n",
    "#             ind3 += ind\n",
    "#             print('X2')\n",
    "        #else:\n",
    "        ind1 += ind[:int(l_t*0.6)]\n",
    "        ind2 += ind[int(l_t*0.6):int(l_t*0.8)]\n",
    "        ind3 += ind[int(l_t*0.8):]\n",
    "\n",
    "    l1 = len(ind1)\n",
    "    l2 = len(ind2)\n",
    "    l3 = len(ind3)\n",
    "\n",
    "    fi = [file]*len(ind1)\n",
    "    fi2 = [file]*len(ind2)\n",
    "    fi3 = [file]*len(ind3)\n",
    "    \n",
    "    X += x[ind1].tolist()\n",
    "    Y += y[ind1].tolist()\n",
    "    \n",
    "    X2 += x[ind2].tolist()\n",
    "    Y2 += y[ind2].tolist()\n",
    "    \n",
    "    X3 += x[ind3].tolist()\n",
    "    Y3 += y[ind3].tolist()\n",
    "    \n",
    "    F += fi\n",
    "    F2 += fi2\n",
    "    F3 += fi3\n",
    "    print('%d/%d: '%(i,N)+file)\n",
    "\n",
    "ind_c = [True,True,False,False,True,True,False,False]\n",
    "X = np.array(X)#[:,:,ind_c]\n",
    "Y = np.array(Y)\n",
    "X2 = np.array(X2)#[:,:,ind_c]\n",
    "Y2 = np.array(Y2)\n",
    "X3 = np.array(X3)#[:,:,ind_c]\n",
    "Y3 = np.array(Y3)\n",
    "end = time.time()\n",
    "duration = end-start\n",
    "print('Duration: %f'%(duration))\n",
    "player.Beep(1000,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_processing' from 'E:\\\\Document\\\\jupyter\\\\Master Thesis\\\\data_processing.py'>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp\n",
    "imp.reload(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#width = 256\n",
    "threshold_WAMP = 1\n",
    "threshold_ZC = 0.00\n",
    "#threshold_ZC = np.linspace(-4,4,11)\n",
    "threshold_SSC = 0.001\n",
    "bins=3\n",
    "bound = 3\n",
    "HIST_range = (-bound,bound)\n",
    "level = 3\n",
    "num = 3\n",
    "wavelet='db7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = dp.generate_feature_pd(X,threshold_WAMP=threshold_WAMP,\n",
    "                                 threshold_ZC=threshold_ZC,\n",
    "                                 threshold_SSC=threshold_SSC,\n",
    "                                 bins=bins,ranges=HIST_range,\n",
    "                                 num = num,\n",
    "                                 wavelet = wavelet, level = level)\n",
    "feature2 = dp.generate_feature_pd(X2,threshold_WAMP=threshold_WAMP,\n",
    "                                  threshold_ZC=threshold_ZC,\n",
    "                                  threshold_SSC=threshold_SSC,\n",
    "                                  bins=bins,ranges=HIST_range,\n",
    "                                  num = num,\n",
    "                                  wavelet = wavelet, level = level)\n",
    "feature3 = dp.generate_feature_pd(X3,threshold_WAMP=threshold_WAMP,\n",
    "                                  threshold_ZC=threshold_ZC,\n",
    "                                  threshold_SSC=threshold_SSC,\n",
    "                                  bins=bins,ranges=HIST_range,\n",
    "                                  num = num,\n",
    "                                  wavelet = wavelet, level = level)\n",
    "player.Beep(1000,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = ((Y==0)|(Y==2)|(Y==6))\n",
    "ind2 = ((Y2==0)|(Y2==2)|(Y2==6))\n",
    "ind3 = ((Y3==0)|(Y3==2)|(Y3==6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler((0,1))\n",
    "#scaler = StandardScaler(with_mean=True)\n",
    "feature = scaler.fit_transform(feature)\n",
    "feature2 = scaler.fit_transform(feature2)\n",
    "feature3 = scaler.fit_transform(feature3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skb = SelectKBest(chi2, k=200)\n",
    "skb = SelectKBest(mutual_info_classif, k=200)\n",
    "feature_new=skb.fit_transform(feature, Y)\n",
    "feature_new2=skb.transform(feature2)\n",
    "feature_new3=skb.transform(feature3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "vt = VarianceThreshold(threshold=0.005)\n",
    "feature_new=vt.fit_transform(feature)\n",
    "feature_new2=vt.transform(feature2)\n",
    "feature_new3=vt.transform(feature3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=150,copy=True)\n",
    "feature_new = pca.fit_transform(feature)\n",
    "feature_new2 = pca.transform(feature2)\n",
    "feature_new3 = pca.transform(feature3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfm = SelectFromModel(GradientBoostingClassifier(),max_features=200)\n",
    "feature_new = sfm.fit_transform(feature,Y)\n",
    "feature_new2 = sfm.transform(feature2)\n",
    "feature_new3 = sfm.transform(feature3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(estimator=LogisticRegression(max_iter=10000), n_features_to_select=200)\n",
    "feature_new = rfe.fit_transform(feature,Y)\n",
    "feature_new2 = rfe.transform(feature2)\n",
    "feature_new3 = rfe.transform(feature3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5111, 200)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,feature,y,binary=True,file=None):\n",
    "    if binary:\n",
    "        ind = ((y==0)|(y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "        ind1 = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "        y_01 = y.copy()\n",
    "        y_01[ind1] = 1\n",
    "        metric = 'error'\n",
    "    else:\n",
    "        ind = ((y==1)|(y==2)|(y==6))\n",
    "        y_01 = y[ind].copy()\n",
    "        metric = 'merror'\n",
    "    x_full,x_test,y_full,y_test = train_test_split(feature.loc[ind,:],y_01,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=123,\n",
    "                                                   shuffle=False)\n",
    "    x_train,x_valid,y_train,y_valid = train_test_split(x_full,y_full,\n",
    "                                                       test_size=0.2,\n",
    "                                                       random_state=555,\n",
    "                                                       shuffle=True)\n",
    "    \n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred_t=model.predict(x_test)\n",
    "    test = metrics.accuracy_score(y_test,y_pred_t)\n",
    "    y_pred_v=model.predict(x_valid)\n",
    "    valid = metrics.accuracy_score(y_valid,y_pred_v)\n",
    "    y_pred_ta=model.predict(x_train)\n",
    "    train = metrics.accuracy_score(y_train,y_pred_ta)\n",
    "    if binary == False:\n",
    "        print('train: \\n',metrics.confusion_matrix(y_train,y_pred_ta))\n",
    "        print('valid: \\n',metrics.confusion_matrix(y_valid,y_pred_v))\n",
    "        print('test: \\n',metrics.confusion_matrix(y_test,y_pred_t))\n",
    "    print('test:%f'%test)\n",
    "    print('valid:%f'%valid)\n",
    "    print('train:%f'%train)\n",
    "    if file != None:\n",
    "        model.save_model(file)\n",
    "    return train,valid,test\n",
    "\n",
    "def test_model(model,feature,y):\n",
    "    ind = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "    y_01 = y.copy()\n",
    "    y_01[ind] = 1\n",
    "    y_pred=model.predict(feature)\n",
    "    test = metrics.accuracy_score(y_01,y_pred>0.5)\n",
    "    print('acc:%f'%test)\n",
    "    return test\n",
    "\n",
    "def test_load_model(path,feature,y):\n",
    "    ind = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "    y_01 = y.copy()\n",
    "    y_01[ind] = 1\n",
    "    booster = xgb.Booster()\n",
    "    booster.load_model(path)\n",
    "    model = xgb.XGBClassifier()\n",
    "    model._Booster = booster\n",
    "    y_pred=model.predict(feature)\n",
    "    test = metrics.accuracy_score(y_01,y_pred)\n",
    "    print('acc:%f'%test)\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_no_shuffle(model,feature,y,binary=True,file=None):\n",
    "    if binary:\n",
    "        ind1 = ((y[0]==0)|(y[0]==1)|(y[0]==2)|(y[0]==3)|(y[0]==4)|(y[0]==6))\n",
    "        ind2 = ((y[1]==0)|(y[1]==1)|(y[1]==2)|(y[1]==3)|(y[1]==4)|(y[1]==6))\n",
    "        ind3 = ((y[2]==0)|(y[2]==1)|(y[2]==2)|(y[2]==3)|(y[2]==4)|(y[2]==6))\n",
    "        ind01 = ((y[0]==4)|(y[0]==1)|(y[0]==2)|(y[0]==3)|(y[0]==6))\n",
    "        ind11 = ((y[1]==4)|(y[1]==1)|(y[1]==2)|(y[1]==3)|(y[1]==6))\n",
    "        ind21 = ((y[2]==4)|(y[2]==1)|(y[2]==2)|(y[2]==3)|(y[2]==6))\n",
    "        \n",
    "        y_01 = y[0][ind1].copy()\n",
    "        y_02 = y[1][ind2].copy()\n",
    "        y_03 = y[2][ind3].copy()\n",
    "\n",
    "        #ind1 = ((y_01==1)|(y_01==2)|(y_01==6))\n",
    "        y_01[ind01] = 1\n",
    "        y_02[ind11] = 1\n",
    "        y_03[ind21] = 1\n",
    "        \n",
    "    else:\n",
    "        ind1 = ((y[0]==1)|(y[0]==2)|(y[0]==6))\n",
    "        ind2 = ((y[1]==1)|(y[1]==2)|(y[1]==6))\n",
    "        ind3 = ((y[2]==1)|(y[2]==2)|(y[2]==6))\n",
    "        #ind = ((y==2)|(y==6))\n",
    "        y_01 = y[0][ind1].copy()\n",
    "        y_02 = y[1][ind2].copy()\n",
    "        y_03 = y[2][ind3].copy()\n",
    "\n",
    "\n",
    "    x_train = feature[0][ind1]\n",
    "    y_train = y_01\n",
    "    x_valid = feature[1][ind2]\n",
    "    y_valid = y_02\n",
    "    x_test = feature[2][ind3]\n",
    "    y_test = y_03\n",
    "    #sm = BorderlineSMOTE(random_state=50)\n",
    "    #x_train,y_train = sm.fit_resample(x_train,y_train)\n",
    "#     s_ind1 = y_train == 0\n",
    "#     s_ind2 = y_train == 2\n",
    "#     s_ind3 = y_train == 6\n",
    "#     samples_weights = np.zeros(y_train.shape)\n",
    "#     samples_weights[s_ind1]=1\n",
    "#     samples_weights[s_ind2]=100\n",
    "#     samples_weights[s_ind3]=100\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    if binary:\n",
    "        y_pred_t=model.predict(x_test)\n",
    "        test = metrics.accuracy_score(y_test,y_pred_t)\n",
    "        y_pred_v=model.predict(x_valid)\n",
    "        valid = metrics.accuracy_score(y_valid,y_pred_v)\n",
    "        y_pred_ta=model.predict(x_train)\n",
    "        train = metrics.accuracy_score(y_train,y_pred_ta)        \n",
    "        print('train: \\n',metrics.confusion_matrix(y_train,y_pred_ta))\n",
    "        print('valid: \\n',metrics.confusion_matrix(y_valid,y_pred_v))\n",
    "        print('test: \\n',metrics.confusion_matrix(y_test,y_pred_t))\n",
    "        print('test:%f'%test)\n",
    "        print('valid:%f'%valid)\n",
    "        print('train:%f'%train)\n",
    "\n",
    "    else:\n",
    "        y_pred_t=model.predict(x_test)\n",
    "        test = metrics.accuracy_score(y_test,y_pred_t)\n",
    "        y_pred_v=model.predict(x_valid)\n",
    "        valid = metrics.accuracy_score(y_valid,y_pred_v)\n",
    "        y_pred_ta=model.predict(x_train)\n",
    "        train = metrics.accuracy_score(y_train,y_pred_ta)        \n",
    "        print('train: \\n',metrics.confusion_matrix(y_train,y_pred_ta))\n",
    "        print('valid: \\n',metrics.confusion_matrix(y_valid,y_pred_v))\n",
    "        print('test: \\n',metrics.confusion_matrix(y_test,y_pred_t))\n",
    "        print('test:%f'%test)\n",
    "        print('valid:%f'%valid)\n",
    "        print('train:%f'%train)\n",
    "    if file != None:\n",
    "        model.save_model(file)\n",
    "    #return train,test     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = ((y==0)|(y==2))\n",
    "y_02 = y[ind]\n",
    "y_02[y_02==2] = 1\n",
    "oh_ec = OneHotEncoder()\n",
    "y_oh = oh_ec.fit_transform(y[:,np.newaxis]).toarray()\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(feature[ind],y_02,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = ((y==1)|(y==2)|(y==3)|(y==6))\n",
    "#ind = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "ind_f = [0,1,6,42,46,57,62]\n",
    "y_01 = y[ind]\n",
    "y_01[y_01==1]=0\n",
    "y_01[y_01==2]=1\n",
    "y_01[y_01==3]=2\n",
    "y_01[y_01==6]=3\n",
    "#y_01 = y\n",
    "#y_01[ind] = 1\n",
    "oh_ec = OneHotEncoder()\n",
    "y_oh = oh_ec.fit_transform(y_01[:,np.newaxis]).toarray()\n",
    "x_full,x_test,y_full,y_test = train_test_split(feature_sc[ind][:-2000],y_01[:-2000],test_size=0.2,random_state=123)\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(x_full,y_full,test_size=0.2,random_state=555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel=\"rbf\",C=10,\n",
    "            #class_weight={0:1,2:10,6:10},\n",
    "            class_weight={0:1,1:5},\n",
    "            gamma='auto',\n",
    "           #decision_function_shape='ovo'\n",
    "           )\n",
    "#model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: \n",
      " [[4456  117]\n",
      " [  11  527]]\n",
      "valid: \n",
      " [[1518   26]\n",
      " [  12  171]]\n",
      "test: \n",
      " [[1558   53]\n",
      " [  11  184]]\n",
      "test:0.964563\n",
      "valid:0.977997\n",
      "train:0.974956\n"
     ]
    }
   ],
   "source": [
    "train_model_no_shuffle(model,(feature,feature2,feature3),\n",
    "                       (np.array(Y),np.array(Y2),np.array(Y3)),\n",
    "                       True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:0.672977\n",
      "valid:0.779115\n",
      "train:0.777857\n",
      "acc:0.744774\n",
      "0.7447739426349052\n"
     ]
    }
   ],
   "source": [
    "sc = StandardScaler(with_mean=True)\n",
    "feature_sc = sc.fit_transform(feature)\n",
    "feature2_sc = sc.transform(feature2)\n",
    "acc={}\n",
    "train,valid,test = train_model(model,feature,np.array(y),True)\n",
    "acc_rest=test_model(model,feature2,y2)\n",
    "acc['SVM'] = [train,valid,test,acc_rest]\n",
    "print(acc_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,  353,    0],\n",
       "       [   0, 1440,    0],\n",
       "       [   0,  207,    0]], dtype=int64)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_01[28844:],y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.36727542, 10.87437042, 10.70533875, ..., 11.55049708,\n",
       "       11.4378093 , 11.26877764])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_sc.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    8334\n",
       "0    5207\n",
       "3    4359\n",
       "2     560\n",
       "dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
