{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.config import Config_f\n",
    "from lib.data_set import Features\n",
    "from lib.model import SimpleModel\n",
    "from lib import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from 'E:\\\\Document\\\\jupyter\\\\Master Thesis\\\\utils.py'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp\n",
    "imp.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pywt\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest,f_classif,chi2,mutual_info_classif,VarianceThreshold,RFE,SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "\n",
    "from tensorflow.keras import layers as KL\n",
    "from tensorflow.keras import models as KM\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File name read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file name of data with various Labels\n",
    "df = pd.read_csv('./useful_data_label.csv',index_col=0) \n",
    "# read file name of data with only label 0\n",
    "df2 = pd.read_csv('./unuseful_data_label.csv',index_col=0)\n",
    "# read some of the data with only label 0\n",
    "df3 = pd.read_csv('./data/file_name.txt',header=None)\n",
    "player = ctypes.windll.kernel32\n",
    "\n",
    "ind = df2.iloc[1].isna()\n",
    "files = np.concatenate([np.array(df.columns),np.array('normal/'+df2.columns[ind])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override the base class of Config and Features for ANN Model\n",
    "class ANN_Config(Config_f):\n",
    "    NAME = 'ANN'\n",
    "    NUM_CLASSES = 2\n",
    "    EPOCHS = 300\n",
    "    BATCH_SIZE = 32\n",
    "    CLASS_WEIGHTS = None\n",
    "    COST_SENSITIVE = False\n",
    "    \n",
    "    FN_LP = 300\n",
    "    DETREND_LAMBDA = 50\n",
    "    TEST_FILES = files[[6,30,31,32,33,34,35]]\n",
    "    \n",
    "    \n",
    "class ANN_dataset(Features):\n",
    "    \n",
    "    def __init__(self,config):\n",
    "        super(ANN_dataset,self).__init__(config)\n",
    "        self.config = config\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BATCH_SIZE                     32\n",
      "BINS                           3\n",
      "CHANNELS                       ['LEFT_TA', 'LEFT_TS', 'LEFT_BF', 'LEFT_RF', 'RIGHT_TA', 'RIGHT_TS', 'RIGHT_BF', 'RIGHT_RF']\n",
      "CLASS_WEIGHTS                  None\n",
      "COST_SENSITIVE                 False\n",
      "DETREND_LAMBDA                 50\n",
      "EPOCHS                         300\n",
      "FEATURES_LIST                  ['IEMG', 'SSI', 'WL', 'ZC', 'ku', 'SSC', 'skew', 'Acti', 'AR', 'HIST', 'MDF', 'MNF', 'mDWT']\n",
      "FN_LP                          300\n",
      "LEVEL_DWT                      3\n",
      "NAME                           ANN\n",
      "NUM_CLASSES                    2\n",
      "NUM_MF                         3\n",
      "N_ENV                          20\n",
      "RANGES                         (-3, 3)\n",
      "RECT                           False\n",
      "SAME_LABEL                     True\n",
      "SCALE                          True\n",
      "SHUFFLE                        True\n",
      "STEP_SIZE                      512\n",
      "TEST_FILES                     ['G08_FoG_1_trial_1_emg.csv' 'normal/G09_Walking_trial_2_emg.csv'\n",
      " 'normal/G09_Walking_trial_4_emg.csv' 'normal/G09_Walking_trial_6_emg.csv'\n",
      " 'normal/G11_Walking_trial_2_emg.csv' 'normal/G11_Walking_trial_4_emg.csv'\n",
      " 'normal/P231_M050_A_Walking_trial_2_emg.csv']\n",
      "THRESHOLD_SSC                  0.01\n",
      "THRESHOLD_WAMP                 1\n",
      "THRESHOLD_ZC                   0.0\n",
      "TRAIN_SET_RATIO                0.8\n",
      "WAVELET_DWT                    db7\n",
      "WINDOW_SIZE                    1024\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate ANN configuration\n",
    "config = ANN_Config()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ANN_dataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n",
      "skip\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1b62b1a1a321>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Load data from files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Extract features from data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Document\\jupyter\\Master Thesis\\lib\\data_set.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(self, files)\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0memg_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memg_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchannels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             x,y = utils.generate_window_slide_data_time_continue(emg_data, \n\u001b[0m\u001b[0;32m     60\u001b[0m                                               \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                                               \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "# Choose features to use\n",
    "data.feature_list = ['IEMG', 'SSI', 'WL', 'ZC', 'ku', 'SSC', 'skew', 'Acti', 'AR', 'HIST', 'MF','MDF', 'MNF', 'mDWT']\n",
    "data.num_mf = 10\n",
    "\n",
    "# Load data from files\n",
    "data.load_data(files)\n",
    "\n",
    "# Extract features from data\n",
    "data.extract_features()\n",
    "\n",
    "X_train,Y_train,_ = data.train_set\n",
    "X_valid,Y_valid,_ = data.valid_set\n",
    "X_test, Y_test, _ = data.test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override base class of SimpleMode for ANN\n",
    "class ANN_Model(SimpleModel):\n",
    "    \n",
    "    def build(self,config):\n",
    "        \n",
    "        reg = keras.regularizers.l1(0)\n",
    "        acti = 'relu'\n",
    "        drop = 0.2\n",
    "        init = 'glorot_normal'\n",
    "\n",
    "        model = KM.Sequential()\n",
    "        #model.add(layers.BatchNormalization())\n",
    "        model.add(KL.Dense(128,\n",
    "                               kernel_initializer=init,\n",
    "                               kernel_regularizer = reg,\n",
    "                               #use_bias=False\n",
    "                         ))\n",
    "        # model.add(layers.BatchNormalization())\n",
    "        model.add(KL.Activation(acti))\n",
    "        # model.add(layers.LeakyReLU(0.3))\n",
    "        model.add(KL.Dropout(drop))\n",
    "\n",
    "        model.add(KL.Dense(64,\n",
    "                               kernel_initializer=init,\n",
    "                               kernel_regularizer = reg,\n",
    "                              # use_bias=False\n",
    "                         ))\n",
    "        # model.add(layers.BatchNormalization())\n",
    "        model.add(KL.Activation(acti))\n",
    "        # model.add(layers.LeakyReLU(0.1))\n",
    "        model.add(KL.Dropout(drop))\n",
    "\n",
    "        model.add(KL.Dense(32,\n",
    "                               kernel_initializer=init,\n",
    "                               kernel_regularizer = reg,\n",
    "                               #use_bias=False\n",
    "                         ))\n",
    "        # model.add(layers.BatchNormalization())\n",
    "        model.add(KL.Activation(acti))\n",
    "        # model.add(layers.LeakyReLU(0.1))\n",
    "        model.add(KL.Dropout(drop))\n",
    "\n",
    "        model.add(KL.Dense(16,\n",
    "                               kernel_initializer=init,\n",
    "                               kernel_regularizer = reg,\n",
    "                               #use_bias=False\n",
    "                         ))\n",
    "        # model.add(layers.BatchNormalization())\n",
    "        model.add(KL.Activation(acti))\n",
    "        # model.add(layers.LeakyReLU(0.1))\n",
    "        model.add(KL.Dropout(drop))\n",
    "\n",
    "        model.add(KL.Dense(config.NUM_CLASSES,activation='softmax'))\n",
    "\n",
    "        \n",
    "        if config.COST_SENSITIVE:\n",
    "            self.cost_matrix = config.COST_MATRIX\n",
    "            model.compile(loss=self.sparse_cost_sensitive_loss, optimizer=\"adam\", metrics=['accuracy'])\n",
    "            print('Using cost sensitive with cost matrix:\\n',np.array(self.cost_matrix))\n",
    "        else:\n",
    "            model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "            if config.CLASS_WEIGHTS != None:\n",
    "                print('Using categorical crossentropy with class weights:\\n',config.CLASS_WEIGHTS)\n",
    "            else:\n",
    "                print('Using categorical crossentropy without class weights.')\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train(self, train_dataset, val_dataset, transformer=None, callbacks=None):\n",
    "        \n",
    "        self.X_train = train_dataset[0]\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(train_dataset[0])\n",
    "        \n",
    "        scaler.fit(np.concatenate([train_dataset[0],val_dataset[0]]))\n",
    "        X_val = scaler.transform(val_dataset[0])\n",
    "        \n",
    "        if transformer != None:\n",
    "            self.transformer = transformer\n",
    "            self.transformer.fit(X_train,train_dataset[1])\n",
    "            X_train = self.transformer.transform(X_train)\n",
    "            X_val = self.transformer.transform(X_val)\n",
    "        else:\n",
    "            self.transformer = None\n",
    "\n",
    "        self.simple_model.fit(X_train,\n",
    "                              train_dataset[1],\n",
    "                              validation_data=(X_val,val_dataset[1]),\n",
    "                              epochs=self.config.EPOCHS,\n",
    "                              batch_size=self.config.BATCH_SIZE,\n",
    "                              class_weight=self.config.CLASS_WEIGHTS,\n",
    "                              callbacks=callbacks,\n",
    "                              shuffle=True)\n",
    "\n",
    "    def sparse_cost_sensitive_loss (self,y_true,y_pred):\n",
    "        cost_matrix = self.cost_matrix\n",
    "        batch_cost_matrix = tf.nn.embedding_lookup(cost_matrix, tf.argmax(y_true,axis=1))\n",
    "        eps = 1e-6\n",
    "        probability = tf.clip_by_value(y_pred, eps, 1-eps)\n",
    "        cost_values = tf.math.log(1-probability)*batch_cost_matrix\n",
    "        loss = tf.reduce_mean(-tf.reduce_sum(cost_values, axis=1))\n",
    "        return loss\n",
    "    \n",
    "    def model_metrics(self,data,label):\n",
    "        pred = self.predict(data)\n",
    "        acc = accuracy_score(np.argmax(label,axis=1),np.argmax(pred,axis=1))\n",
    "        cm = confusion_matrix(np.argmax(label,axis=1),np.argmax(pred,axis=1))\n",
    "        return acc,cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split and processing for model\n",
    "class_id = [2,6]\n",
    "binary = True\n",
    "x_train,y_train,x_valid,y_valid,x_test,y_test = utils.data_split_oh((X_train,X_valid,X_test),\n",
    "                                                                    (Y_train,Y_valid,Y_test),\n",
    "                                                                    class_id,\n",
    "                                                                    binary,\n",
    "                                                                    random_state = 555)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using categorical crossentropy without class weights.\n"
     ]
    }
   ],
   "source": [
    "config.EPOCHS = 300\n",
    "config.NUM_CLASSES = 2\n",
    "config.CLASS_WEIGHTS = None#{0:1,1:5}\n",
    "\n",
    "config.COST_MATRIX = tf.constant([[0,1.],\n",
    "              [10,0]])\n",
    "\n",
    "if binary:\n",
    "    config.COST_SENSITIVE = True\n",
    "    config.NUM_CLASSES = 2\n",
    "else:\n",
    "    config.COST_SENSITIVE = False\n",
    "    config.NUM_CLASSES = len(class_id)\n",
    "\n",
    "# Generate ANN Model\n",
    "ANN_model = ANN_Model('ANN',config,'./model/ANN/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "14/14 [==============================] - 1s 23ms/step - loss: 0.6374 - accuracy: 0.6284 - val_loss: 0.5202 - val_accuracy: 0.6879\n",
      "Epoch 2/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.5522 - accuracy: 0.6112 - val_loss: 0.4008 - val_accuracy: 0.8369\n",
      "Epoch 3/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.4437 - accuracy: 0.7323 - val_loss: 0.3095 - val_accuracy: 0.9220\n",
      "Epoch 4/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3588 - accuracy: 0.8348 - val_loss: 0.2585 - val_accuracy: 0.9220\n",
      "Epoch 5/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.2863 - accuracy: 0.8935 - val_loss: 0.2213 - val_accuracy: 0.9220\n",
      "Epoch 6/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.2414 - accuracy: 0.9411 - val_loss: 0.1959 - val_accuracy: 0.9291\n",
      "Epoch 7/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.2005 - accuracy: 0.9483 - val_loss: 0.1827 - val_accuracy: 0.9220\n",
      "Epoch 8/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.1666 - accuracy: 0.9510 - val_loss: 0.1756 - val_accuracy: 0.9291\n",
      "Epoch 9/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.1136 - accuracy: 0.9792 - val_loss: 0.1728 - val_accuracy: 0.9291\n",
      "Epoch 10/300\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0973 - accuracy: 0.9818 - val_loss: 0.1622 - val_accuracy: 0.9291\n",
      "Epoch 11/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0806 - accuracy: 0.9858 - val_loss: 0.1513 - val_accuracy: 0.9362\n",
      "Epoch 12/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0684 - accuracy: 0.9853 - val_loss: 0.1363 - val_accuracy: 0.9574\n",
      "Epoch 13/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0622 - accuracy: 0.9799 - val_loss: 0.1353 - val_accuracy: 0.9574\n",
      "Epoch 14/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0412 - accuracy: 0.9896 - val_loss: 0.1277 - val_accuracy: 0.9504\n",
      "Epoch 15/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0491 - accuracy: 0.9839 - val_loss: 0.1275 - val_accuracy: 0.9433\n",
      "Epoch 16/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0482 - accuracy: 0.9803 - val_loss: 0.1185 - val_accuracy: 0.9574\n",
      "Epoch 17/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0209 - accuracy: 0.9990 - val_loss: 0.1095 - val_accuracy: 0.9645\n",
      "Epoch 18/300\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0358 - accuracy: 0.9922 - val_loss: 0.1196 - val_accuracy: 0.9433\n",
      "Epoch 19/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0155 - accuracy: 0.9956 - val_loss: 0.1484 - val_accuracy: 0.9504\n",
      "Epoch 20/300\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0168 - accuracy: 0.9922 - val_loss: 0.1517 - val_accuracy: 0.9504\n",
      "Epoch 21/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0308 - accuracy: 0.9931 - val_loss: 0.1306 - val_accuracy: 0.9504\n",
      "Epoch 22/300\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.1446 - val_accuracy: 0.9574\n",
      "Epoch 23/300\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0208 - accuracy: 0.9951 - val_loss: 0.2015 - val_accuracy: 0.9362\n",
      "Epoch 24/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.1928 - val_accuracy: 0.9362\n",
      "Epoch 25/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0084 - accuracy: 0.9969 - val_loss: 0.1657 - val_accuracy: 0.9433\n",
      "Epoch 26/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9990 - val_loss: 0.1474 - val_accuracy: 0.9504\n",
      "Epoch 27/300\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.1625 - val_accuracy: 0.9433\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=150,copy=True)\n",
    "sfm = SelectFromModel(GradientBoostingClassifier(),max_features=80)\n",
    "rfe = RFE(estimator=LogisticRegression(max_iter=10000), n_features_to_select=100)\n",
    "vt = VarianceThreshold(threshold=0.01)\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience = 10,\n",
    "                                             monitor = 'val_loss', \n",
    "                                             #baseline = 0.9,\n",
    "                                             restore_best_weights=True)\n",
    "\n",
    "ANN_model.train((x_train,y_train),\n",
    "                (x_valid,y_valid),\n",
    "                pca,\n",
    "                [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_train: 1.000000\n",
      "confusion_matrix:\n",
      " [[259   0]\n",
      " [  0 161]] \n",
      "\n",
      "acc_valid: 0.964539\n",
      "confusion_matrix:\n",
      " [[93  3]\n",
      " [ 2 43]] \n",
      "\n",
      "acc_test: 0.926174\n",
      "confusion_matrix:\n",
      " [[55  2]\n",
      " [ 9 83]]\n"
     ]
    }
   ],
   "source": [
    "acc_train,cm_train = ANN_model.model_metrics(x_train,y_train)\n",
    "acc_valid,cm_valid = ANN_model.model_metrics(x_valid,y_valid)\n",
    "acc_test,cm_test = ANN_model.model_metrics(x_test,y_test)\n",
    "print('acc_train: %f\\nconfusion_matrix:\\n'%acc_train,cm_train,'\\n')\n",
    "print('acc_valid: %f\\nconfusion_matrix:\\n'%acc_valid,cm_valid,'\\n')\n",
    "print('acc_test: %f\\nconfusion_matrix:\\n'%acc_test,cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
