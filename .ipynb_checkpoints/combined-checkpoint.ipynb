{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import data_processing as dp\n",
    "from scipy import signal\n",
    "from scipy.stats import skew\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv('./processed data/dataframe_W256_S64_DWTLmax_samelabel_mc.csv')\n",
    "\n",
    "# read file name of data with various Labels\n",
    "df = pd.read_csv('./useful_data_label.csv',index_col=0) \n",
    "drop = 'G07_Freezing_Trial1_trial_1_emg.csv'\n",
    "drop2= 'P812_M050_2_B_FoG_trial_2_emg.csv'\n",
    "drop3= 'P812_M050_2_B_FoG_trial_1_emg.csv'\n",
    "ind_drop = (df.columns!=drop)# & (df.columns!=drop2) & (df.columns!=drop3)\n",
    "# read file name of data with only label 0\n",
    "df2 = pd.read_csv('./unuseful_data_label.csv',index_col=0)\n",
    "# read some of the data with only label 0\n",
    "df3 = pd.read_csv('./data/file_name.txt',header=None)\n",
    "\n",
    "files = np.concatenate([np.array(df.columns),np.array(df3.loc[:,0])])\n",
    "ind = Data.File.isin(df.columns)\n",
    "ind2 = Data.File == drop\n",
    "#ind = (Data.File != drop) & (Data.File != drop2)\n",
    "Data_sel = Data[ind]\n",
    "Data_rest = Data[ind2]\n",
    "#ind2 = Data_rest.File == drop\n",
    "#Data_rest = Data_rest[ind2]\n",
    "#Data_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G04_FoG_trial_1_emg.csv\n",
      "G04_FoG_trial_2_emg.csv\n",
      "G06_FoG_trial_1_emg.csv\n",
      "G06_FoG_trial_2_emg.csv\n",
      "G06_FoG_trial_3_emg.csv\n",
      "G07_Freezing_Trial1_trial_1_emg.csv\n",
      "G08_FoG_1_trial_1_emg.csv\n",
      "G08_FoG_2_trial_1_emg.csv\n",
      "G11_FoG_trial_1_emg.csv\n",
      "G11_FoG_trial_2_emg.csv\n",
      "P379_M050_2_OFF_A_FoG_trial_1_emg.csv\n",
      "P379_M050_2_OFF_A_FoG_trial_2_emg.csv\n",
      "P379_M050_2_OFF_A_FoG_trial_3_emg.csv\n",
      "P379_M050_2_OFF_B_FoG_trial_1_emg.csv\n",
      "P379_M050_2_OFF_B_FoG_trial_2_emg.csv\n",
      "P379_M050_2_OFF_B_FoG_trial_3_emg.csv\n",
      "P551_M050_2_A_FoG_trial_1_emg.csv\n",
      "P551_M050_2_B_FoG_trial_1_emg.csv\n",
      "P551_M050_2_B_FoG_trial_2_emg.csv\n",
      "P812_M050_2_B_FoG_trial_1_emg.csv\n",
      "P812_M050_2_B_FoG_trial_2_emg.csv\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G04_FoG_trial_1_emg.csv\n",
      "G04_FoG_trial_2_emg.csv\n",
      "G06_FoG_trial_1_emg.csv\n",
      "G06_FoG_trial_2_emg.csv\n",
      "G06_FoG_trial_3_emg.csv\n",
      "G07_Freezing_Trial1_trial_1_emg.csv\n",
      "G08_FoG_1_trial_1_emg.csv\n",
      "G08_FoG_2_trial_1_emg.csv\n",
      "G11_FoG_trial_1_emg.csv\n",
      "G11_FoG_trial_2_emg.csv\n",
      "P379_M050_2_OFF_A_FoG_trial_1_emg.csv\n",
      "P379_M050_2_OFF_A_FoG_trial_2_emg.csv\n",
      "P379_M050_2_OFF_A_FoG_trial_3_emg.csv\n",
      "P379_M050_2_OFF_B_FoG_trial_1_emg.csv\n",
      "P379_M050_2_OFF_B_FoG_trial_2_emg.csv\n",
      "P379_M050_2_OFF_B_FoG_trial_3_emg.csv\n",
      "P551_M050_2_A_FoG_trial_1_emg.csv\n",
      "P551_M050_2_B_FoG_trial_1_emg.csv\n",
      "P551_M050_2_B_FoG_trial_2_emg.csv\n",
      "P812_M050_2_B_FoG_trial_1_emg.csv\n",
      "P812_M050_2_B_FoG_trial_2_emg.csv\n"
     ]
    }
   ],
   "source": [
    "feature = pd.DataFrame()\n",
    "feature2 = pd.DataFrame()\n",
    "y_ann = pd.DataFrame()\n",
    "y2_ann = pd.DataFrame()\n",
    "m = 0\n",
    "for i in df.columns:\n",
    "    print(i)\n",
    "    ind = Data.File==i\n",
    "    temp = Data[ind].reset_index(drop=True)\n",
    "    ind3 = []\n",
    "    ind4 = []\n",
    "    for j in set(temp.Label):\n",
    "        #ind2 = temp.Label == j\n",
    "        #temp2 = temp[ind2]\n",
    "        #l = len(temp2)\n",
    "        #feature = pd.concat([feature,temp2.iloc[:int(0.8*l),1:-1]])\n",
    "        #y_ann = pd.concat([y_ann,temp2.iloc[:int(0.8*l),0]])\n",
    "        #feature2 = pd.concat([feature2,temp2.iloc[int(0.8*l):,1:-1]])\n",
    "        #y2_ann = pd.concat([y2_ann,temp2.iloc[int(0.8*l):,0]])\n",
    "        ind2 = np.where(temp.Label == j)[0].tolist()\n",
    "        l_t = len(ind2)\n",
    "        ind3 += ind2[:int(l_t*0.8)]\n",
    "        ind4 += ind2[int(l_t*0.8):] \n",
    "    feature = pd.concat([feature,temp.iloc[ind3,1:-1]])\n",
    "    y_ann = pd.concat([y_ann,temp.iloc[ind3,0]])\n",
    "    feature2 = pd.concat([feature2,temp.iloc[ind4,1:-1]])\n",
    "    y2_ann = pd.concat([y2_ann,temp.iloc[ind4,0]])\n",
    "y_ann = np.array(y_ann)[:,0]\n",
    "y2_ann = np.array(y2_ann)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,Model,callbacks,regularizers,models\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder,normalize,MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE,BorderlineSMOTE,ADASYN,SVMSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ind = ((y[:20000]==1) | (y[:20000]==2) | (y[:20000]==3) | (y[:20000]==4) | (y[:20000]==6))\n",
    "#y_02 = y[:20000]\n",
    "#ind = ((y==1) | (y==2) | (y==3) | (y==4) | (y==6))\n",
    "file = './processed data/cwt_W256_S64_Wc32_mexh_split.hdf5'\n",
    "with h5py.File(file,'r') as f:\n",
    "    cwtmatr = f['features']\n",
    "    y = f['labels'][...]\n",
    "    cwtmatr2 = f['features2']\n",
    "    y2 = f['labels2'][...]\n",
    "    ind1 = ((y==1) | (y==2) | (y==6))\n",
    "    ind = np.where(ind1==True)\n",
    "    #print(ind)\n",
    "    oc = OneHotEncoder()\n",
    "    #y_02 = y.copy()\n",
    "    y_02 = oc.fit_transform(y[ind1,np.newaxis]).toarray()\n",
    "    #y_02[ind]=1\n",
    "    ind2 = ((y2==1) | (y2==2) | (y2==6))\n",
    "    ind22 = np.where(ind2==True)\n",
    "    X2 = cwtmatr2[ind22]\n",
    "    Y2 = oc.transform(y2[ind2,np.newaxis]).toarray()\n",
    "    X_full,X_test,y_full,y_test = train_test_split(cwtmatr[ind],y_02,test_size = 0.2,shuffle = True,random_state=123)\n",
    "    #X_full,X_test,y_full,y_test= train_test_split(X_full1,y_full1,test_size = 0.2,shuffle = True,random_state=123)\n",
    "    #X_train,X_valid,y_train,y_valid= train_test_split(X_full,y_full,test_size = 0.2,random_state=555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 3., 3., 3.])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,feature,y,binary=True,file=None):\n",
    "    if binary:\n",
    "        ind = ((y==0)|(y==1)|(y==2)|(y==6))\n",
    "        #ind1 = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "        y_01 = y[ind].copy()\n",
    "        ind1 = ((y_01==1)|(y_01==2)|(y_01==6))\n",
    "        y_01[ind1] = 1\n",
    "        oc = OneHotEncoder()\n",
    "        y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "        cw = None#{0:1,1:5}\n",
    "    else:\n",
    "        ind = ((y==1)|(y==2)|(y==6))\n",
    "        #ind = ((y==1)|(y==6))\n",
    "        y_01 = y[ind].copy()\n",
    "        oc = OneHotEncoder()\n",
    "        y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "        cw = None#{0:5,1:1,2:1}#{0:2,1:1,2:10,3:2}\n",
    "    x_full,x_test,y_full,y_test = train_test_split(np.array(feature)[ind,:],y_01,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=123,\n",
    "                                                   shuffle=True)\n",
    "    x_train,x_valid,y_train,y_valid = train_test_split(x_full,y_full,\n",
    "                                                       test_size=0.25,\n",
    "                                                       random_state=555,\n",
    "                                                       shuffle=True)\n",
    "    \n",
    "    #sm = BorderlineSMOTE(random_state=50,kind='borderline-2')\n",
    "    #sm = SMOTE(random_state=50)\n",
    "    #print(y_full.shape)\n",
    "    #x_full,y_full = sm.fit_resample(x_full,y_full)\n",
    "    #print(y_full_n.shape)\n",
    "    sc = StandardScaler(with_mean=True)\n",
    "    #sc = MinMaxScaler()\n",
    "    x_train = sc.fit_transform(x_full)\n",
    "    pca = PCA(n_components=100)\n",
    "    #x_train = pca.fit_transform(x_full)\n",
    "    #x_valid = sc.transform(x_valid)\n",
    "    x_test = sc.transform(x_test)\n",
    "    #x_test = pca.transform(x_test)\n",
    "    #x_train = x_full\n",
    "    \n",
    "    early_stopping = callbacks.EarlyStopping(patience = 20,\n",
    "                                             monitor = 'val_loss', \n",
    "                                             restore_best_weights=True)\n",
    "    history = model.fit([x_train,X_full],y_full,validation_data=[[x_test,X_test],y_test],\n",
    "                        epochs=300,batch_size=32,class_weight=cw,\n",
    "                        callbacks=[early_stopping],\n",
    "                        shuffle=True)\n",
    "    \n",
    "    if binary:\n",
    "        y_pred_t=model.predict([x_test,X_test])\n",
    "        test = metrics.accuracy_score(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1))\n",
    "        #test = metrics.accuracy_score(y_test,y_pred_t>0.5)\n",
    "        \n",
    "        #y_pred_v=model.predict(x_valid)\n",
    "        #valid = metrics.accuracy_score(y_valid,np.argmax(y_pred_v,axis=1))\n",
    "        y_pred_ta=model.predict([x_train,X_full])\n",
    "        train = metrics.accuracy_score(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1))\n",
    "        #train = metrics.accuracy_score(y_full,y_pred_ta>0.5)\n",
    "        \n",
    "        print('train: \\n',metrics.confusion_matrix(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1)))\n",
    "        #print('valid: \\n',metrics.confusion_matrix(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1)))\n",
    "        print('test: \\n',metrics.confusion_matrix(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1)))\n",
    "        \n",
    "        #print('train: \\n',metrics.confusion_matrix(y_full,y_pred_ta>0.5))\n",
    "        #print('test: \\n',metrics.confusion_matrix(y_test,y_pred_t>0.5))\n",
    "\n",
    "    else:\n",
    "        y_pred_t=model.predict([x_test,X_test])\n",
    "        test = metrics.accuracy_score(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1))\n",
    "        #y_pred_v=model.predict(x_valid)\n",
    "        #valid = metrics.accuracy_score(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1))\n",
    "        y_pred_ta=model.predict([x_train,X_full])\n",
    "        train = metrics.accuracy_score(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1))        \n",
    "        print('train: \\n',metrics.confusion_matrix(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1)))\n",
    "        #print('valid: \\n',metrics.confusion_matrix(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1)))\n",
    "        print('test: \\n',metrics.confusion_matrix(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1)))\n",
    "    print('test:%f'%test)\n",
    "    #print('valid:%f'%valid)\n",
    "    print('train:%f'%train)\n",
    "    if file != None:\n",
    "        model.save_model(file)\n",
    "    return train,test,sc,pca\n",
    "\n",
    "def test_model(model,feature,y,sc,pca,binary=True):\n",
    "    if binary:\n",
    "        ind = ((y==0)|(y==1)|(y==2)|(y==3)|(y==4)|(y==6))\n",
    "        #ind1 = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "        y_01 = y[ind].copy()\n",
    "        ind1 = ((y_01==1)|(y_01==2)|(y_01==6))\n",
    "        y_01[ind1] = 1\n",
    "        oc = OneHotEncoder()\n",
    "        y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "        cw = None#{0:1,1:5}\n",
    "    else:\n",
    "        ind = ((y==1)|(y==2)|(y==6))\n",
    "        #ind = ((y==1)|(y==6))\n",
    "        y_01 = y[ind].copy()\n",
    "        oc = OneHotEncoder()\n",
    "        y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "        cw = None#{0:5,1:1,2:1}#{0:2,1:1,2:10,3:2}\n",
    "\n",
    "    #print(y_01)\n",
    "    feature=sc.transform(feature[ind])\n",
    "    #feature = feature[ind]\n",
    "    #feature=pca.transform(feature)\n",
    "    y_pred=model.predict([feature,X2])\n",
    "    test = metrics.accuracy_score(np.argmax(y_01,axis=1),np.argmax(y_pred,axis=1))\n",
    "    #test = metrics.accuracy_score(y_01,y_pred>0.5)\n",
    "    \n",
    "    print('acc:%f'%test)\n",
    "    print(metrics.confusion_matrix(np.argmax(y_01,axis=1),np.argmax(y_pred,axis=1)))\n",
    "    #print(metrics.confusion_matrix(y_01,y_pred>0.5))\n",
    "    return test\n",
    "\n",
    "def sparse_cost_sensitive_loss (y_true,y_pred):\n",
    "    #cost_matrix = tf.constant([[0,1.,1,1.],\n",
    "    #              [2,0,5,5],\n",
    "    #              [1,1,0,1],\n",
    "    #              [1.,2.,1,0]])\n",
    "    cost_matrix = tf.constant([[0,2.,2],\n",
    "                  [1,0,1],\n",
    "                  [1.0,1.,0]])\n",
    "    #cost_matrix = tf.constant([[0,1.],\n",
    "    #              [5.,0]])\n",
    "    batch_cost_matrix = tf.nn.embedding_lookup(cost_matrix, tf.argmax(y_true,axis=1))\n",
    "    eps = 1e-6\n",
    "    probability = tf.clip_by_value(y_pred, eps, 1-eps)\n",
    "    cost_values = tf.math.log(1-probability)*batch_cost_matrix\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(cost_values, axis=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ann = layers.Input(shape=feature.shape[1:])#feature.shape[1:]\n",
    "l1 = layers.Dense(128,activation='elu',\n",
    "                  #kernel_initializer='lecun_normal',\n",
    "                  #kernel_regularizer = regularizers.l2(0.001),\n",
    "                 )(input_ann)\n",
    "l1 = layers.Dropout(0.2)(l1)\n",
    "#bn1 = layers.BatchNormalization()(l1)\n",
    "\n",
    "l2 = layers.Dense(64,activation='elu',\n",
    "                  #kernel_initializer='lecun_normal',\n",
    "                  #kernel_regularizer = regularizers.l2(0.001),\n",
    "                 )(l1)\n",
    "l2 = layers.Dropout(0.2)(l2)\n",
    "#bn2 = layers.BatchNormalization()(l2)\n",
    "\n",
    "l3 = layers.Dense(32,activation='elu',\n",
    "                  #kernel_initializer='lecun_normal',\n",
    "                  #kernel_regularizer = regularizers.l2(0.001),\n",
    "                 )(l2)\n",
    "l3 = layers.Dropout(0.2)(l3)\n",
    "\n",
    "#bn3 = layers.BatchNormalization()(l3)\n",
    "l4 = layers.Dense(16,activation='elu',\n",
    "                 #kernel_regularizer = regularizers.l2(0.001),\n",
    "                 )(l3)\n",
    "#l4 = layers.Dropout(0.2)(l4)\n",
    "\n",
    "#l5 = layers.Dense(8,activation='relu')(drop4)\n",
    "#drop5 = layers.Dropout(0.5)(l5)\n",
    "\n",
    "\n",
    "input_cnn = layers.Input(shape=X_full.shape[1:])\n",
    "#bn = layers.BatchNormalization()(input_)\n",
    "max_pool = layers.MaxPooling2D((2,2))(input_cnn)\n",
    "#bn = layers.BatchNormalization()(input_)\n",
    "cnn1 = layers.Conv2D(16,(3,6),strides=(1,1),\n",
    "                      kernel_initializer=TruncatedNormal(),\n",
    "                      #use_bias=False,#activation='elu',\n",
    "                      padding='same')(max_pool)\n",
    "cnn1 = layers.Activation('elu')(cnn1)\n",
    "cnn1 = layers.BatchNormalization()(cnn1)\n",
    "cnn1 = layers.MaxPooling2D((2,2))(cnn1)\n",
    "\n",
    "cnn2 = layers.Conv2D(32,(3,6),strides=(1,1),\n",
    "                      kernel_initializer=TruncatedNormal(),\n",
    "                      #use_bias=False,#activation='elu',\n",
    "                      padding='same')(cnn1)\n",
    "cnn2 = layers.Activation('elu')(cnn2)\n",
    "cnn2 = layers.BatchNormalization()(cnn2)\n",
    "cnn2 = layers.MaxPooling2D(2)(cnn2)\n",
    "\n",
    "cnn3 = layers.Conv2D(64,(3,6),strides=(1,1),\n",
    "                      kernel_initializer=TruncatedNormal(),\n",
    "                      #use_bias=False,#activation='elu',\n",
    "                      padding='same')(cnn2)\n",
    "cnn3 = layers.Activation('elu')(cnn3)\n",
    "cnn3 = layers.BatchNormalization()(cnn3)\n",
    "cnn3 = layers.MaxPooling2D(2)(cnn3)\n",
    "\n",
    "cnn4 = layers.Conv2D(128,(3,6),strides=(1,1),\n",
    "                      kernel_initializer=TruncatedNormal(),\n",
    "                      #use_bias=False,#activation='elu',\n",
    "                      padding='same')(cnn3)\n",
    "cnn4 = layers.Activation('elu')(cnn4)\n",
    "cnn4 = layers.BatchNormalization()(cnn4)\n",
    "cnn4 = layers.MaxPooling2D(2)(cnn4)\n",
    "flatten = layers.Flatten()(cnn4)\n",
    "\n",
    "#dropout = layers.Dropout(0.2)(flatten)\n",
    "#dense1 = layers.Dense(256,activation = 'relu')(dropout)\n",
    "#dense1 = layers.Dropout(0.2)(dense1)\n",
    "#dense2 = layers.Dense(128,activation = 'relu')(dense1)\n",
    "#dropout2 = layers.Dropout(0.2)(layer2)\n",
    "\n",
    "concat = layers.Concatenate()([l4,flatten])\n",
    "\n",
    "output = layers.Dense(3,activation='softmax')(concat)\n",
    "model = Model(inputs=[input_ann,input_cnn],outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6812 samples, validate on 1704 samples\n",
      "Epoch 1/300\n",
      "6812/6812 [==============================] - 69s 10ms/sample - loss: 0.7756 - accuracy: 0.6723 - val_loss: 1.5976 - val_accuracy: 0.2799\n",
      "Epoch 2/300\n",
      "6812/6812 [==============================] - 55s 8ms/sample - loss: 0.6643 - accuracy: 0.7124 - val_loss: 0.7905 - val_accuracy: 0.6590\n",
      "Epoch 3/300\n",
      "6812/6812 [==============================] - 61s 9ms/sample - loss: 0.6150 - accuracy: 0.7366 - val_loss: 0.5693 - val_accuracy: 0.7576\n",
      "Epoch 4/300\n",
      "6812/6812 [==============================] - 55s 8ms/sample - loss: 0.5861 - accuracy: 0.7551 - val_loss: 0.5472 - val_accuracy: 0.7570\n",
      "Epoch 5/300\n",
      "6812/6812 [==============================] - 52s 8ms/sample - loss: 0.5560 - accuracy: 0.7672 - val_loss: 0.6229 - val_accuracy: 0.7218\n",
      "Epoch 6/300\n",
      "6812/6812 [==============================] - 55s 8ms/sample - loss: 0.5204 - accuracy: 0.7805 - val_loss: 0.5154 - val_accuracy: 0.7782\n",
      "Epoch 7/300\n",
      "6812/6812 [==============================] - 54s 8ms/sample - loss: 0.4938 - accuracy: 0.7942 - val_loss: 0.5496 - val_accuracy: 0.7506\n",
      "Epoch 8/300\n",
      "6812/6812 [==============================] - 53s 8ms/sample - loss: 0.4403 - accuracy: 0.8147 - val_loss: 0.4687 - val_accuracy: 0.7958\n",
      "Epoch 9/300\n",
      "6812/6812 [==============================] - 61s 9ms/sample - loss: 0.3934 - accuracy: 0.8379 - val_loss: 0.5846 - val_accuracy: 0.7588\n",
      "Epoch 10/300\n",
      "6812/6812 [==============================] - 54s 8ms/sample - loss: 0.3434 - accuracy: 0.8595 - val_loss: 0.5352 - val_accuracy: 0.7864\n",
      "Epoch 11/300\n",
      "6812/6812 [==============================] - 54s 8ms/sample - loss: 0.2846 - accuracy: 0.8874 - val_loss: 0.4645 - val_accuracy: 0.8058\n",
      "Epoch 12/300\n",
      "6812/6812 [==============================] - 52s 8ms/sample - loss: 0.2313 - accuracy: 0.9174 - val_loss: 0.5278 - val_accuracy: 0.8022\n",
      "Epoch 13/300\n",
      "6812/6812 [==============================] - 55s 8ms/sample - loss: 0.1863 - accuracy: 0.9329 - val_loss: 0.3571 - val_accuracy: 0.8815\n",
      "Epoch 14/300\n",
      "6812/6812 [==============================] - 56s 8ms/sample - loss: 0.1435 - accuracy: 0.9479 - val_loss: 0.4077 - val_accuracy: 0.8703\n",
      "Epoch 15/300\n",
      "6812/6812 [==============================] - 57s 8ms/sample - loss: 0.1113 - accuracy: 0.9620 - val_loss: 0.3355 - val_accuracy: 0.8961\n",
      "Epoch 16/300\n",
      "6812/6812 [==============================] - 54s 8ms/sample - loss: 0.1080 - accuracy: 0.9639 - val_loss: 0.3435 - val_accuracy: 0.9014\n",
      "Epoch 17/300\n",
      "6812/6812 [==============================] - 55s 8ms/sample - loss: 0.0624 - accuracy: 0.9774 - val_loss: 0.3484 - val_accuracy: 0.8961\n",
      "Epoch 18/300\n",
      "6812/6812 [==============================] - 54s 8ms/sample - loss: 0.0816 - accuracy: 0.9703 - val_loss: 0.3631 - val_accuracy: 0.9002\n",
      "Epoch 19/300\n",
      "6812/6812 [==============================] - 55s 8ms/sample - loss: 0.0591 - accuracy: 0.9793 - val_loss: 0.2916 - val_accuracy: 0.9108\n",
      "Epoch 20/300\n",
      "6812/6812 [==============================] - 60s 9ms/sample - loss: 0.0534 - accuracy: 0.9808 - val_loss: 0.2390 - val_accuracy: 0.9290\n",
      "Epoch 21/300\n",
      "6812/6812 [==============================] - 57s 8ms/sample - loss: 0.0438 - accuracy: 0.9843 - val_loss: 0.2826 - val_accuracy: 0.9219\n",
      "Epoch 22/300\n",
      "6812/6812 [==============================] - 54s 8ms/sample - loss: 0.0330 - accuracy: 0.9884 - val_loss: 0.4041 - val_accuracy: 0.9096\n",
      "Epoch 23/300\n",
      "6812/6812 [==============================] - 54s 8ms/sample - loss: 0.0476 - accuracy: 0.9844 - val_loss: 0.3654 - val_accuracy: 0.9067\n",
      "Epoch 24/300\n",
      "6812/6812 [==============================] - 59s 9ms/sample - loss: 0.0425 - accuracy: 0.9849 - val_loss: 0.3332 - val_accuracy: 0.9184\n",
      "Epoch 25/300\n",
      "6812/6812 [==============================] - 54s 8ms/sample - loss: 0.0402 - accuracy: 0.9872 - val_loss: 0.3648 - val_accuracy: 0.9114\n",
      "Epoch 26/300\n",
      "6812/6812 [==============================] - 58s 9ms/sample - loss: 0.0368 - accuracy: 0.9874 - val_loss: 0.5002 - val_accuracy: 0.8879\n",
      "Epoch 27/300\n",
      "6812/6812 [==============================] - 43s 6ms/sample - loss: 0.0267 - accuracy: 0.9899 - val_loss: 0.2837 - val_accuracy: 0.9308\n",
      "Epoch 28/300\n",
      "6812/6812 [==============================] - 36s 5ms/sample - loss: 0.0267 - accuracy: 0.9912 - val_loss: 0.4248 - val_accuracy: 0.8950\n",
      "Epoch 29/300\n",
      "6812/6812 [==============================] - 40s 6ms/sample - loss: 0.0463 - accuracy: 0.9847 - val_loss: 0.3271 - val_accuracy: 0.9120\n",
      "Epoch 30/300\n",
      "6812/6812 [==============================] - 39s 6ms/sample - loss: 0.0452 - accuracy: 0.9852 - val_loss: 0.2346 - val_accuracy: 0.9372\n",
      "Epoch 31/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0281 - accuracy: 0.9906 - val_loss: 0.2942 - val_accuracy: 0.9208\n",
      "Epoch 32/300\n",
      "6812/6812 [==============================] - 33s 5ms/sample - loss: 0.0380 - accuracy: 0.9863 - val_loss: 0.2671 - val_accuracy: 0.9184\n",
      "Epoch 33/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0224 - accuracy: 0.9935 - val_loss: 0.3327 - val_accuracy: 0.9102\n",
      "Epoch 34/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0165 - accuracy: 0.9941 - val_loss: 0.2985 - val_accuracy: 0.9378\n",
      "Epoch 35/300\n",
      "6812/6812 [==============================] - 39s 6ms/sample - loss: 0.0361 - accuracy: 0.9890 - val_loss: 0.2784 - val_accuracy: 0.9208\n",
      "Epoch 36/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0238 - accuracy: 0.9930 - val_loss: 0.2357 - val_accuracy: 0.9425\n",
      "Epoch 37/300\n",
      "6812/6812 [==============================] - 33s 5ms/sample - loss: 0.0115 - accuracy: 0.9956 - val_loss: 0.3880 - val_accuracy: 0.9231\n",
      "Epoch 38/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0354 - accuracy: 0.9891 - val_loss: 0.2334 - val_accuracy: 0.9360\n",
      "Epoch 39/300\n",
      "6812/6812 [==============================] - 33s 5ms/sample - loss: 0.0257 - accuracy: 0.9910 - val_loss: 0.2762 - val_accuracy: 0.9278\n",
      "Epoch 40/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0284 - accuracy: 0.9900 - val_loss: 0.3030 - val_accuracy: 0.9296\n",
      "Epoch 41/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0348 - accuracy: 0.9875 - val_loss: 0.2366 - val_accuracy: 0.9290\n",
      "Epoch 42/300\n",
      "6812/6812 [==============================] - 33s 5ms/sample - loss: 0.0284 - accuracy: 0.9908 - val_loss: 0.2561 - val_accuracy: 0.9360\n",
      "Epoch 43/300\n",
      "6812/6812 [==============================] - 33s 5ms/sample - loss: 0.0167 - accuracy: 0.9940 - val_loss: 0.4499 - val_accuracy: 0.9049\n",
      "Epoch 44/300\n",
      "6812/6812 [==============================] - 38s 6ms/sample - loss: 0.0151 - accuracy: 0.9956 - val_loss: 0.2737 - val_accuracy: 0.9319\n",
      "Epoch 45/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0204 - accuracy: 0.9931 - val_loss: 0.3423 - val_accuracy: 0.9231\n",
      "Epoch 46/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0231 - accuracy: 0.9935 - val_loss: 0.2889 - val_accuracy: 0.9319\n",
      "Epoch 47/300\n",
      "6812/6812 [==============================] - 33s 5ms/sample - loss: 0.0170 - accuracy: 0.9953 - val_loss: 0.2994 - val_accuracy: 0.9178\n",
      "Epoch 48/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0207 - accuracy: 0.9941 - val_loss: 0.2533 - val_accuracy: 0.9413\n",
      "Epoch 49/300\n",
      "6812/6812 [==============================] - 33s 5ms/sample - loss: 0.0193 - accuracy: 0.9943 - val_loss: 0.2183 - val_accuracy: 0.9507\n",
      "Epoch 50/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0269 - accuracy: 0.9909 - val_loss: 0.2703 - val_accuracy: 0.9343\n",
      "Epoch 51/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.2343 - val_accuracy: 0.9495\n",
      "Epoch 52/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0330 - accuracy: 0.9897 - val_loss: 0.2209 - val_accuracy: 0.9372\n",
      "Epoch 53/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0233 - accuracy: 0.9931 - val_loss: 0.2831 - val_accuracy: 0.9202\n",
      "Epoch 54/300\n",
      "6812/6812 [==============================] - 38s 6ms/sample - loss: 0.0184 - accuracy: 0.9944 - val_loss: 0.2295 - val_accuracy: 0.9437\n",
      "Epoch 55/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0088 - accuracy: 0.9977 - val_loss: 0.2607 - val_accuracy: 0.9431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "6812/6812 [==============================] - 33s 5ms/sample - loss: 0.0216 - accuracy: 0.9935 - val_loss: 0.2341 - val_accuracy: 0.9331\n",
      "Epoch 57/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0136 - accuracy: 0.9957 - val_loss: 0.2607 - val_accuracy: 0.9442\n",
      "Epoch 58/300\n",
      "6812/6812 [==============================] - 33s 5ms/sample - loss: 0.0128 - accuracy: 0.9962 - val_loss: 0.3618 - val_accuracy: 0.9290\n",
      "Epoch 59/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0215 - accuracy: 0.9919 - val_loss: 0.3518 - val_accuracy: 0.9208\n",
      "Epoch 60/300\n",
      "6812/6812 [==============================] - 33s 5ms/sample - loss: 0.0203 - accuracy: 0.9932 - val_loss: 0.4929 - val_accuracy: 0.9008\n",
      "Epoch 61/300\n",
      "6812/6812 [==============================] - 33s 5ms/sample - loss: 0.0201 - accuracy: 0.9938 - val_loss: 0.3015 - val_accuracy: 0.9202\n",
      "Epoch 62/300\n",
      "6812/6812 [==============================] - 33s 5ms/sample - loss: 0.0215 - accuracy: 0.9938 - val_loss: 0.3735 - val_accuracy: 0.9131\n",
      "Epoch 63/300\n",
      "6812/6812 [==============================] - 42s 6ms/sample - loss: 0.0158 - accuracy: 0.9949 - val_loss: 0.2604 - val_accuracy: 0.9343\n",
      "Epoch 64/300\n",
      "6812/6812 [==============================] - 41s 6ms/sample - loss: 0.0112 - accuracy: 0.9963 - val_loss: 0.2247 - val_accuracy: 0.9525\n",
      "Epoch 65/300\n",
      "6812/6812 [==============================] - 37s 5ms/sample - loss: 0.0142 - accuracy: 0.9952 - val_loss: 0.3484 - val_accuracy: 0.9184\n",
      "Epoch 66/300\n",
      "6812/6812 [==============================] - 36s 5ms/sample - loss: 0.0063 - accuracy: 0.9981 - val_loss: 0.2792 - val_accuracy: 0.9431\n",
      "Epoch 67/300\n",
      "6812/6812 [==============================] - 38s 6ms/sample - loss: 0.0187 - accuracy: 0.9957 - val_loss: 0.5868 - val_accuracy: 0.8721\n",
      "Epoch 68/300\n",
      "6812/6812 [==============================] - 42s 6ms/sample - loss: 0.0274 - accuracy: 0.9910 - val_loss: 0.2391 - val_accuracy: 0.9319\n",
      "Epoch 69/300\n",
      "6812/6812 [==============================] - 38s 6ms/sample - loss: 0.0156 - accuracy: 0.9957 - val_loss: 0.2029 - val_accuracy: 0.9472\n",
      "Epoch 70/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0066 - accuracy: 0.9979 - val_loss: 0.2637 - val_accuracy: 0.9384\n",
      "Epoch 71/300\n",
      "6812/6812 [==============================] - 36s 5ms/sample - loss: 0.0081 - accuracy: 0.9974 - val_loss: 0.5224 - val_accuracy: 0.9137\n",
      "Epoch 72/300\n",
      "6812/6812 [==============================] - 40s 6ms/sample - loss: 0.0121 - accuracy: 0.9959 - val_loss: 0.2488 - val_accuracy: 0.9384\n",
      "Epoch 73/300\n",
      "6812/6812 [==============================] - 39s 6ms/sample - loss: 0.0106 - accuracy: 0.9974 - val_loss: 0.2435 - val_accuracy: 0.9442\n",
      "Epoch 74/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0048 - accuracy: 0.9988 - val_loss: 0.4509 - val_accuracy: 0.9149\n",
      "Epoch 75/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0159 - accuracy: 0.9944 - val_loss: 0.3490 - val_accuracy: 0.9208\n",
      "Epoch 76/300\n",
      "6812/6812 [==============================] - 37s 5ms/sample - loss: 0.0234 - accuracy: 0.9921 - val_loss: 0.2051 - val_accuracy: 0.9472\n",
      "Epoch 77/300\n",
      "6812/6812 [==============================] - 41s 6ms/sample - loss: 0.0162 - accuracy: 0.9949 - val_loss: 0.2362 - val_accuracy: 0.9466\n",
      "Epoch 78/300\n",
      "6812/6812 [==============================] - 38s 6ms/sample - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.2148 - val_accuracy: 0.9577\n",
      "Epoch 79/300\n",
      "6812/6812 [==============================] - 37s 5ms/sample - loss: 0.0050 - accuracy: 0.9982 - val_loss: 0.5135 - val_accuracy: 0.9102\n",
      "Epoch 80/300\n",
      "6812/6812 [==============================] - 40s 6ms/sample - loss: 0.0165 - accuracy: 0.9959 - val_loss: 0.2570 - val_accuracy: 0.9413\n",
      "Epoch 81/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0189 - accuracy: 0.9950 - val_loss: 0.3936 - val_accuracy: 0.9266\n",
      "Epoch 82/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0125 - accuracy: 0.9960 - val_loss: 0.3270 - val_accuracy: 0.9390\n",
      "Epoch 83/300\n",
      "6812/6812 [==============================] - 36s 5ms/sample - loss: 0.0184 - accuracy: 0.9946 - val_loss: 0.2409 - val_accuracy: 0.9390\n",
      "Epoch 84/300\n",
      "6812/6812 [==============================] - 37s 5ms/sample - loss: 0.0122 - accuracy: 0.9965 - val_loss: 0.2773 - val_accuracy: 0.9407\n",
      "Epoch 85/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0210 - accuracy: 0.9947 - val_loss: 0.1571 - val_accuracy: 0.9525\n",
      "Epoch 86/300\n",
      "6812/6812 [==============================] - 36s 5ms/sample - loss: 0.0100 - accuracy: 0.9974 - val_loss: 0.2165 - val_accuracy: 0.9442\n",
      "Epoch 87/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0082 - accuracy: 0.9978 - val_loss: 0.2446 - val_accuracy: 0.9413\n",
      "Epoch 88/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0137 - accuracy: 0.9954 - val_loss: 0.2029 - val_accuracy: 0.9513\n",
      "Epoch 89/300\n",
      "6812/6812 [==============================] - 40s 6ms/sample - loss: 0.0058 - accuracy: 0.9984 - val_loss: 0.2287 - val_accuracy: 0.9495\n",
      "Epoch 90/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0101 - accuracy: 0.9968 - val_loss: 0.3017 - val_accuracy: 0.9354\n",
      "Epoch 91/300\n",
      "6812/6812 [==============================] - 36s 5ms/sample - loss: 0.0243 - accuracy: 0.9916 - val_loss: 0.2282 - val_accuracy: 0.9442\n",
      "Epoch 92/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0072 - accuracy: 0.9974 - val_loss: 0.2169 - val_accuracy: 0.9501\n",
      "Epoch 93/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0056 - accuracy: 0.9984 - val_loss: 0.2360 - val_accuracy: 0.9495\n",
      "Epoch 94/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.1957 - val_accuracy: 0.9589\n",
      "Epoch 95/300\n",
      "6812/6812 [==============================] - 36s 5ms/sample - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.2380 - val_accuracy: 0.9548\n",
      "Epoch 96/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0161 - accuracy: 0.9956 - val_loss: 0.3049 - val_accuracy: 0.9354\n",
      "Epoch 97/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0084 - accuracy: 0.9977 - val_loss: 0.4103 - val_accuracy: 0.9272\n",
      "Epoch 98/300\n",
      "6812/6812 [==============================] - 39s 6ms/sample - loss: 0.0180 - accuracy: 0.9953 - val_loss: 0.2421 - val_accuracy: 0.9425\n",
      "Epoch 99/300\n",
      "6812/6812 [==============================] - 38s 6ms/sample - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.2931 - val_accuracy: 0.9437\n",
      "Epoch 100/300\n",
      "6812/6812 [==============================] - 36s 5ms/sample - loss: 0.0131 - accuracy: 0.9956 - val_loss: 0.2843 - val_accuracy: 0.9384\n",
      "Epoch 101/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0108 - accuracy: 0.9969 - val_loss: 0.2204 - val_accuracy: 0.9472\n",
      "Epoch 102/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0198 - accuracy: 0.9941 - val_loss: 0.2475 - val_accuracy: 0.9413\n",
      "Epoch 103/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.2648 - val_accuracy: 0.9390\n",
      "Epoch 104/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0082 - accuracy: 0.9966 - val_loss: 0.2754 - val_accuracy: 0.9531\n",
      "Epoch 105/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0059 - accuracy: 0.9981 - val_loss: 0.2637 - val_accuracy: 0.9448\n",
      "train: \n",
      " [[1503    0    0]\n",
      " [   1 3424    0]\n",
      " [   0    0 1884]]\n",
      "test: \n",
      " [[366  12  13]\n",
      " [ 20 809  17]\n",
      " [ 16   3 448]]\n",
      "test:0.952465\n",
      "train:0.999853\n"
     ]
    }
   ],
   "source": [
    "train,test,sc,pca = train_model(model,feature,np.array(y),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.570229\n",
      "[[278  48 150]\n",
      " [221 568 288]\n",
      " [214   0 376]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5702286514232384"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model,np.array(feature2),np.array(y2),sc,pca,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "model_ann = models.load_model('./model/ann.h5')\n",
    "model_cnn = models.load_model('./model/cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_30\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_76 (InputLayer)           [(None, 32, 256, 8)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_27 (InputLayer)           multiple             0           input_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              multiple             1552        input_27[7][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      multiple             0           conv2d_56[6][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo multiple             64          activation_56[7][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_71 (MaxPooling2D) multiple             0           batch_normalization_28[6][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              multiple             6176        max_pooling2d_71[6][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      multiple             0           conv2d_57[6][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo multiple             128         activation_57[6][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_72 (MaxPooling2D) multiple             0           batch_normalization_29[6][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              multiple             24640       max_pooling2d_72[6][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_75 (InputLayer)           [(None, 232)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      multiple             0           conv2d_58[6][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 128)          29824       input_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo multiple             256         activation_58[6][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 128)          0           dense_25[6][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_73 (MaxPooling2D) multiple             0           batch_normalization_30[6][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 64)           8256        dropout_16[6][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              multiple             98432       max_pooling2d_73[6][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 64)           0           dense_26[6][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      multiple             0           conv2d_59[6][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 32)           2080        dropout_17[6][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo multiple             512         activation_59[6][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 32)           0           dense_27[6][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_74 (MaxPooling2D) multiple             0           batch_normalization_31[6][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 16)           528         dropout_18[6][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            multiple             0           max_pooling2d_74[6][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 4112)         0           dense_28[6][0]                   \n",
      "                                                                 flatten_13[6][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_84 (Dense)                (None, 256)          1052928     concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 256)          0           dense_84[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_85 (Dense)                (None, 126)          32382       dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 126)          0           dense_85[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_86 (Dense)                (None, 3)            381         dropout_56[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,258,139\n",
      "Trainable params: 1,085,691\n",
      "Non-trainable params: 172,448\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_ann = layers.Input(shape=feature.shape[1:])\n",
    "ann = model_ann.layers[0](input_ann)\n",
    "ann.trainable = False\n",
    "for layer in model_ann.layers[1:-2]:\n",
    "    ann = layer(ann)\n",
    "    ann.trainable = False\n",
    "#model_ann = Model(inputs=[input_],outputs=[ann])\n",
    "\n",
    "input_cnn = layers.Input(shape=X_full.shape[1:])\n",
    "cnn = model_cnn.layers[0](input_cnn)\n",
    "cnn .trainable = False\n",
    "for layer in model_cnn.layers[2:-1]:\n",
    "    cnn = layer(cnn)\n",
    "    cnn.trainable = False\n",
    "#model_cnn = Model(inputs=[input_cnn],outputs=[cnn])\n",
    "concat = layers.Concatenate()([ann,cnn])\n",
    "dense1 = layers.Dense(1024,activation='relu')(concat)\n",
    "dense1 = layers.Dropout(0.2)(dense1)\n",
    "dense2 = layers.Dense(512,activation='relu')(dense1)\n",
    "dense2 = layers.Dropout(0.2)(dense2)\n",
    "dense3 = layers.Dense(256,activation='relu')(dense2)\n",
    "dense3 = layers.Dropout(0.2)(dense3)\n",
    "output = layers.Dense(3,activation='softmax')(dense3)\n",
    "\n",
    "model = Model(inputs=[input_ann,input_cnn],outputs=[output])\n",
    "for layer in model.layers[:-6]:\n",
    "    layer.trainable=False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x1db859cc548>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[2].trainable=False\n",
    "model.layers[-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6812 samples, validate on 1704 samples\n",
      "Epoch 1/300\n",
      "6812/6812 [==============================] - 61s 9ms/sample - loss: 0.1799 - accuracy: 0.9306 - val_loss: 0.2565 - val_accuracy: 0.9096\n",
      "Epoch 2/300\n",
      "6812/6812 [==============================] - 67s 10ms/sample - loss: 0.0415 - accuracy: 0.9866 - val_loss: 0.2906 - val_accuracy: 0.9243\n",
      "Epoch 3/300\n",
      "6812/6812 [==============================] - 60s 9ms/sample - loss: 0.0313 - accuracy: 0.9894 - val_loss: 0.3368 - val_accuracy: 0.9173\n",
      "Epoch 4/300\n",
      "6812/6812 [==============================] - 59s 9ms/sample - loss: 0.0298 - accuracy: 0.9919 - val_loss: 0.3970 - val_accuracy: 0.9049\n",
      "Epoch 5/300\n",
      "6812/6812 [==============================] - 59s 9ms/sample - loss: 0.0207 - accuracy: 0.9930 - val_loss: 0.3764 - val_accuracy: 0.9137\n",
      "Epoch 6/300\n",
      "6812/6812 [==============================] - 59s 9ms/sample - loss: 0.0155 - accuracy: 0.9952 - val_loss: 0.4694 - val_accuracy: 0.9114\n",
      "Epoch 7/300\n",
      "6812/6812 [==============================] - 65s 10ms/sample - loss: 0.0230 - accuracy: 0.9943 - val_loss: 0.3874 - val_accuracy: 0.9190\n",
      "Epoch 8/300\n",
      "6812/6812 [==============================] - 60s 9ms/sample - loss: 0.0129 - accuracy: 0.9960 - val_loss: 0.4384 - val_accuracy: 0.9208\n",
      "Epoch 9/300\n",
      "6812/6812 [==============================] - 60s 9ms/sample - loss: 0.0131 - accuracy: 0.9957 - val_loss: 0.5122 - val_accuracy: 0.9231\n",
      "Epoch 10/300\n",
      "6812/6812 [==============================] - 59s 9ms/sample - loss: 0.0210 - accuracy: 0.9943 - val_loss: 0.4372 - val_accuracy: 0.9137\n",
      "Epoch 11/300\n",
      "6812/6812 [==============================] - 59s 9ms/sample - loss: 0.0197 - accuracy: 0.9944 - val_loss: 0.3815 - val_accuracy: 0.9196\n",
      "Epoch 12/300\n",
      "6812/6812 [==============================] - 59s 9ms/sample - loss: 0.0126 - accuracy: 0.9969 - val_loss: 0.5183 - val_accuracy: 0.9178\n",
      "Epoch 13/300\n",
      "6812/6812 [==============================] - 64s 9ms/sample - loss: 0.0133 - accuracy: 0.9960 - val_loss: 0.4057 - val_accuracy: 0.9178\n",
      "Epoch 14/300\n",
      "6812/6812 [==============================] - 58s 9ms/sample - loss: 0.0129 - accuracy: 0.9960 - val_loss: 0.5107 - val_accuracy: 0.9167\n",
      "Epoch 15/300\n",
      "6812/6812 [==============================] - 60s 9ms/sample - loss: 0.0040 - accuracy: 0.9982 - val_loss: 0.4642 - val_accuracy: 0.9225\n",
      "Epoch 16/300\n",
      "6812/6812 [==============================] - 60s 9ms/sample - loss: 0.0078 - accuracy: 0.9969 - val_loss: 0.5434 - val_accuracy: 0.9137\n",
      "Epoch 17/300\n",
      "6812/6812 [==============================] - 61s 9ms/sample - loss: 0.0132 - accuracy: 0.9965 - val_loss: 0.4877 - val_accuracy: 0.9202\n",
      "Epoch 18/300\n",
      "6812/6812 [==============================] - 82s 12ms/sample - loss: 0.0144 - accuracy: 0.9969 - val_loss: 0.5779 - val_accuracy: 0.9126\n",
      "Epoch 19/300\n",
      "6812/6812 [==============================] - 67s 10ms/sample - loss: 0.0134 - accuracy: 0.9962 - val_loss: 0.5421 - val_accuracy: 0.9131\n",
      "Epoch 20/300\n",
      "6812/6812 [==============================] - 61s 9ms/sample - loss: 0.0170 - accuracy: 0.9937 - val_loss: 0.5206 - val_accuracy: 0.9085\n",
      "Epoch 21/300\n",
      "6812/6812 [==============================] - 67s 10ms/sample - loss: 0.0075 - accuracy: 0.9975 - val_loss: 0.5669 - val_accuracy: 0.9114\n",
      "train: \n",
      " [[1499    4    0]\n",
      " [  18 3406    1]\n",
      " [   6    4 1874]]\n",
      "test: \n",
      " [[338  39  14]\n",
      " [ 37 800   9]\n",
      " [ 26  29 412]]\n",
      "test:0.909624\n",
      "train:0.995156\n"
     ]
    }
   ],
   "source": [
    "train,test,sc,pca = train_model(model,feature,np.array(y),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.737284\n",
      "[[150 216 110]\n",
      " [ 66 964  47]\n",
      " [ 44  80 466]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7372841810545964"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model,np.array(feature2),np.array(y2),sc,pca,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = layers.Input(shape=X_full.shape[1:])\n",
    "#bn = layers.BatchNormalization()(input_)\n",
    "max_pool = layers.MaxPooling2D((2,2))(input_)\n",
    "#bn = layers.BatchNormalization()(input_)\n",
    "\n",
    "cnn1 = layers.Conv2D(16,(2,6),strides=(1,1),\n",
    "                      kernel_initializer=TruncatedNormal(),\n",
    "                      #use_bias=False,#activation='elu',\n",
    "                      padding='same')(max_pool)\n",
    "cnn1 = layers.Activation('relu')(cnn1)\n",
    "cnn1 = layers.BatchNormalization()(cnn1)\n",
    "cnn1 = layers.MaxPooling2D((2,2))(cnn1)\n",
    "\n",
    "cnn2 = layers.Conv2D(32,(2,6),strides=(1,1),\n",
    "                      kernel_initializer=TruncatedNormal(),\n",
    "                      #use_bias=False,#activation='elu',\n",
    "                      padding='same')(cnn1)\n",
    "cnn2 = layers.Activation('relu')(cnn2)\n",
    "cnn2 = layers.BatchNormalization()(cnn2)\n",
    "cnn2 = layers.MaxPooling2D(2)(cnn2)\n",
    "\n",
    "cnn3 = layers.Conv2D(64,(2,6),strides=(1,1),\n",
    "                      kernel_initializer=TruncatedNormal(),\n",
    "                      #use_bias=False,#activation='elu',\n",
    "                      padding='same')(cnn2)\n",
    "cnn3 = layers.Activation('relu')(cnn3)\n",
    "cnn3 = layers.BatchNormalization()(cnn3)\n",
    "cnn3 = layers.MaxPooling2D(2)(cnn3)\n",
    "\n",
    "cnn4 = layers.Conv2D(128,(2,6),strides=(1,1),\n",
    "                      kernel_initializer=TruncatedNormal(),\n",
    "                      #use_bias=False,#activation='elu',\n",
    "                      padding='same')(cnn3)\n",
    "cnn4 = layers.Activation('relu')(cnn4)\n",
    "cnn4 = layers.BatchNormalization()(cnn4)\n",
    "cnn4 = layers.MaxPooling2D(2)(cnn4)\n",
    "\n",
    "flatten = layers.Flatten()(cnn4)\n",
    "#dropout = layers.Dropout(0.2)(flatten)\n",
    "#layer1 = layers.Dense(256,activation = 'relu')(dropout)\n",
    "#dropout1 = layers.Dropout(0.2)(layer1)\n",
    "#layer2 = layers.Dense(128,activation = 'relu')(dropout1)\n",
    "#dropout2 = layers.Dropout(0.2)(layer2)\n",
    "output = layers.Dense(3,activation = 'softmax')(flatten)\n",
    "model = Model(inputs=[input_],outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = callbacks.EarlyStopping(patience = 10,monitor = 'val_accuracy', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6812 samples, validate on 1704 samples\n",
      "Epoch 1/300\n",
      "6812/6812 [==============================] - 38s 6ms/sample - loss: 0.8749 - accuracy: 0.6735 - val_loss: 1.5026 - val_accuracy: 0.2758\n",
      "Epoch 2/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.6004 - accuracy: 0.7449 - val_loss: 0.8417 - val_accuracy: 0.5804\n",
      "Epoch 3/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.5318 - accuracy: 0.7689 - val_loss: 0.5847 - val_accuracy: 0.7465\n",
      "Epoch 4/300\n",
      "6812/6812 [==============================] - 36s 5ms/sample - loss: 0.4653 - accuracy: 0.8077 - val_loss: 0.5940 - val_accuracy: 0.7600\n",
      "Epoch 5/300\n",
      "6812/6812 [==============================] - 37s 5ms/sample - loss: 0.3698 - accuracy: 0.8523 - val_loss: 0.5277 - val_accuracy: 0.7964\n",
      "Epoch 6/300\n",
      "6812/6812 [==============================] - 44s 7ms/sample - loss: 0.2748 - accuracy: 0.8920 - val_loss: 0.4538 - val_accuracy: 0.8281\n",
      "Epoch 7/300\n",
      "6812/6812 [==============================] - 37s 5ms/sample - loss: 0.2033 - accuracy: 0.9219 - val_loss: 0.5903 - val_accuracy: 0.8134\n",
      "Epoch 8/300\n",
      "6812/6812 [==============================] - 38s 6ms/sample - loss: 0.1561 - accuracy: 0.9398 - val_loss: 0.8788 - val_accuracy: 0.7782\n",
      "Epoch 9/300\n",
      "6812/6812 [==============================] - 38s 6ms/sample - loss: 0.0972 - accuracy: 0.9658 - val_loss: 0.4324 - val_accuracy: 0.8457\n",
      "Epoch 10/300\n",
      "6812/6812 [==============================] - 38s 6ms/sample - loss: 0.0860 - accuracy: 0.9693 - val_loss: 0.4319 - val_accuracy: 0.8685\n",
      "Epoch 11/300\n",
      "6812/6812 [==============================] - 39s 6ms/sample - loss: 0.0756 - accuracy: 0.9725 - val_loss: 0.3781 - val_accuracy: 0.8768\n",
      "Epoch 12/300\n",
      "6812/6812 [==============================] - 36s 5ms/sample - loss: 0.1072 - accuracy: 0.9604 - val_loss: 0.4670 - val_accuracy: 0.8709\n",
      "Epoch 13/300\n",
      "6812/6812 [==============================] - 37s 5ms/sample - loss: 0.0552 - accuracy: 0.9818 - val_loss: 0.3443 - val_accuracy: 0.9026\n",
      "Epoch 14/300\n",
      "6812/6812 [==============================] - 39s 6ms/sample - loss: 0.0301 - accuracy: 0.9896 - val_loss: 0.3839 - val_accuracy: 0.8914\n",
      "Epoch 15/300\n",
      "6812/6812 [==============================] - 38s 6ms/sample - loss: 0.0355 - accuracy: 0.9874 - val_loss: 0.8488 - val_accuracy: 0.8104\n",
      "Epoch 16/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0777 - accuracy: 0.9742 - val_loss: 0.6138 - val_accuracy: 0.8597\n",
      "Epoch 17/300\n",
      "6812/6812 [==============================] - 36s 5ms/sample - loss: 0.0863 - accuracy: 0.9673 - val_loss: 0.4227 - val_accuracy: 0.8668\n",
      "Epoch 18/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0669 - accuracy: 0.9749 - val_loss: 0.4297 - val_accuracy: 0.8932\n",
      "Epoch 19/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0326 - accuracy: 0.9877 - val_loss: 0.4171 - val_accuracy: 0.8908\n",
      "Epoch 20/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0251 - accuracy: 0.9909 - val_loss: 0.4090 - val_accuracy: 0.8979\n",
      "Epoch 21/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0395 - accuracy: 0.9861 - val_loss: 0.4465 - val_accuracy: 0.8885\n",
      "Epoch 22/300\n",
      "6812/6812 [==============================] - 36s 5ms/sample - loss: 0.0324 - accuracy: 0.9893 - val_loss: 0.3798 - val_accuracy: 0.9061\n",
      "Epoch 23/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0297 - accuracy: 0.9899 - val_loss: 0.4193 - val_accuracy: 0.8862\n",
      "Epoch 24/300\n",
      "6812/6812 [==============================] - 36s 5ms/sample - loss: 0.0524 - accuracy: 0.9833 - val_loss: 0.4740 - val_accuracy: 0.8785\n",
      "Epoch 25/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0667 - accuracy: 0.9767 - val_loss: 0.7062 - val_accuracy: 0.8462\n",
      "Epoch 26/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0380 - accuracy: 0.9863 - val_loss: 0.3156 - val_accuracy: 0.9137\n",
      "Epoch 27/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0095 - accuracy: 0.9968 - val_loss: 0.3052 - val_accuracy: 0.9202\n",
      "Epoch 28/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.1863 - val_accuracy: 0.9478\n",
      "Epoch 29/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.2833 - val_accuracy: 0.9266\n",
      "Epoch 30/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.3584 - val_accuracy: 0.9096\n",
      "Epoch 31/300\n",
      "6812/6812 [==============================] - 32s 5ms/sample - loss: 0.0554 - accuracy: 0.9819 - val_loss: 0.9091 - val_accuracy: 0.8187\n",
      "Epoch 32/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.1243 - accuracy: 0.9590 - val_loss: 0.4162 - val_accuracy: 0.9020\n",
      "Epoch 33/300\n",
      "6812/6812 [==============================] - 32s 5ms/sample - loss: 0.0324 - accuracy: 0.9903 - val_loss: 0.3011 - val_accuracy: 0.9178\n",
      "Epoch 34/300\n",
      "6812/6812 [==============================] - 37s 5ms/sample - loss: 0.0214 - accuracy: 0.9925 - val_loss: 0.2758 - val_accuracy: 0.9208\n",
      "Epoch 35/300\n",
      "6812/6812 [==============================] - 31s 4ms/sample - loss: 0.0107 - accuracy: 0.9957 - val_loss: 0.2507 - val_accuracy: 0.9372\n",
      "Epoch 36/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0110 - accuracy: 0.9962 - val_loss: 0.3776 - val_accuracy: 0.9161\n",
      "Epoch 37/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0117 - accuracy: 0.9957 - val_loss: 0.2855 - val_accuracy: 0.9272\n",
      "Epoch 38/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0217 - accuracy: 0.9921 - val_loss: 0.3518 - val_accuracy: 0.9219\n",
      "Epoch 39/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0444 - accuracy: 0.9836 - val_loss: 0.5230 - val_accuracy: 0.8967\n",
      "Epoch 40/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0535 - accuracy: 0.9818 - val_loss: 0.3621 - val_accuracy: 0.9114\n",
      "Epoch 41/300\n",
      "6812/6812 [==============================] - 32s 5ms/sample - loss: 0.0213 - accuracy: 0.9924 - val_loss: 0.4041 - val_accuracy: 0.9055\n",
      "Epoch 42/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0208 - accuracy: 0.9924 - val_loss: 0.3531 - val_accuracy: 0.9131\n",
      "Epoch 43/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0301 - accuracy: 0.9902 - val_loss: 0.2995 - val_accuracy: 0.9178\n",
      "Epoch 44/300\n",
      "6812/6812 [==============================] - 38s 6ms/sample - loss: 0.0126 - accuracy: 0.9954 - val_loss: 0.2491 - val_accuracy: 0.9384\n",
      "Epoch 45/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0134 - accuracy: 0.9959 - val_loss: 0.3122 - val_accuracy: 0.9208\n",
      "Epoch 46/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0306 - accuracy: 0.9894 - val_loss: 0.3778 - val_accuracy: 0.9131\n",
      "Epoch 47/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0264 - accuracy: 0.9905 - val_loss: 0.2822 - val_accuracy: 0.9331\n",
      "Epoch 48/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0122 - accuracy: 0.9966 - val_loss: 0.2160 - val_accuracy: 0.9554\n",
      "Epoch 49/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0076 - accuracy: 0.9975 - val_loss: 0.2396 - val_accuracy: 0.9489\n",
      "Epoch 50/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0147 - accuracy: 0.9949 - val_loss: 0.3464 - val_accuracy: 0.9149\n",
      "Epoch 51/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0399 - accuracy: 0.9865 - val_loss: 0.4722 - val_accuracy: 0.9096\n",
      "Epoch 52/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0481 - accuracy: 0.9849 - val_loss: 0.3885 - val_accuracy: 0.9055\n",
      "Epoch 53/300\n",
      "6812/6812 [==============================] - 35s 5ms/sample - loss: 0.0290 - accuracy: 0.9902 - val_loss: 0.3867 - val_accuracy: 0.9155\n",
      "Epoch 54/300\n",
      "6812/6812 [==============================] - 42s 6ms/sample - loss: 0.0202 - accuracy: 0.9931 - val_loss: 0.2670 - val_accuracy: 0.9372\n",
      "Epoch 55/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0060 - accuracy: 0.9979 - val_loss: 0.2288 - val_accuracy: 0.9472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "6812/6812 [==============================] - 30s 4ms/sample - loss: 0.0063 - accuracy: 0.9977 - val_loss: 0.2073 - val_accuracy: 0.9513\n",
      "Epoch 57/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0037 - accuracy: 0.9987 - val_loss: 0.1399 - val_accuracy: 0.9630\n",
      "Epoch 58/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.1471 - val_accuracy: 0.9630\n",
      "Epoch 59/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.1613 - val_accuracy: 0.9601\n",
      "Epoch 60/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0133 - accuracy: 0.9949 - val_loss: 0.3807 - val_accuracy: 0.9161\n",
      "Epoch 61/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0459 - accuracy: 0.9859 - val_loss: 0.5706 - val_accuracy: 0.8920\n",
      "Epoch 62/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0577 - accuracy: 0.9830 - val_loss: 0.5757 - val_accuracy: 0.8826\n",
      "Epoch 63/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0212 - accuracy: 0.9935 - val_loss: 0.2805 - val_accuracy: 0.9354\n",
      "Epoch 64/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0149 - accuracy: 0.9953 - val_loss: 0.2046 - val_accuracy: 0.9484\n",
      "Epoch 65/300\n",
      "6812/6812 [==============================] - 32s 5ms/sample - loss: 0.0082 - accuracy: 0.9971 - val_loss: 0.2524 - val_accuracy: 0.9401\n",
      "Epoch 66/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.2198 - val_accuracy: 0.9548\n",
      "Epoch 67/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.2307 - val_accuracy: 0.9507\n",
      "Epoch 68/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.2281 - val_accuracy: 0.9478\n",
      "Epoch 69/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0324 - accuracy: 0.9902 - val_loss: 0.4759 - val_accuracy: 0.9049\n",
      "Epoch 70/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0314 - accuracy: 0.9893 - val_loss: 0.2851 - val_accuracy: 0.9401\n",
      "Epoch 71/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0150 - accuracy: 0.9952 - val_loss: 0.3012 - val_accuracy: 0.9331\n",
      "Epoch 72/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0117 - accuracy: 0.9969 - val_loss: 0.2385 - val_accuracy: 0.9437\n",
      "Epoch 73/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.3262 - val_accuracy: 0.9349\n",
      "Epoch 74/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0182 - accuracy: 0.9944 - val_loss: 0.3217 - val_accuracy: 0.9266\n",
      "Epoch 75/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0099 - accuracy: 0.9977 - val_loss: 0.2761 - val_accuracy: 0.9419\n",
      "Epoch 76/300\n",
      "6812/6812 [==============================] - 30s 4ms/sample - loss: 0.0123 - accuracy: 0.9971 - val_loss: 0.2924 - val_accuracy: 0.9413\n",
      "Epoch 77/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.2185 - val_accuracy: 0.9566\n",
      "Epoch 78/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 5.0167e-04 - accuracy: 1.0000 - val_loss: 0.1963 - val_accuracy: 0.9560\n",
      "Epoch 79/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 2.2570e-04 - accuracy: 1.0000 - val_loss: 0.1742 - val_accuracy: 0.9630\n",
      "Epoch 80/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 1.5567e-04 - accuracy: 1.0000 - val_loss: 0.1899 - val_accuracy: 0.9595\n",
      "Epoch 81/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 6.3231e-04 - accuracy: 0.9999 - val_loss: 0.1809 - val_accuracy: 0.9619\n",
      "Epoch 82/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 1.3942e-04 - accuracy: 1.0000 - val_loss: 0.1668 - val_accuracy: 0.9648\n",
      "Epoch 83/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 6.6799e-05 - accuracy: 1.0000 - val_loss: 0.1719 - val_accuracy: 0.9636\n",
      "Epoch 84/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 1.2088e-04 - accuracy: 1.0000 - val_loss: 0.1892 - val_accuracy: 0.9583\n",
      "Epoch 85/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.1488 - accuracy: 0.9652 - val_loss: 0.9696 - val_accuracy: 0.8269\n",
      "Epoch 86/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0419 - accuracy: 0.9871 - val_loss: 0.3870 - val_accuracy: 0.9225\n",
      "Epoch 87/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0171 - accuracy: 0.9931 - val_loss: 0.2625 - val_accuracy: 0.9419\n",
      "Epoch 88/300\n",
      "6812/6812 [==============================] - 33s 5ms/sample - loss: 0.0028 - accuracy: 0.9988 - val_loss: 0.2476 - val_accuracy: 0.9437\n",
      "Epoch 89/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.2277 - val_accuracy: 0.9466\n",
      "Epoch 90/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.2278 - val_accuracy: 0.9519\n",
      "Epoch 91/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 5.1252e-04 - accuracy: 1.0000 - val_loss: 0.1940 - val_accuracy: 0.9572\n",
      "Epoch 92/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 2.3189e-04 - accuracy: 1.0000 - val_loss: 0.1732 - val_accuracy: 0.9589\n",
      "Epoch 93/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 1.5451e-04 - accuracy: 1.0000 - val_loss: 0.1784 - val_accuracy: 0.9613\n",
      "Epoch 94/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 9.6681e-05 - accuracy: 1.0000 - val_loss: 0.1785 - val_accuracy: 0.9601\n",
      "Epoch 95/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 7.7391e-05 - accuracy: 1.0000 - val_loss: 0.1806 - val_accuracy: 0.9601\n",
      "Epoch 96/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 4.4991e-04 - accuracy: 0.9999 - val_loss: 0.1903 - val_accuracy: 0.9566\n",
      "Epoch 97/300\n",
      "6812/6812 [==============================] - 30s 4ms/sample - loss: 0.0339 - accuracy: 0.9902 - val_loss: 0.7481 - val_accuracy: 0.8773\n",
      "Epoch 98/300\n",
      "6812/6812 [==============================] - 30s 4ms/sample - loss: 0.0905 - accuracy: 0.9731 - val_loss: 0.3849 - val_accuracy: 0.9131\n",
      "Epoch 99/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0196 - accuracy: 0.9938 - val_loss: 0.2786 - val_accuracy: 0.9313\n",
      "Epoch 100/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0055 - accuracy: 0.9981 - val_loss: 0.2543 - val_accuracy: 0.9390\n",
      "Epoch 101/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.1662 - val_accuracy: 0.9507\n",
      "Epoch 102/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 5.3440e-04 - accuracy: 1.0000 - val_loss: 0.1534 - val_accuracy: 0.9613\n",
      "Epoch 103/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 4.6605e-04 - accuracy: 0.9999 - val_loss: 0.1694 - val_accuracy: 0.9525\n",
      "Epoch 104/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 7.7610e-04 - accuracy: 0.9999 - val_loss: 0.1484 - val_accuracy: 0.9589\n",
      "Epoch 105/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 1.6102e-04 - accuracy: 1.0000 - val_loss: 0.1390 - val_accuracy: 0.9619\n",
      "Epoch 106/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 9.9476e-05 - accuracy: 1.0000 - val_loss: 0.1387 - val_accuracy: 0.9613\n",
      "Epoch 107/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 2.4936e-04 - accuracy: 1.0000 - val_loss: 0.1859 - val_accuracy: 0.9542\n",
      "Epoch 108/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.2914 - val_accuracy: 0.9354\n",
      "Epoch 109/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0649 - accuracy: 0.9809 - val_loss: 0.4791 - val_accuracy: 0.9032\n",
      "Epoch 110/300\n",
      "6812/6812 [==============================] - 32s 5ms/sample - loss: 0.0467 - accuracy: 0.9863 - val_loss: 0.3368 - val_accuracy: 0.9284\n",
      "Epoch 111/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0132 - accuracy: 0.9956 - val_loss: 0.2386 - val_accuracy: 0.9454\n",
      "Epoch 112/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0044 - accuracy: 0.9990 - val_loss: 0.2704 - val_accuracy: 0.9431\n",
      "Epoch 113/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.1893 - val_accuracy: 0.9595\n",
      "Epoch 114/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0023 - accuracy: 0.9991 - val_loss: 0.1985 - val_accuracy: 0.9525\n",
      "Epoch 115/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.2039 - val_accuracy: 0.9595\n",
      "Epoch 116/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0038 - accuracy: 0.9987 - val_loss: 0.2018 - val_accuracy: 0.9607\n",
      "Epoch 117/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0076 - accuracy: 0.9968 - val_loss: 0.2781 - val_accuracy: 0.9378\n",
      "Epoch 118/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0044 - accuracy: 0.9981 - val_loss: 0.2759 - val_accuracy: 0.9454\n",
      "Epoch 119/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.2371 - val_accuracy: 0.9566\n",
      "Epoch 120/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0040 - accuracy: 0.9985 - val_loss: 0.2816 - val_accuracy: 0.9360\n",
      "Epoch 121/300\n",
      "6812/6812 [==============================] - 30s 4ms/sample - loss: 0.0131 - accuracy: 0.9950 - val_loss: 0.3945 - val_accuracy: 0.9243\n",
      "Epoch 122/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0249 - accuracy: 0.9921 - val_loss: 0.4349 - val_accuracy: 0.9178\n",
      "Epoch 123/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0085 - accuracy: 0.9971 - val_loss: 0.3010 - val_accuracy: 0.9343\n",
      "Epoch 124/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0172 - accuracy: 0.9944 - val_loss: 0.3758 - val_accuracy: 0.9284\n",
      "Epoch 125/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0092 - accuracy: 0.9969 - val_loss: 0.3842 - val_accuracy: 0.9308\n",
      "Epoch 126/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0120 - accuracy: 0.9962 - val_loss: 0.3337 - val_accuracy: 0.9384\n",
      "Epoch 127/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0101 - accuracy: 0.9965 - val_loss: 0.2717 - val_accuracy: 0.9501\n",
      "Epoch 128/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 9.4449e-04 - accuracy: 0.9999 - val_loss: 0.2183 - val_accuracy: 0.9577\n",
      "Epoch 129/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 6.4086e-04 - accuracy: 0.9999 - val_loss: 0.2335 - val_accuracy: 0.9566\n",
      "Epoch 130/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0037 - accuracy: 0.9990 - val_loss: 0.2603 - val_accuracy: 0.9507\n",
      "Epoch 131/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.3026 - val_accuracy: 0.9407\n",
      "Epoch 132/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0175 - accuracy: 0.9941 - val_loss: 0.3819 - val_accuracy: 0.9272\n",
      "Epoch 133/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0203 - accuracy: 0.9940 - val_loss: 0.4782 - val_accuracy: 0.9225\n",
      "Epoch 134/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0206 - accuracy: 0.9930 - val_loss: 0.3491 - val_accuracy: 0.9396\n",
      "Epoch 135/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0146 - accuracy: 0.9965 - val_loss: 0.2103 - val_accuracy: 0.9560\n",
      "Epoch 136/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0044 - accuracy: 0.9988 - val_loss: 0.2303 - val_accuracy: 0.9531\n",
      "Epoch 137/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0018 - accuracy: 0.9993 - val_loss: 0.2641 - val_accuracy: 0.9484\n",
      "Epoch 138/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0059 - accuracy: 0.9984 - val_loss: 0.1802 - val_accuracy: 0.9595\n",
      "Epoch 139/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.1629 - val_accuracy: 0.9630\n",
      "Epoch 140/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 2.0743e-04 - accuracy: 1.0000 - val_loss: 0.1498 - val_accuracy: 0.9636\n",
      "Epoch 141/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 1.3137e-04 - accuracy: 1.0000 - val_loss: 0.1553 - val_accuracy: 0.9660\n",
      "Epoch 142/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 1.6932e-04 - accuracy: 1.0000 - val_loss: 0.1484 - val_accuracy: 0.9689\n",
      "Epoch 143/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.3207 - val_accuracy: 0.9378\n",
      "Epoch 144/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0284 - accuracy: 0.9913 - val_loss: 0.6516 - val_accuracy: 0.9038\n",
      "Epoch 145/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0380 - accuracy: 0.9891 - val_loss: 0.2926 - val_accuracy: 0.9325\n",
      "Epoch 146/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0152 - accuracy: 0.9944 - val_loss: 0.3137 - val_accuracy: 0.9413\n",
      "Epoch 147/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0066 - accuracy: 0.9977 - val_loss: 0.1965 - val_accuracy: 0.9560\n",
      "Epoch 148/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0040 - accuracy: 0.9985 - val_loss: 0.1921 - val_accuracy: 0.9589\n",
      "Epoch 149/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 8.4865e-04 - accuracy: 0.9996 - val_loss: 0.2017 - val_accuracy: 0.9648\n",
      "Epoch 150/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0045 - accuracy: 0.9981 - val_loss: 0.3232 - val_accuracy: 0.9390\n",
      "Epoch 151/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0023 - accuracy: 0.9991 - val_loss: 0.2604 - val_accuracy: 0.9478\n",
      "Epoch 152/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 9.4622e-04 - accuracy: 0.9999 - val_loss: 0.3241 - val_accuracy: 0.9331\n",
      "Epoch 153/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.3849 - val_accuracy: 0.9384\n",
      "Epoch 154/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0052 - accuracy: 0.9982 - val_loss: 0.1817 - val_accuracy: 0.9630\n",
      "Epoch 155/300\n",
      "6812/6812 [==============================] - 30s 4ms/sample - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.2201 - val_accuracy: 0.9619\n",
      "Epoch 156/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.2744 - val_accuracy: 0.9419\n",
      "Epoch 157/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0145 - accuracy: 0.9962 - val_loss: 0.5601 - val_accuracy: 0.9126\n",
      "Epoch 158/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0157 - accuracy: 0.9949 - val_loss: 0.2764 - val_accuracy: 0.9484\n",
      "Epoch 159/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0170 - accuracy: 0.9954 - val_loss: 0.3045 - val_accuracy: 0.9442\n",
      "Epoch 160/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0079 - accuracy: 0.9972 - val_loss: 0.2328 - val_accuracy: 0.9613\n",
      "Epoch 161/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0051 - accuracy: 0.9979 - val_loss: 0.2662 - val_accuracy: 0.9542\n",
      "Epoch 162/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.2088 - val_accuracy: 0.9624\n",
      "Epoch 163/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0036 - accuracy: 0.9987 - val_loss: 0.1770 - val_accuracy: 0.9648\n",
      "Epoch 164/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.2499 - val_accuracy: 0.9554\n",
      "Epoch 165/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0122 - accuracy: 0.9960 - val_loss: 0.2575 - val_accuracy: 0.9454\n",
      "Epoch 166/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0096 - accuracy: 0.9974 - val_loss: 0.3412 - val_accuracy: 0.9431\n",
      "Epoch 167/300\n",
      "6812/6812 [==============================] - 33s 5ms/sample - loss: 0.0086 - accuracy: 0.9971 - val_loss: 0.2587 - val_accuracy: 0.9466\n",
      "Epoch 168/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0064 - accuracy: 0.9984 - val_loss: 0.2469 - val_accuracy: 0.9607\n",
      "Epoch 169/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0067 - accuracy: 0.9978 - val_loss: 0.3411 - val_accuracy: 0.9442\n",
      "Epoch 170/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.3116 - val_accuracy: 0.9472\n",
      "Epoch 171/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0076 - accuracy: 0.9978 - val_loss: 0.2627 - val_accuracy: 0.9454\n",
      "Epoch 172/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0088 - accuracy: 0.9977 - val_loss: 0.2541 - val_accuracy: 0.9536\n",
      "Epoch 173/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0054 - accuracy: 0.9979 - val_loss: 0.1888 - val_accuracy: 0.9601\n",
      "Epoch 174/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0051 - accuracy: 0.9988 - val_loss: 0.2180 - val_accuracy: 0.9601\n",
      "Epoch 175/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0072 - accuracy: 0.9974 - val_loss: 0.3009 - val_accuracy: 0.9501\n",
      "Epoch 176/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0106 - accuracy: 0.9971 - val_loss: 0.4402 - val_accuracy: 0.9290\n",
      "Epoch 177/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0047 - accuracy: 0.9978 - val_loss: 0.2909 - val_accuracy: 0.9478\n",
      "Epoch 178/300\n",
      "6812/6812 [==============================] - 32s 5ms/sample - loss: 0.0067 - accuracy: 0.9975 - val_loss: 0.4813 - val_accuracy: 0.9249\n",
      "Epoch 179/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0051 - accuracy: 0.9982 - val_loss: 0.3139 - val_accuracy: 0.9401\n",
      "Epoch 180/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0073 - accuracy: 0.9981 - val_loss: 0.2856 - val_accuracy: 0.9495\n",
      "Epoch 181/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0051 - accuracy: 0.9981 - val_loss: 0.2844 - val_accuracy: 0.9442\n",
      "Epoch 182/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0058 - accuracy: 0.9984 - val_loss: 0.2888 - val_accuracy: 0.9519\n",
      "Epoch 183/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0031 - accuracy: 0.9985 - val_loss: 0.2641 - val_accuracy: 0.9577\n",
      "Epoch 184/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0040 - accuracy: 0.9984 - val_loss: 0.2369 - val_accuracy: 0.9601\n",
      "Epoch 185/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0024 - accuracy: 0.9991 - val_loss: 0.2570 - val_accuracy: 0.9519\n",
      "Epoch 186/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 8.1145e-04 - accuracy: 0.9999 - val_loss: 0.2110 - val_accuracy: 0.9583\n",
      "Epoch 187/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 2.7577e-04 - accuracy: 1.0000 - val_loss: 0.2118 - val_accuracy: 0.9601\n",
      "Epoch 188/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 1.3600e-04 - accuracy: 1.0000 - val_loss: 0.2001 - val_accuracy: 0.9624\n",
      "Epoch 189/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 1.8185e-05 - accuracy: 1.0000 - val_loss: 0.1985 - val_accuracy: 0.9630\n",
      "Epoch 190/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.4552 - val_accuracy: 0.9266\n",
      "Epoch 191/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0168 - accuracy: 0.9944 - val_loss: 0.4376 - val_accuracy: 0.9290\n",
      "Epoch 192/300\n",
      "6812/6812 [==============================] - 32s 5ms/sample - loss: 0.0464 - accuracy: 0.9887 - val_loss: 0.3010 - val_accuracy: 0.9425\n",
      "Epoch 193/300\n",
      "6812/6812 [==============================] - 33s 5ms/sample - loss: 0.0098 - accuracy: 0.9971 - val_loss: 0.3267 - val_accuracy: 0.9466\n",
      "Epoch 194/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0090 - accuracy: 0.9978 - val_loss: 0.2427 - val_accuracy: 0.9560\n",
      "Epoch 195/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0015 - accuracy: 0.9993 - val_loss: 0.2077 - val_accuracy: 0.9566\n",
      "Epoch 196/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 8.0815e-04 - accuracy: 0.9994 - val_loss: 0.1928 - val_accuracy: 0.9619\n",
      "Epoch 197/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0015 - accuracy: 0.9993 - val_loss: 0.2789 - val_accuracy: 0.9519\n",
      "Epoch 198/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0032 - accuracy: 0.9985 - val_loss: 0.3116 - val_accuracy: 0.9431\n",
      "Epoch 199/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0040 - accuracy: 0.9984 - val_loss: 0.3444 - val_accuracy: 0.9507\n",
      "Epoch 200/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0070 - accuracy: 0.9974 - val_loss: 0.5957 - val_accuracy: 0.9219\n",
      "Epoch 201/300\n",
      "6812/6812 [==============================] - 32s 5ms/sample - loss: 0.0023 - accuracy: 0.9991 - val_loss: 0.4121 - val_accuracy: 0.9225\n",
      "Epoch 202/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.3178 - val_accuracy: 0.9437\n",
      "Epoch 203/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0049 - accuracy: 0.9987 - val_loss: 0.3299 - val_accuracy: 0.9484\n",
      "Epoch 204/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.2423 - val_accuracy: 0.9577\n",
      "Epoch 205/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0033 - accuracy: 0.9984 - val_loss: 0.3836 - val_accuracy: 0.9384\n",
      "Epoch 206/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.3039 - val_accuracy: 0.9489\n",
      "Epoch 207/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0054 - accuracy: 0.9987 - val_loss: 0.2835 - val_accuracy: 0.9560\n",
      "Epoch 208/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0075 - accuracy: 0.9978 - val_loss: 0.2999 - val_accuracy: 0.9519\n",
      "Epoch 209/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0015 - accuracy: 0.9993 - val_loss: 0.2934 - val_accuracy: 0.9525\n",
      "Epoch 210/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 6.6442e-04 - accuracy: 0.9999 - val_loss: 0.2713 - val_accuracy: 0.9613\n",
      "Epoch 211/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 7.3532e-04 - accuracy: 0.9997 - val_loss: 0.2872 - val_accuracy: 0.9601\n",
      "Epoch 212/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 4.5826e-04 - accuracy: 0.9999 - val_loss: 0.2567 - val_accuracy: 0.9648\n",
      "Epoch 213/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 3.0565e-04 - accuracy: 0.9999 - val_loss: 0.3605 - val_accuracy: 0.9495\n",
      "Epoch 214/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0146 - accuracy: 0.9954 - val_loss: 0.5270 - val_accuracy: 0.9272\n",
      "Epoch 215/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0197 - accuracy: 0.9947 - val_loss: 0.3299 - val_accuracy: 0.9484\n",
      "Epoch 216/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0056 - accuracy: 0.9985 - val_loss: 0.3086 - val_accuracy: 0.9507\n",
      "Epoch 217/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0037 - accuracy: 0.9987 - val_loss: 0.3144 - val_accuracy: 0.9472\n",
      "Epoch 218/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0027 - accuracy: 0.9988 - val_loss: 0.3181 - val_accuracy: 0.9519\n",
      "Epoch 219/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.2620 - val_accuracy: 0.9542\n",
      "Epoch 220/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.3517 - val_accuracy: 0.9425\n",
      "Epoch 221/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0015 - accuracy: 0.9993 - val_loss: 0.2261 - val_accuracy: 0.9630\n",
      "Epoch 222/300\n",
      "6812/6812 [==============================] - 32s 5ms/sample - loss: 0.0072 - accuracy: 0.9975 - val_loss: 0.2595 - val_accuracy: 0.9548\n",
      "Epoch 223/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0092 - accuracy: 0.9974 - val_loss: 0.3062 - val_accuracy: 0.9442\n",
      "Epoch 224/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0068 - accuracy: 0.9975 - val_loss: 0.3774 - val_accuracy: 0.9349\n",
      "Epoch 225/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0107 - accuracy: 0.9977 - val_loss: 0.2529 - val_accuracy: 0.9519\n",
      "Epoch 226/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0094 - accuracy: 0.9974 - val_loss: 0.3680 - val_accuracy: 0.9437\n",
      "Epoch 227/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0072 - accuracy: 0.9975 - val_loss: 0.2487 - val_accuracy: 0.9566\n",
      "Epoch 228/300\n",
      "6812/6812 [==============================] - 30s 4ms/sample - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.2706 - val_accuracy: 0.9454\n",
      "Epoch 229/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0054 - accuracy: 0.9981 - val_loss: 0.2245 - val_accuracy: 0.9595\n",
      "Epoch 230/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0051 - accuracy: 0.9979 - val_loss: 0.2962 - val_accuracy: 0.9489\n",
      "Epoch 231/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0049 - accuracy: 0.9984 - val_loss: 0.3435 - val_accuracy: 0.9466\n",
      "Epoch 232/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0033 - accuracy: 0.9988 - val_loss: 0.2895 - val_accuracy: 0.9495\n",
      "Epoch 233/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0017 - accuracy: 0.9991 - val_loss: 0.2584 - val_accuracy: 0.9554\n",
      "Epoch 234/300\n",
      "6812/6812 [==============================] - 32s 5ms/sample - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.2997 - val_accuracy: 0.9548\n",
      "Epoch 235/300\n",
      "6812/6812 [==============================] - 30s 4ms/sample - loss: 3.0488e-04 - accuracy: 1.0000 - val_loss: 0.2492 - val_accuracy: 0.9566\n",
      "Epoch 236/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 3.2408e-05 - accuracy: 1.0000 - val_loss: 0.2322 - val_accuracy: 0.9583\n",
      "Epoch 237/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 7.8774e-05 - accuracy: 1.0000 - val_loss: 0.2215 - val_accuracy: 0.9613\n",
      "Epoch 238/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 4.1727e-05 - accuracy: 1.0000 - val_loss: 0.2150 - val_accuracy: 0.9589\n",
      "Epoch 239/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 6.0717e-05 - accuracy: 1.0000 - val_loss: 0.2087 - val_accuracy: 0.9624\n",
      "Epoch 240/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 5.2715e-05 - accuracy: 1.0000 - val_loss: 0.2200 - val_accuracy: 0.9572\n",
      "Epoch 241/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 4.7531e-05 - accuracy: 1.0000 - val_loss: 0.2065 - val_accuracy: 0.9595\n",
      "Epoch 242/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 5.2349e-05 - accuracy: 1.0000 - val_loss: 0.1948 - val_accuracy: 0.9630\n",
      "Epoch 243/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 7.9101e-05 - accuracy: 1.0000 - val_loss: 0.1846 - val_accuracy: 0.9619\n",
      "Epoch 244/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0442 - accuracy: 0.9891 - val_loss: 0.6856 - val_accuracy: 0.9049\n",
      "Epoch 245/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 0.0243 - accuracy: 0.9924 - val_loss: 0.2905 - val_accuracy: 0.9437\n",
      "Epoch 246/300\n",
      "6812/6812 [==============================] - 31s 4ms/sample - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.2603 - val_accuracy: 0.9501\n",
      "Epoch 247/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0025 - accuracy: 0.9990 - val_loss: 0.2588 - val_accuracy: 0.9560\n",
      "Epoch 248/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 5.4939e-04 - accuracy: 0.9999 - val_loss: 0.2081 - val_accuracy: 0.9624\n",
      "Epoch 249/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 2.8633e-04 - accuracy: 1.0000 - val_loss: 0.2031 - val_accuracy: 0.9595\n",
      "Epoch 250/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 1.0309e-04 - accuracy: 1.0000 - val_loss: 0.1992 - val_accuracy: 0.9660\n",
      "Epoch 251/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 2.1629e-04 - accuracy: 0.9999 - val_loss: 0.2396 - val_accuracy: 0.9548\n",
      "Epoch 252/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 1.7495e-04 - accuracy: 1.0000 - val_loss: 0.2176 - val_accuracy: 0.9613\n",
      "Epoch 253/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 1.3624e-04 - accuracy: 1.0000 - val_loss: 0.2257 - val_accuracy: 0.9624\n",
      "Epoch 254/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0112 - accuracy: 0.9972 - val_loss: 0.4927 - val_accuracy: 0.9225\n",
      "Epoch 255/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0222 - accuracy: 0.9950 - val_loss: 0.3215 - val_accuracy: 0.9448\n",
      "Epoch 256/300\n",
      "6812/6812 [==============================] - 30s 4ms/sample - loss: 0.0106 - accuracy: 0.9971 - val_loss: 0.3081 - val_accuracy: 0.9489\n",
      "Epoch 257/300\n",
      "6812/6812 [==============================] - 32s 5ms/sample - loss: 0.0033 - accuracy: 0.9985 - val_loss: 0.2455 - val_accuracy: 0.9613\n",
      "Epoch 258/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 8.4374e-04 - accuracy: 0.9997 - val_loss: 0.2379 - val_accuracy: 0.9577\n",
      "Epoch 259/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 3.8241e-04 - accuracy: 1.0000 - val_loss: 0.1978 - val_accuracy: 0.9648\n",
      "Epoch 260/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 6.3057e-04 - accuracy: 0.9997 - val_loss: 0.2388 - val_accuracy: 0.9624\n",
      "Epoch 261/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 1.9161e-04 - accuracy: 1.0000 - val_loss: 0.2035 - val_accuracy: 0.9665\n",
      "Epoch 262/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 1.0518e-04 - accuracy: 1.0000 - val_loss: 0.1991 - val_accuracy: 0.9677\n",
      "Epoch 263/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 2.8241e-05 - accuracy: 1.0000 - val_loss: 0.2000 - val_accuracy: 0.9671\n",
      "Epoch 264/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 1.9741e-05 - accuracy: 1.0000 - val_loss: 0.2004 - val_accuracy: 0.9683\n",
      "Epoch 265/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.2746 - val_accuracy: 0.9601\n",
      "Epoch 266/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0128 - accuracy: 0.9972 - val_loss: 0.9126 - val_accuracy: 0.8650\n",
      "Epoch 267/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0335 - accuracy: 0.9910 - val_loss: 0.2757 - val_accuracy: 0.9519\n",
      "Epoch 268/300\n",
      "6812/6812 [==============================] - 34s 5ms/sample - loss: 0.0105 - accuracy: 0.9971 - val_loss: 0.3325 - val_accuracy: 0.9478\n",
      "Epoch 269/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0049 - accuracy: 0.9984 - val_loss: 0.2966 - val_accuracy: 0.9484\n",
      "Epoch 270/300\n",
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 0.0016 - accuracy: 0.9991 - val_loss: 0.2247 - val_accuracy: 0.9607\n",
      "Epoch 271/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0011 - accuracy: 0.9994 - val_loss: 0.2149 - val_accuracy: 0.9542\n",
      "Epoch 272/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6812/6812 [==============================] - 29s 4ms/sample - loss: 1.3996e-04 - accuracy: 1.0000 - val_loss: 0.2263 - val_accuracy: 0.9554\n",
      "Epoch 273/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 1.0789e-04 - accuracy: 1.0000 - val_loss: 0.2298 - val_accuracy: 0.9572\n",
      "Epoch 274/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 9.0233e-05 - accuracy: 1.0000 - val_loss: 0.2066 - val_accuracy: 0.9619\n",
      "Epoch 275/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 1.2084e-04 - accuracy: 1.0000 - val_loss: 0.2098 - val_accuracy: 0.9613\n",
      "Epoch 276/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 3.3861e-05 - accuracy: 1.0000 - val_loss: 0.1963 - val_accuracy: 0.9636\n",
      "Epoch 277/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 1.5473e-04 - accuracy: 1.0000 - val_loss: 0.2120 - val_accuracy: 0.9613\n",
      "Epoch 278/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0012 - accuracy: 0.9994 - val_loss: 0.3786 - val_accuracy: 0.9419\n",
      "Epoch 279/300\n",
      "6812/6812 [==============================] - 33s 5ms/sample - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.3563 - val_accuracy: 0.9390\n",
      "Epoch 280/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.3668 - val_accuracy: 0.9472\n",
      "Epoch 281/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0205 - accuracy: 0.9947 - val_loss: 0.7092 - val_accuracy: 0.9184\n",
      "Epoch 282/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0063 - accuracy: 0.9979 - val_loss: 0.2598 - val_accuracy: 0.9531\n",
      "Epoch 283/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.2800 - val_accuracy: 0.9560\n",
      "Epoch 284/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 8.7879e-04 - accuracy: 0.9997 - val_loss: 0.3353 - val_accuracy: 0.9536\n",
      "Epoch 285/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.2836 - val_accuracy: 0.9519\n",
      "Epoch 286/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 4.1022e-04 - accuracy: 0.9999 - val_loss: 0.2750 - val_accuracy: 0.9577\n",
      "Epoch 287/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 1.9579e-04 - accuracy: 1.0000 - val_loss: 0.2476 - val_accuracy: 0.9607\n",
      "Epoch 288/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 3.5367e-04 - accuracy: 1.0000 - val_loss: 0.2430 - val_accuracy: 0.9624\n",
      "Epoch 289/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 2.3538e-04 - accuracy: 1.0000 - val_loss: 0.2398 - val_accuracy: 0.9619\n",
      "Epoch 290/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 2.8514e-04 - accuracy: 0.9999 - val_loss: 0.2797 - val_accuracy: 0.9536\n",
      "Epoch 291/300\n",
      "6812/6812 [==============================] - 31s 5ms/sample - loss: 7.2440e-05 - accuracy: 1.0000 - val_loss: 0.2549 - val_accuracy: 0.9607\n",
      "Epoch 292/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 2.2521e-04 - accuracy: 1.0000 - val_loss: 0.2338 - val_accuracy: 0.9589\n",
      "Epoch 293/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 5.6838e-05 - accuracy: 1.0000 - val_loss: 0.2239 - val_accuracy: 0.9619\n",
      "Epoch 294/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 6.9691e-05 - accuracy: 1.0000 - val_loss: 0.2211 - val_accuracy: 0.9660\n",
      "Epoch 295/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 1.8583e-05 - accuracy: 1.0000 - val_loss: 0.2156 - val_accuracy: 0.9648\n",
      "Epoch 296/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 8.7863e-06 - accuracy: 1.0000 - val_loss: 0.2148 - val_accuracy: 0.9660\n",
      "Epoch 297/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.4210 - val_accuracy: 0.9378\n",
      "Epoch 298/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0518 - accuracy: 0.9890 - val_loss: 0.3367 - val_accuracy: 0.9372\n",
      "Epoch 299/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0172 - accuracy: 0.9946 - val_loss: 0.2287 - val_accuracy: 0.9572\n",
      "Epoch 300/300\n",
      "6812/6812 [==============================] - 28s 4ms/sample - loss: 0.0018 - accuracy: 0.9993 - val_loss: 0.2056 - val_accuracy: 0.9665\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_full,y_full,validation_data=[X_test,y_test],\n",
    "                    epochs=300,batch_size=32,\n",
    "                    #class_weight=cw,\n",
    "                    #callbacks=[early_stopping]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "2143/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 984us/sample - loss: 2.0220 - accuracy: 0.7116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.359199850518766, 0.7116192]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X2,Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[191, 184, 101],\n",
       "       [121, 898,  58],\n",
       "       [ 56,  98, 436]], dtype=int64)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=model.predict(X2)\n",
    "metrics.confusion_matrix(np.argmax(Y2,axis=1),np.argmax(y_pred,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:0.966549\n",
      "train:1.000000\n",
      "train: \n",
      " [[1503    0    0]\n",
      " [   0 3425    0]\n",
      " [   0    0 1884]]\n",
      "test: \n",
      " [[371  10  10]\n",
      " [ 15 823   8]\n",
      " [  9   5 453]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_t=model.predict(X_test)\n",
    "test = metrics.accuracy_score(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1))\n",
    "#y_pred_v=model.predict(x_valid)\n",
    "#valid = metrics.accuracy_score(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1))\n",
    "y_pred_ta=model.predict(X_full)\n",
    "train = metrics.accuracy_score(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1))\n",
    "print('test:%f'%test)\n",
    "#print('valid:%f'%valid)\n",
    "print('train:%f'%train)\n",
    "print('train: \\n',metrics.confusion_matrix(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1)))\n",
    "#print('valid: \\n',metrics.confusion_matrix(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1)))\n",
    "print('test: \\n',metrics.confusion_matrix(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model/cnn.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
