{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder,normalize\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "import data_processing as dp\n",
    "from scipy import signal\n",
    "from scipy.stats import skew,pearsonr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "from itertools import combinations\n",
    "from sklearn.feature_selection import SelectKBest,f_classif,chi2,mutual_info_classif,VarianceThreshold,RFE,SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\data\\file_name.txt\n",
      ".\\data\\G04_FoG_trial_1_emg.csv\n",
      ".\\data\\G04_FoG_trial_2_emg.csv\n",
      ".\\data\\G06_FoG_trial_1_emg.csv\n",
      ".\\data\\G06_FoG_trial_2_emg.csv\n",
      ".\\data\\G06_FoG_trial_3_emg.csv\n",
      ".\\data\\G07_Freezing_Trial1_trial_1_emg.csv\n",
      ".\\data\\G08_FoG_1_trial_1_emg.csv\n",
      ".\\data\\G08_FoG_2_trial_1_emg.csv\n",
      ".\\data\\G11_FoG_trial_1_emg.csv\n",
      ".\\data\\G11_FoG_trial_2_emg.csv\n",
      ".\\data\\P379_M050_2_OFF_A_FoG_trial_1_emg.csv\n",
      ".\\data\\P379_M050_2_OFF_A_FoG_trial_2_emg.csv\n",
      ".\\data\\P379_M050_2_OFF_A_FoG_trial_3_emg.csv\n",
      ".\\data\\P379_M050_2_OFF_B_FoG_trial_1_emg.csv\n",
      ".\\data\\P379_M050_2_OFF_B_FoG_trial_2_emg.csv\n",
      ".\\data\\P379_M050_2_OFF_B_FoG_trial_3_emg.csv\n",
      ".\\data\\P551_M050_2_A_FoG_trial_1_emg.csv\n",
      ".\\data\\P551_M050_2_B_FoG_trial_1_emg.csv\n",
      ".\\data\\P551_M050_2_B_FoG_trial_2_emg.csv\n",
      ".\\data\\P812_M050_2_B_FoG_trial_1_emg.csv\n",
      ".\\data\\P812_M050_2_B_FoG_trial_2_emg.csv\n",
      ".\\data\\some_of_rest.txt\n",
      ".\\data\\其他\\labels.txt\n",
      ".\\data\\其他\\P812_M050_2_B_FoG_trial1_annotation.csv\n",
      ".\\data\\其他\\P812_M050_2_B_FoG_trials.mat\n",
      ".\\data\\其他\\P812_M050_2_B_FoG_trial_1_out_left_foot.csv\n",
      ".\\data\\其他\\P812_M050_2_B_FoG_trial_1_out_lower_left_foot.csv\n",
      ".\\data\\其他\\P812_M050_2_B_FoG_trial_1_out_lower_right_foot.csv\n",
      ".\\data\\其他\\P812_M050_2_B_FoG_trial_1_out_right_foot.csv\n",
      ".\\data\\其他\\P812_M050_2_B_FoG_trial_2_out_left_foot.csv\n",
      ".\\data\\其他\\P812_M050_2_B_FoG_trial_2_out_lower_left_foot.csv\n",
      ".\\data\\其他\\P812_M050_2_B_FoG_trial_2_out_lower_right_foot.csv\n",
      ".\\data\\其他\\P812_M050_2_B_FoG_trial_2_out_right_foot.csv\n",
      ".\\data\\其他\\P812_M50_2_B_FoG_trial2_annotation.csv\n",
      ".\\data\\正常\\G02_Walking_trial_1_emg.csv\n",
      ".\\data\\正常\\G03_Walking_trial_1_emg.csv\n",
      ".\\data\\正常\\G03_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\G05_Walking_struct_fixed_trial_1_emg.csv\n",
      ".\\data\\正常\\G05_Walking_struct_fixed_trial_2_emg.csv\n",
      ".\\data\\正常\\G05_Walking_struct_fixed_trial_3_emg.csv\n",
      ".\\data\\正常\\G09_FoG_trial_1_emg.csv\n",
      ".\\data\\正常\\G09_FoG_trial_2_emg.csv\n",
      ".\\data\\正常\\G09_FoG_trial_3_emg.csv\n",
      ".\\data\\正常\\G09_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\G09_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\G09_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\G11_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\G11_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P231_M050_A_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P231_M050_A_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P231_M050_A_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\P231_M050_B_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P231_M050_B_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P231_M050_B_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\P231_M100_2_A_FoG_trial_3_emg.csv\n",
      ".\\data\\正常\\P231_M100_2_A_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P231_M100_2_A_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\P231_M100_ON_A_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P231_M100_ON_A_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P231_M100_ON_A_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\P231_Msham_A_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P231_Msham_A_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\P231_Msham_B_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P351_M050_2_A_FoG_trial_1_emg.csv\n",
      ".\\data\\正常\\P351_M050_2_A_FoG_trial_2_emg.csv\n",
      ".\\data\\正常\\P351_M050_2_A_FoG_trial_3_emg.csv\n",
      ".\\data\\正常\\P351_M050_2_A_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P351_M050_2_A_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P351_M050_2_A_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\P351_M050_2_B_FoG_trial_1_emg.csv\n",
      ".\\data\\正常\\P351_M050_2_B_FoG_trial_2_emg.csv\n",
      ".\\data\\正常\\P351_M050_2_B_FoG_trial_3_emg.csv\n",
      ".\\data\\正常\\P351_M050_2_B_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P351_M050_2_B_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P351_M050_2_B_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\P351_M050_A_FoG_trial_1_emg.csv\n",
      ".\\data\\正常\\P351_M050_A_FoG_trial_2_emg.csv\n",
      ".\\data\\正常\\P351_M050_A_FoG_trial_3_emg.csv\n",
      ".\\data\\正常\\P351_M050_A_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P351_M050_A_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P351_M050_B_FoG_trial_1_emg.csv\n",
      ".\\data\\正常\\P351_M050_B_FoG_trial_2_emg.csv\n",
      ".\\data\\正常\\P351_M050_B_FoG_trial_3_emg.csv\n",
      ".\\data\\正常\\P351_M050_B_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P351_M050_B_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P351_M050_B_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\P351_Msham_A_FoG_trial_1_emg.csv\n",
      ".\\data\\正常\\P351_Msham_A_FoG_trial_2_emg.csv\n",
      ".\\data\\正常\\P351_Msham_A_FoG_trial_3_emg.csv\n",
      ".\\data\\正常\\P351_Msham_A_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P351_Msham_A_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P351_Msham_A_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\P351_Msham_B_FoG_trial_1_emg.csv\n",
      ".\\data\\正常\\P351_Msham_B_FoG_trial_2_emg.csv\n",
      ".\\data\\正常\\P351_Msham_B_FoG_trial_3_emg.csv\n",
      ".\\data\\正常\\P351_Msham_B_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P351_Msham_B_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P351_Msham_B_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\P379_M050_A_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P379_M050_A_Walking_trial_3_emg.csv\n",
      ".\\data\\正常\\P379_M050_B_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P379_Msham_B_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\P533_M050_A_Walking_trial_1_emg.csv\n",
      ".\\data\\正常\\P533_M050_A_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P533_M050_B_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P533_M050_B_Walking_trial_3_emg.csv\n",
      ".\\data\\正常\\P533_M100_A_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P533_M100_B_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P551_M50_B_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\P623_M050_2_A_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P623_M050_2_A_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P623_M050_2_A_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\P623_M050_2_B_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P623_M050_2_B_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\P623_M050_A_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P623_M100_A_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P623_M100_B_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P623_Msham_A_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P623_Msham_A_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\P623_Msham_B_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P623_Msham_B_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P645_M050_A_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P645_M050_A_Walking_trial_3_emg.csv\n",
      ".\\data\\正常\\P645_M050_B_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P645_M050_B_Walking_trial_3_emg.csv\n",
      ".\\data\\正常\\P812_M050_2_A_FoG_trial_1_emg.csv\n",
      ".\\data\\正常\\P812_M050_2_A_FoG_trial_3_emg.csv\n",
      ".\\data\\正常\\P812_M050_2_A_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P812_M050_2_A_Walking_trial_3_emg.csv\n",
      ".\\data\\正常\\P812_M050_2_B_Walking_1_trial_4_emg.csv\n",
      ".\\data\\正常\\P812_M050_A_FoG_trial_1_emg.csv\n",
      ".\\data\\正常\\P812_M050_A_FoG_trial_2_emg.csv\n",
      ".\\data\\正常\\P812_M050_A_FoG_trial_3_emg.csv\n",
      ".\\data\\正常\\P812_M050_A_Walking_trial_1_emg.csv\n",
      ".\\data\\正常\\P812_M050_A_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P812_M050_B_FoG_trial_1_emg.csv\n",
      ".\\data\\正常\\P812_M050_B_FoG_trial_2_emg.csv\n",
      ".\\data\\正常\\P812_M050_B_FoG_trial_3_emg.csv\n",
      ".\\data\\正常\\P812_M050_B_Walking_trial_1_emg.csv\n",
      ".\\data\\正常\\P812_M050_B_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P812_M100_A_FoG_trial_1_emg.csv\n",
      ".\\data\\正常\\P812_M100_A_Walking_trial_3_emg.csv\n",
      ".\\data\\正常\\P812_M100_B_FoG_trial_1_emg.csv\n",
      ".\\data\\正常\\P812_M100_B_FoG_trial_3_emg.csv\n",
      ".\\data\\正常\\P812_M100_B_Walking2_trial_1_emg.csv\n",
      ".\\data\\正常\\P812_M100_B_Walking2_trial_2_emg.csv\n",
      ".\\data\\正常\\P876_M100_B_FoG_trial_1_emg.csv\n",
      ".\\data\\正常\\P876_M100_B_FoG_trial_2_emg.csv\n",
      ".\\data\\正常\\P876_M100_B_FoG_trial_3_emg.csv\n",
      ".\\data\\正常\\P876_M100_B_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P876_M100_B_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\P940_M050_2_A_FoG_trial_3_emg.csv\n",
      ".\\data\\正常\\P940_M050_2_A_FoG_trial_4_emg.csv\n",
      ".\\data\\正常\\P940_M050_2_A_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P940_M050_2_B_FoG_trial_1_emg.csv\n",
      ".\\data\\正常\\P940_M050_2_B_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P940_M050_2_B_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P940_M050_2_B_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\P940_M050_A_FoG_trial_2_emg.csv\n",
      ".\\data\\正常\\P940_M050_A_FoG_trial_3_emg.csv\n",
      ".\\data\\正常\\P940_M050_A_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P940_M050_A_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P940_M050_A_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\P940_M050_B_FoG_trial_1_emg.csv\n",
      ".\\data\\正常\\P940_M050_B_FoG_trial_2_emg.csv\n",
      ".\\data\\正常\\P940_M050_B_FoG_trial_3_emg.csv\n",
      ".\\data\\正常\\P940_M050_B_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P940_M050_B_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P940_M050_B_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\P940_M100_A_FoG_trial_1_emg.csv\n",
      ".\\data\\正常\\P940_M100_A_FoG_trial_2_emg.csv\n",
      ".\\data\\正常\\P940_M100_A_FoG_trial_3_emg.csv\n",
      ".\\data\\正常\\P940_M100_A_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P940_M100_A_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P940_M100_A_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\P940_M100_B_FoG_trial_2_emg.csv\n",
      ".\\data\\正常\\P940_M100_B_FoG_trial_3_emg.csv\n",
      ".\\data\\正常\\P940_M100_B_Walking_2_trial_2_emg.csv\n",
      ".\\data\\正常\\P940_M100_B_Walking_2_trial_6_emg.csv\n",
      ".\\data\\正常\\P940_MSham_A_FoG_trial_1_emg.csv\n",
      ".\\data\\正常\\P940_MSham_A_FoG_trial_3_emg.csv\n",
      ".\\data\\正常\\P940_MSham_A_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P940_MSham_A_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P940_MSham_A_Walking_trial_6_emg.csv\n",
      ".\\data\\正常\\P940_MSham_B_Walking_trial_2_emg.csv\n",
      ".\\data\\正常\\P940_MSham_B_Walking_trial_4_emg.csv\n",
      ".\\data\\正常\\P940_MSham_B_Walking_trial_6_emg.csv\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('.\\data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 = '.\\data\\正常\\G11_Walking_trial_4_emg.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv('./processed data/dataframe_W1024_S256_DWTLmax_samelabel_sc_detrend.csv')\n",
    "\n",
    "# read file name of data with various Labels\n",
    "df = pd.read_csv('./useful_data_label.csv',index_col=0) \n",
    "drop = 'G04_FoG_trial_1_emg.csv'\n",
    "drop2= 'G04_FoG_trial_2_emg.csv'\n",
    "drop3= 'P812_M050_2_B_FoG_trial_1_emg.csv'\n",
    "ind_drop = (df.columns!=drop) & (df.columns!=drop2) #& (df.columns!=drop3)\n",
    "# read file name of data with only label 0\n",
    "df2 = pd.read_csv('./unuseful_data_label.csv',index_col=0)\n",
    "# read some of the data with only label 0\n",
    "df3 = pd.read_csv('./data/file_name.txt',header=None)\n",
    "\n",
    "files = np.concatenate([np.array(df.columns),np.array(df3.loc[:,0])])\n",
    "ind = Data.File.isin('detrend_300_'+files)\n",
    "ind2 = Data.File == drop\n",
    "#ind = (Data.File != drop) & (Data.File != drop2)\n",
    "Data_sel = Data#[ind]\n",
    "Data_rest = Data[ind2]\n",
    "#ind2 = Data_rest.File == drop\n",
    "#Data_rest = Data_rest[ind2]\n",
    "#Data_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_col = ['_IEMG','_MAV','_SSI','_VAR','_RMS',\n",
    "               '_WL','_ZC','_SSC','_WAMP','_skew','_Acti','_AR','_HIST','_MDF']\n",
    "\n",
    "feature_all = Data_sel.iloc[:,1:-1]\n",
    "#ind_temp1 = feature_all.columns.str.contains('_mDWT')\n",
    "#ind_temp2 = feature_all.columns.str.contains('_Coe')\n",
    "#ind_temp3 = feature_all.columns.str.contains('_Scale')\n",
    "#ind_temp4 = feature_all.columns.str.contains('_HIST')\n",
    "#ind_temp = ind_temp1|ind_temp2|ind_temp3|ind_temp4\n",
    "ind_temp = feature_all.columns.str.contains('_Acti')\n",
    "#feature = feature_all.loc[:,~ind_temp]\n",
    "feature = Data_sel.iloc[:,1:-1]\n",
    "#feature = Data_sel.iloc[:,:-2]\n",
    "y = Data_sel.Label\n",
    "feature2_all = Data_rest.iloc[:,1:-1]\n",
    "#feature2 = feature2_all.loc[:,~ind_temp]\n",
    "feature2 = Data_rest.iloc[:,1:-1]\n",
    "#feature2 = Data_rest.iloc[:,:-2]\n",
    "y2 = Data_rest.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = pd.DataFrame()\n",
    "feature2 = pd.DataFrame()\n",
    "y = pd.DataFrame()\n",
    "y2 = pd.DataFrame()\n",
    "m = 0\n",
    "for i in 'detrend_300_'+df.columns:\n",
    "    ind = Data_sel.File==i\n",
    "    temp = Data_sel[ind]\n",
    "    #if i != 'G08_FoG_2_trial_1_emg.csv':\n",
    "        #feature = pd.concat([feature,temp.iloc[:,1:-1]])\n",
    "        #y = pd.concat([y,temp.iloc[:,0]])\n",
    "    #else:\n",
    "    for j in set(temp.Label):\n",
    "        ind2 = temp.Label == j\n",
    "        temp2 = temp[ind2]\n",
    "        l = len(temp2)\n",
    "        feature = pd.concat([feature,temp2.iloc[:int(0.8*l),1:-1]])\n",
    "        y = pd.concat([y,temp2.iloc[:int(0.8*l),0]])\n",
    "        feature2 = pd.concat([feature2,temp2.iloc[int(0.8*l):,1:-1]])\n",
    "        y2 = pd.concat([y2,temp2.iloc[int(0.8*l):,0]])\n",
    "y = np.array(y)[:,0]\n",
    "y2 = np.array(y2)[:,0]\n",
    "#ind_temp = feature.columns.str.contains('_mDWT')\n",
    "#feature = feature.loc[:,~ind_temp]\n",
    "#feature2 = feature2.loc[:,~ind_temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(649,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ind_temp1 = feature_all.columns.str.contains('_MF')\n",
    "#ind_temp2 = feature_all.columns.str.contains('_MDF')\n",
    "#ind_temp3 = feature_all.columns.str.contains('_AR')\n",
    "#ind_temp4 = feature_all.columns.str.contains('_MNF')\n",
    "#ind_temp = ind_temp1|ind_temp2|ind_temp3|ind_temp4\n",
    "#feature = feature.loc[:,~ind_temp]\n",
    "#feature2 = feature2.loc[:,~ind_temp]\n",
    "#('detrend_300_'+files)\n",
    "y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>LEFT_TA_IEMG</th>\n",
       "      <th>LEFT_TS_IEMG</th>\n",
       "      <th>LEFT_BF_IEMG</th>\n",
       "      <th>LEFT_RF_IEMG</th>\n",
       "      <th>RIGHT_TA_IEMG</th>\n",
       "      <th>RIGHT_TS_IEMG</th>\n",
       "      <th>RIGHT_BF_IEMG</th>\n",
       "      <th>RIGHT_RF_IEMG</th>\n",
       "      <th>LEFT_TA_SSI</th>\n",
       "      <th>...</th>\n",
       "      <th>RIGHT_RF_mDWT1</th>\n",
       "      <th>RIGHT_RF_mDWT2</th>\n",
       "      <th>RIGHT_RF_mDWT3</th>\n",
       "      <th>RIGHT_RF_mDWT4</th>\n",
       "      <th>RIGHT_RF_mDWT5</th>\n",
       "      <th>RIGHT_RF_mDWT6</th>\n",
       "      <th>RIGHT_RF_mDWT7</th>\n",
       "      <th>RIGHT_RF_mDWT8</th>\n",
       "      <th>RIGHT_RF_mDWT9</th>\n",
       "      <th>File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>532.99800</td>\n",
       "      <td>682.87820</td>\n",
       "      <td>617.53705</td>\n",
       "      <td>496.16614</td>\n",
       "      <td>448.80520</td>\n",
       "      <td>706.48083</td>\n",
       "      <td>752.47130</td>\n",
       "      <td>588.93210</td>\n",
       "      <td>1024.0030</td>\n",
       "      <td>...</td>\n",
       "      <td>285.226318</td>\n",
       "      <td>154.851379</td>\n",
       "      <td>59.555408</td>\n",
       "      <td>31.748150</td>\n",
       "      <td>3.224998</td>\n",
       "      <td>1.554702</td>\n",
       "      <td>0.766442</td>\n",
       "      <td>0.344387</td>\n",
       "      <td>0.195914</td>\n",
       "      <td>detrend_300_G06_FoG_trial_1_emg.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>610.86285</td>\n",
       "      <td>688.22974</td>\n",
       "      <td>597.88810</td>\n",
       "      <td>566.05260</td>\n",
       "      <td>516.76044</td>\n",
       "      <td>660.64325</td>\n",
       "      <td>485.37173</td>\n",
       "      <td>482.71650</td>\n",
       "      <td>1024.0051</td>\n",
       "      <td>...</td>\n",
       "      <td>331.483459</td>\n",
       "      <td>208.414215</td>\n",
       "      <td>122.168480</td>\n",
       "      <td>97.462158</td>\n",
       "      <td>77.872330</td>\n",
       "      <td>57.111313</td>\n",
       "      <td>28.516342</td>\n",
       "      <td>14.124079</td>\n",
       "      <td>6.673710</td>\n",
       "      <td>detrend_300_G06_FoG_trial_1_emg.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>717.85050</td>\n",
       "      <td>731.97920</td>\n",
       "      <td>594.86550</td>\n",
       "      <td>674.61707</td>\n",
       "      <td>651.91720</td>\n",
       "      <td>717.54380</td>\n",
       "      <td>542.46875</td>\n",
       "      <td>607.74850</td>\n",
       "      <td>1024.0077</td>\n",
       "      <td>...</td>\n",
       "      <td>291.858368</td>\n",
       "      <td>109.412689</td>\n",
       "      <td>42.897900</td>\n",
       "      <td>23.493301</td>\n",
       "      <td>2.436239</td>\n",
       "      <td>1.730924</td>\n",
       "      <td>0.861943</td>\n",
       "      <td>0.425198</td>\n",
       "      <td>0.203946</td>\n",
       "      <td>detrend_300_G06_FoG_trial_1_emg.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>784.56880</td>\n",
       "      <td>767.33844</td>\n",
       "      <td>636.39390</td>\n",
       "      <td>761.74896</td>\n",
       "      <td>743.55646</td>\n",
       "      <td>753.26245</td>\n",
       "      <td>582.45764</td>\n",
       "      <td>707.13880</td>\n",
       "      <td>1024.0006</td>\n",
       "      <td>...</td>\n",
       "      <td>345.837891</td>\n",
       "      <td>156.745499</td>\n",
       "      <td>52.142414</td>\n",
       "      <td>29.755404</td>\n",
       "      <td>5.823022</td>\n",
       "      <td>4.090141</td>\n",
       "      <td>2.037884</td>\n",
       "      <td>1.007435</td>\n",
       "      <td>0.474716</td>\n",
       "      <td>detrend_300_G06_FoG_trial_1_emg.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>784.83930</td>\n",
       "      <td>761.79780</td>\n",
       "      <td>694.74960</td>\n",
       "      <td>790.04110</td>\n",
       "      <td>730.96857</td>\n",
       "      <td>763.32684</td>\n",
       "      <td>623.00880</td>\n",
       "      <td>793.52216</td>\n",
       "      <td>1023.9998</td>\n",
       "      <td>...</td>\n",
       "      <td>385.259094</td>\n",
       "      <td>196.491730</td>\n",
       "      <td>57.551464</td>\n",
       "      <td>25.937048</td>\n",
       "      <td>14.986317</td>\n",
       "      <td>10.811022</td>\n",
       "      <td>5.363523</td>\n",
       "      <td>2.578475</td>\n",
       "      <td>1.295517</td>\n",
       "      <td>detrend_300_G06_FoG_trial_1_emg.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3163</th>\n",
       "      <td>0</td>\n",
       "      <td>742.91990</td>\n",
       "      <td>697.15643</td>\n",
       "      <td>622.27050</td>\n",
       "      <td>819.97250</td>\n",
       "      <td>718.64650</td>\n",
       "      <td>567.61755</td>\n",
       "      <td>793.67810</td>\n",
       "      <td>672.50260</td>\n",
       "      <td>1024.0135</td>\n",
       "      <td>...</td>\n",
       "      <td>350.094971</td>\n",
       "      <td>181.753113</td>\n",
       "      <td>64.880173</td>\n",
       "      <td>35.884857</td>\n",
       "      <td>24.737333</td>\n",
       "      <td>18.071045</td>\n",
       "      <td>9.008681</td>\n",
       "      <td>4.517670</td>\n",
       "      <td>2.074450</td>\n",
       "      <td>detrend_300_P812_M050_2_B_FoG_trial_2_emg.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3164</th>\n",
       "      <td>0</td>\n",
       "      <td>717.32360</td>\n",
       "      <td>697.38390</td>\n",
       "      <td>581.20807</td>\n",
       "      <td>813.19196</td>\n",
       "      <td>734.41210</td>\n",
       "      <td>680.13920</td>\n",
       "      <td>787.93365</td>\n",
       "      <td>643.52880</td>\n",
       "      <td>1024.0034</td>\n",
       "      <td>...</td>\n",
       "      <td>315.500580</td>\n",
       "      <td>167.535095</td>\n",
       "      <td>45.436825</td>\n",
       "      <td>19.283266</td>\n",
       "      <td>16.084208</td>\n",
       "      <td>11.476061</td>\n",
       "      <td>5.672655</td>\n",
       "      <td>2.823398</td>\n",
       "      <td>1.397688</td>\n",
       "      <td>detrend_300_P812_M050_2_B_FoG_trial_2_emg.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3165</th>\n",
       "      <td>0</td>\n",
       "      <td>643.79970</td>\n",
       "      <td>676.41580</td>\n",
       "      <td>580.67676</td>\n",
       "      <td>812.15770</td>\n",
       "      <td>720.61710</td>\n",
       "      <td>659.99330</td>\n",
       "      <td>786.36615</td>\n",
       "      <td>675.54670</td>\n",
       "      <td>1024.0194</td>\n",
       "      <td>...</td>\n",
       "      <td>412.005890</td>\n",
       "      <td>259.395691</td>\n",
       "      <td>151.239639</td>\n",
       "      <td>113.130943</td>\n",
       "      <td>101.270905</td>\n",
       "      <td>74.047607</td>\n",
       "      <td>36.972191</td>\n",
       "      <td>18.248516</td>\n",
       "      <td>8.697049</td>\n",
       "      <td>detrend_300_P812_M050_2_B_FoG_trial_2_emg.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3166</th>\n",
       "      <td>0</td>\n",
       "      <td>611.56714</td>\n",
       "      <td>689.03000</td>\n",
       "      <td>609.06960</td>\n",
       "      <td>814.09170</td>\n",
       "      <td>726.28200</td>\n",
       "      <td>661.06270</td>\n",
       "      <td>785.89870</td>\n",
       "      <td>665.97300</td>\n",
       "      <td>1024.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>330.379303</td>\n",
       "      <td>187.993164</td>\n",
       "      <td>67.779640</td>\n",
       "      <td>25.933273</td>\n",
       "      <td>3.889570</td>\n",
       "      <td>2.537868</td>\n",
       "      <td>1.293690</td>\n",
       "      <td>0.790001</td>\n",
       "      <td>0.241916</td>\n",
       "      <td>detrend_300_P812_M050_2_B_FoG_trial_2_emg.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3167</th>\n",
       "      <td>0</td>\n",
       "      <td>508.96143</td>\n",
       "      <td>664.95850</td>\n",
       "      <td>621.22690</td>\n",
       "      <td>809.22797</td>\n",
       "      <td>722.91410</td>\n",
       "      <td>653.01666</td>\n",
       "      <td>786.25195</td>\n",
       "      <td>660.28564</td>\n",
       "      <td>1024.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>274.473389</td>\n",
       "      <td>160.600861</td>\n",
       "      <td>56.822662</td>\n",
       "      <td>24.241621</td>\n",
       "      <td>14.858330</td>\n",
       "      <td>10.114450</td>\n",
       "      <td>5.038644</td>\n",
       "      <td>2.484520</td>\n",
       "      <td>1.321538</td>\n",
       "      <td>detrend_300_P812_M050_2_B_FoG_trial_2_emg.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3168 rows × 338 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label  LEFT_TA_IEMG  LEFT_TS_IEMG  LEFT_BF_IEMG  LEFT_RF_IEMG  \\\n",
       "0         0     532.99800     682.87820     617.53705     496.16614   \n",
       "1         0     610.86285     688.22974     597.88810     566.05260   \n",
       "2         0     717.85050     731.97920     594.86550     674.61707   \n",
       "3         0     784.56880     767.33844     636.39390     761.74896   \n",
       "4         0     784.83930     761.79780     694.74960     790.04110   \n",
       "...     ...           ...           ...           ...           ...   \n",
       "3163      0     742.91990     697.15643     622.27050     819.97250   \n",
       "3164      0     717.32360     697.38390     581.20807     813.19196   \n",
       "3165      0     643.79970     676.41580     580.67676     812.15770   \n",
       "3166      0     611.56714     689.03000     609.06960     814.09170   \n",
       "3167      0     508.96143     664.95850     621.22690     809.22797   \n",
       "\n",
       "      RIGHT_TA_IEMG  RIGHT_TS_IEMG  RIGHT_BF_IEMG  RIGHT_RF_IEMG  LEFT_TA_SSI  \\\n",
       "0         448.80520      706.48083      752.47130      588.93210    1024.0030   \n",
       "1         516.76044      660.64325      485.37173      482.71650    1024.0051   \n",
       "2         651.91720      717.54380      542.46875      607.74850    1024.0077   \n",
       "3         743.55646      753.26245      582.45764      707.13880    1024.0006   \n",
       "4         730.96857      763.32684      623.00880      793.52216    1023.9998   \n",
       "...             ...            ...            ...            ...          ...   \n",
       "3163      718.64650      567.61755      793.67810      672.50260    1024.0135   \n",
       "3164      734.41210      680.13920      787.93365      643.52880    1024.0034   \n",
       "3165      720.61710      659.99330      786.36615      675.54670    1024.0194   \n",
       "3166      726.28200      661.06270      785.89870      665.97300    1024.0010   \n",
       "3167      722.91410      653.01666      786.25195      660.28564    1024.0001   \n",
       "\n",
       "      ...  RIGHT_RF_mDWT1  RIGHT_RF_mDWT2  RIGHT_RF_mDWT3  RIGHT_RF_mDWT4  \\\n",
       "0     ...      285.226318      154.851379       59.555408       31.748150   \n",
       "1     ...      331.483459      208.414215      122.168480       97.462158   \n",
       "2     ...      291.858368      109.412689       42.897900       23.493301   \n",
       "3     ...      345.837891      156.745499       52.142414       29.755404   \n",
       "4     ...      385.259094      196.491730       57.551464       25.937048   \n",
       "...   ...             ...             ...             ...             ...   \n",
       "3163  ...      350.094971      181.753113       64.880173       35.884857   \n",
       "3164  ...      315.500580      167.535095       45.436825       19.283266   \n",
       "3165  ...      412.005890      259.395691      151.239639      113.130943   \n",
       "3166  ...      330.379303      187.993164       67.779640       25.933273   \n",
       "3167  ...      274.473389      160.600861       56.822662       24.241621   \n",
       "\n",
       "      RIGHT_RF_mDWT5  RIGHT_RF_mDWT6  RIGHT_RF_mDWT7  RIGHT_RF_mDWT8  \\\n",
       "0           3.224998        1.554702        0.766442        0.344387   \n",
       "1          77.872330       57.111313       28.516342       14.124079   \n",
       "2           2.436239        1.730924        0.861943        0.425198   \n",
       "3           5.823022        4.090141        2.037884        1.007435   \n",
       "4          14.986317       10.811022        5.363523        2.578475   \n",
       "...              ...             ...             ...             ...   \n",
       "3163       24.737333       18.071045        9.008681        4.517670   \n",
       "3164       16.084208       11.476061        5.672655        2.823398   \n",
       "3165      101.270905       74.047607       36.972191       18.248516   \n",
       "3166        3.889570        2.537868        1.293690        0.790001   \n",
       "3167       14.858330       10.114450        5.038644        2.484520   \n",
       "\n",
       "      RIGHT_RF_mDWT9                                           File  \n",
       "0           0.195914            detrend_300_G06_FoG_trial_1_emg.csv  \n",
       "1           6.673710            detrend_300_G06_FoG_trial_1_emg.csv  \n",
       "2           0.203946            detrend_300_G06_FoG_trial_1_emg.csv  \n",
       "3           0.474716            detrend_300_G06_FoG_trial_1_emg.csv  \n",
       "4           1.295517            detrend_300_G06_FoG_trial_1_emg.csv  \n",
       "...              ...                                            ...  \n",
       "3163        2.074450  detrend_300_P812_M050_2_B_FoG_trial_2_emg.csv  \n",
       "3164        1.397688  detrend_300_P812_M050_2_B_FoG_trial_2_emg.csv  \n",
       "3165        8.697049  detrend_300_P812_M050_2_B_FoG_trial_2_emg.csv  \n",
       "3166        0.241916  detrend_300_P812_M050_2_B_FoG_trial_2_emg.csv  \n",
       "3167        1.321538  detrend_300_P812_M050_2_B_FoG_trial_2_emg.csv  \n",
       "\n",
       "[3168 rows x 338 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "def get_features_from_dwt(data,wavelet='db7',level=5):\n",
    "    coes = pywt.wavedec(data,wavelet=wavelet,mode=0,level=level,axis=1)\n",
    "    #columns = pd.Index(['LEFT_TA', 'LEFT_TS', 'LEFT_BF', 'LEFT_RF',\n",
    "    #   'RIGHT_TA', 'RIGHT_TS', 'RIGHT_BF', 'RIGHT_RF'])\n",
    "    columns = pd.Index(['LEFT_TA', 'LEFT_TS','RIGHT_TA', 'RIGHT_TS'])\n",
    "    feature = pd.DataFrame()\n",
    "    for i in range(len(coes)):\n",
    "        #IEMG = pd.DataFrame(compute_IEMG(coes[i]),columns=columns+'_IEMG')\n",
    "        RMS = pd.DataFrame(dp.compute_RMS(coes[i]),columns=columns+'_RMS%d'%i)\n",
    "        #WL = pd.DataFrame(dp.compute_WL(coes[i]),columns=columns+'_WL%d'%i)\n",
    "        ZC = pd.DataFrame(dp.compute_ZC(coes[i],1e-6),columns=columns+'_ZC%d'%i)\n",
    "        ku = pd.DataFrame(dp.compute_ku(coes[i]),columns=columns+'_ku%d'%i)\n",
    "        #SSC = pd.DataFrame(dp.compute_SSC(coes[i],threshold_SSC),columns=columns+'_SSC%d'%i)\n",
    "        #WAMP = pd.DataFrame(dp.compute_WAMP(coes[i],threshold_WAMP),columns=columns+'_WAMP%d'%i)\n",
    "        skew = pd.DataFrame(dp.compute_Skewness(coes[i]),columns=columns+'_skew%d'%i)\n",
    "        Acti = pd.DataFrame(dp.compute_Acti(coes[i]),columns=columns+'_Acti%d'%i)\n",
    "        #AR = pd.DataFrame(dp.compute_AR(coes[i]),columns=columns+'_AR%d'%i)\n",
    "        #AR = dp.compute_AR_pd(coes[i])\n",
    "        #HIST = dp.compute_HIST_pd(coes[i],bins=bins,ranges=ranges)\n",
    "        #FHIST = dp.compute_FHIST_pd(coes[i],bins=fbins,ranges=franges,threshold=threshold_F)\n",
    "        #MF = dp.compute_MaxFreq_pd(coes[i],num=num)\n",
    "        #MDF = pd.DataFrame(dp.compute_MDF(coes[i]),columns=columns+'_MDF%d'%i)\n",
    "        #MNF = pd.DataFrame(dp.compute_MNF(coes[i]),columns=columns+'_MNF%d'%i)\n",
    "        feature = pd.concat([feature,RMS,ZC,ku,skew,Acti],axis =1)\n",
    "    return feature\n",
    "\n",
    "def get_features_from_dwt2(data,wavelet='db7',level=5):\n",
    "    coes = pywt.wavedec(data,wavelet=wavelet,mode=0,level=level,axis=1)\n",
    "    #columns = pd.Index(['LEFT_TA', 'LEFT_TS', 'LEFT_BF', 'LEFT_RF',\n",
    "    #   'RIGHT_TA', 'RIGHT_TS', 'RIGHT_BF', 'RIGHT_RF'])\n",
    "    columns = pd.Index(['LEFT_TA', 'LEFT_TS','RIGHT_TA', 'RIGHT_TS'])\n",
    "    feature = pd.DataFrame()\n",
    "    for i in range(len(coes)):\n",
    "        MAV = pd.DataFrame(dp.compute_MAV(coes[i]),columns=columns+'_MAV%d'%i)\n",
    "        RMS = pd.DataFrame(dp.compute_RMS(coes[i]),columns=columns+'_RMS%d'%i)\n",
    "        #WL = pd.DataFrame(dp.compute_WL(coes[i]),columns=columns+'_WL%d'%i)\n",
    "        ZC = pd.DataFrame(dp.compute_ZC(coes[i],1e-6),columns=columns+'_ZC%d'%i)\n",
    "        ku = pd.DataFrame(dp.compute_ku(coes[i]),columns=columns+'_ku%d'%i)\n",
    "        Mean = pd.DataFrame(np.mean(coes[i],axis=1),columns=columns+'_Mean%d'%i)\n",
    "        STD = pd.DataFrame(np.std(coes[i],axis=1),columns=columns+'_STD%d'%i)\n",
    "        #EN = pd.DataFrame(entropy(coes[i],axis=1),columns=columns+'_EN%d'%i)\n",
    "        #SSC = pd.DataFrame(dp.compute_SSC(coes[i],threshold_SSC),columns=columns+'_SSC%d'%i)\n",
    "        #WAMP = pd.DataFrame(dp.compute_WAMP(coes[i],threshold_WAMP),columns=columns+'_WAMP%d'%i)\n",
    "        skew = pd.DataFrame(dp.compute_Skewness(coes[i]),columns=columns+'_skew%d'%i)\n",
    "        #Acti = pd.DataFrame(dp.compute_Acti(coes[i]),columns=columns+'_Acti%d'%i)\n",
    "        #AR = pd.DataFrame(dp.compute_AR(coes[i]),columns=columns+'_AR%d'%i)\n",
    "        #AR = dp.compute_AR_pd(coes[i])\n",
    "        #HIST = dp.compute_HIST_pd(coes[i],bins=bins,ranges=ranges)\n",
    "        #FHIST = dp.compute_FHIST_pd(coes[i],bins=fbins,ranges=franges,threshold=threshold_F)\n",
    "        #MF = dp.compute_MaxFreq_pd(coes[i],num=num)\n",
    "        #MDF = pd.DataFrame(dp.compute_MDF(coes[i]),columns=columns+'_MDF%d'%i)\n",
    "        #MNF = pd.DataFrame(dp.compute_MNF(coes[i]),columns=columns+'_MNF%d'%i)\n",
    "        feature = pd.concat([feature,MAV,RMS,ZC,ku,Mean,STD,skew],axis =1)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file name of data with various Labels\n",
    "df = pd.read_csv('./useful_data_label.csv',index_col=0) \n",
    "# read file name of data with only label 0\n",
    "df2 = pd.read_csv('./unuseful_data_label.csv',index_col=0)\n",
    "# read some of the data with only label 0\n",
    "df3 = pd.read_csv('./data/file_name.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ctypes\n",
    "player = ctypes.windll.kernel32\n",
    "player.Beep(1000,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/174: G06_FoG_trial_1_emg.csv\n",
      "4/174: G06_FoG_trial_2_emg.csv\n",
      "5/174: G06_FoG_trial_3_emg.csv\n",
      "6/174: G07_Freezing_Trial1_trial_1_emg.csv\n",
      "X2\n",
      "X2\n",
      "X2\n",
      "X2\n",
      "7/174: G08_FoG_1_trial_1_emg.csv\n",
      "8/174: G08_FoG_2_trial_1_emg.csv\n",
      "9/174: G11_FoG_trial_1_emg.csv\n",
      "10/174: G11_FoG_trial_2_emg.csv\n",
      "11/174: P379_M050_2_OFF_A_FoG_trial_1_emg.csv\n",
      "12/174: P379_M050_2_OFF_A_FoG_trial_2_emg.csv\n",
      "13/174: P379_M050_2_OFF_A_FoG_trial_3_emg.csv\n",
      "14/174: P379_M050_2_OFF_B_FoG_trial_1_emg.csv\n",
      "15/174: P379_M050_2_OFF_B_FoG_trial_2_emg.csv\n",
      "16/174: P379_M050_2_OFF_B_FoG_trial_3_emg.csv\n",
      "17/174: P551_M050_2_A_FoG_trial_1_emg.csv\n",
      "18/174: P551_M050_2_B_FoG_trial_1_emg.csv\n",
      "19/174: P551_M050_2_B_FoG_trial_2_emg.csv\n",
      "20/174: P812_M050_2_B_FoG_trial_1_emg.csv\n",
      "21/174: P812_M050_2_B_FoG_trial_2_emg.csv\n",
      "22/174: 正常/G02_Walking_trial_1_emg.csv\n",
      "23/174: 正常/G03_Walking_trial_1_emg.csv\n",
      "24/174: 正常/G03_Walking_trial_2_emg.csv\n",
      "25/174: 正常/G05_Walking_struct_fixed_trial_1_emg.csv\n",
      "26/174: 正常/G05_Walking_struct_fixed_trial_2_emg.csv\n",
      "27/174: 正常/G05_Walking_struct_fixed_trial_3_emg.csv\n",
      "28/174: 正常/G09_FoG_trial_1_emg.csv\n",
      "29/174: 正常/G09_FoG_trial_2_emg.csv\n",
      "30/174: 正常/G09_FoG_trial_3_emg.csv\n",
      "31/174: 正常/G09_Walking_trial_2_emg.csv\n",
      "32/174: 正常/G09_Walking_trial_4_emg.csv\n",
      "33/174: 正常/G09_Walking_trial_6_emg.csv\n",
      "34/174: 正常/G11_Walking_trial_2_emg.csv\n",
      "35/174: 正常/G11_Walking_trial_4_emg.csv\n",
      "36/174: 正常/P231_M050_A_Walking_trial_2_emg.csv\n",
      "37/174: 正常/P231_M050_A_Walking_trial_4_emg.csv\n",
      "38/174: 正常/P231_M050_A_Walking_trial_6_emg.csv\n",
      "39/174: 正常/P231_M050_B_Walking_trial_2_emg.csv\n",
      "40/174: 正常/P231_M050_B_Walking_trial_4_emg.csv\n",
      "41/174: 正常/P231_M050_B_Walking_trial_6_emg.csv\n",
      "42/174: 正常/P231_M100_2_A_FoG_trial_3_emg.csv\n",
      "43/174: 正常/P231_M100_2_A_Walking_trial_4_emg.csv\n",
      "44/174: 正常/P231_M100_2_A_Walking_trial_6_emg.csv\n",
      "45/174: 正常/P231_M100_ON_A_Walking_trial_2_emg.csv\n",
      "46/174: 正常/P231_M100_ON_A_Walking_trial_4_emg.csv\n",
      "47/174: 正常/P231_M100_ON_A_Walking_trial_6_emg.csv\n",
      "48/174: 正常/P231_Msham_A_Walking_trial_2_emg.csv\n",
      "49/174: 正常/P231_Msham_A_Walking_trial_6_emg.csv\n",
      "50/174: 正常/P231_Msham_B_Walking_trial_2_emg.csv\n",
      "51/174: 正常/P351_M050_2_A_FoG_trial_1_emg.csv\n",
      "52/174: 正常/P351_M050_2_A_FoG_trial_2_emg.csv\n",
      "53/174: 正常/P351_M050_2_A_FoG_trial_3_emg.csv\n",
      "54/174: 正常/P351_M050_2_A_Walking_trial_2_emg.csv\n",
      "55/174: 正常/P351_M050_2_A_Walking_trial_4_emg.csv\n",
      "56/174: 正常/P351_M050_2_A_Walking_trial_6_emg.csv\n",
      "57/174: 正常/P351_M050_2_B_FoG_trial_1_emg.csv\n",
      "58/174: 正常/P351_M050_2_B_FoG_trial_2_emg.csv\n",
      "59/174: 正常/P351_M050_2_B_FoG_trial_3_emg.csv\n",
      "60/174: 正常/P351_M050_2_B_Walking_trial_2_emg.csv\n",
      "61/174: 正常/P351_M050_2_B_Walking_trial_4_emg.csv\n",
      "62/174: 正常/P351_M050_2_B_Walking_trial_6_emg.csv\n",
      "63/174: 正常/P351_M050_A_FoG_trial_1_emg.csv\n",
      "64/174: 正常/P351_M050_A_FoG_trial_2_emg.csv\n",
      "65/174: 正常/P351_M050_A_FoG_trial_3_emg.csv\n",
      "66/174: 正常/P351_M050_A_Walking_trial_2_emg.csv\n",
      "67/174: 正常/P351_M050_A_Walking_trial_4_emg.csv\n",
      "68/174: 正常/P351_M050_B_FoG_trial_1_emg.csv\n",
      "69/174: 正常/P351_M050_B_FoG_trial_2_emg.csv\n",
      "70/174: 正常/P351_M050_B_FoG_trial_3_emg.csv\n",
      "71/174: 正常/P351_M050_B_Walking_trial_2_emg.csv\n",
      "72/174: 正常/P351_M050_B_Walking_trial_4_emg.csv\n",
      "73/174: 正常/P351_M050_B_Walking_trial_6_emg.csv\n",
      "74/174: 正常/P351_Msham_A_FoG_trial_1_emg.csv\n",
      "75/174: 正常/P351_Msham_A_FoG_trial_2_emg.csv\n",
      "76/174: 正常/P351_Msham_A_FoG_trial_3_emg.csv\n",
      "77/174: 正常/P351_Msham_A_Walking_trial_2_emg.csv\n",
      "78/174: 正常/P351_Msham_A_Walking_trial_4_emg.csv\n",
      "79/174: 正常/P351_Msham_A_Walking_trial_6_emg.csv\n",
      "80/174: 正常/P351_Msham_B_FoG_trial_1_emg.csv\n",
      "81/174: 正常/P351_Msham_B_FoG_trial_2_emg.csv\n",
      "82/174: 正常/P351_Msham_B_FoG_trial_3_emg.csv\n",
      "83/174: 正常/P351_Msham_B_Walking_trial_2_emg.csv\n",
      "84/174: 正常/P351_Msham_B_Walking_trial_4_emg.csv\n",
      "85/174: 正常/P351_Msham_B_Walking_trial_6_emg.csv\n",
      "86/174: 正常/P379_M050_A_Walking_trial_2_emg.csv\n",
      "87/174: 正常/P379_M050_A_Walking_trial_3_emg.csv\n",
      "88/174: 正常/P379_M050_B_Walking_trial_2_emg.csv\n",
      "89/174: 正常/P379_Msham_B_Walking_trial_6_emg.csv\n",
      "90/174: 正常/P533_M050_A_Walking_trial_1_emg.csv\n",
      "91/174: 正常/P533_M050_A_Walking_trial_2_emg.csv\n",
      "92/174: 正常/P533_M050_B_Walking_trial_2_emg.csv\n",
      "93/174: 正常/P533_M050_B_Walking_trial_3_emg.csv\n",
      "94/174: 正常/P533_M100_A_Walking_trial_2_emg.csv\n",
      "95/174: 正常/P533_M100_B_Walking_trial_4_emg.csv\n",
      "96/174: 正常/P551_M50_B_Walking_trial_6_emg.csv\n",
      "97/174: 正常/P623_M050_2_A_Walking_trial_2_emg.csv\n",
      "98/174: 正常/P623_M050_2_A_Walking_trial_4_emg.csv\n",
      "99/174: 正常/P623_M050_2_A_Walking_trial_6_emg.csv\n",
      "100/174: 正常/P623_M050_2_B_Walking_trial_2_emg.csv\n",
      "101/174: 正常/P623_M050_2_B_Walking_trial_6_emg.csv\n",
      "102/174: 正常/P623_M050_A_Walking_trial_4_emg.csv\n",
      "103/174: 正常/P623_M100_A_Walking_trial_4_emg.csv\n",
      "104/174: 正常/P623_M100_B_Walking_trial_4_emg.csv\n",
      "105/174: 正常/P623_Msham_A_Walking_trial_4_emg.csv\n",
      "106/174: 正常/P623_Msham_A_Walking_trial_6_emg.csv\n",
      "107/174: 正常/P623_Msham_B_Walking_trial_2_emg.csv\n",
      "108/174: 正常/P623_Msham_B_Walking_trial_4_emg.csv\n",
      "109/174: 正常/P645_M050_A_Walking_trial_2_emg.csv\n",
      "110/174: 正常/P645_M050_A_Walking_trial_3_emg.csv\n",
      "111/174: 正常/P645_M050_B_Walking_trial_2_emg.csv\n",
      "112/174: 正常/P645_M050_B_Walking_trial_3_emg.csv\n",
      "113/174: 正常/P812_M050_2_A_FoG_trial_1_emg.csv\n",
      "114/174: 正常/P812_M050_2_A_FoG_trial_3_emg.csv\n",
      "115/174: 正常/P812_M050_2_A_Walking_trial_2_emg.csv\n",
      "116/174: 正常/P812_M050_2_A_Walking_trial_3_emg.csv\n",
      "117/174: 正常/P812_M050_2_B_Walking_1_trial_4_emg.csv\n",
      "118/174: 正常/P812_M050_A_FoG_trial_1_emg.csv\n",
      "119/174: 正常/P812_M050_A_FoG_trial_2_emg.csv\n",
      "120/174: 正常/P812_M050_A_FoG_trial_3_emg.csv\n",
      "121/174: 正常/P812_M050_A_Walking_trial_1_emg.csv\n",
      "122/174: 正常/P812_M050_A_Walking_trial_2_emg.csv\n",
      "123/174: 正常/P812_M050_B_FoG_trial_1_emg.csv\n",
      "124/174: 正常/P812_M050_B_FoG_trial_2_emg.csv\n",
      "125/174: 正常/P812_M050_B_FoG_trial_3_emg.csv\n",
      "126/174: 正常/P812_M050_B_Walking_trial_1_emg.csv\n",
      "127/174: 正常/P812_M050_B_Walking_trial_2_emg.csv\n",
      "128/174: 正常/P812_M100_A_FoG_trial_1_emg.csv\n",
      "129/174: 正常/P812_M100_A_Walking_trial_3_emg.csv\n",
      "130/174: 正常/P812_M100_B_FoG_trial_1_emg.csv\n",
      "131/174: 正常/P812_M100_B_FoG_trial_3_emg.csv\n",
      "132/174: 正常/P812_M100_B_Walking2_trial_1_emg.csv\n",
      "133/174: 正常/P812_M100_B_Walking2_trial_2_emg.csv\n",
      "134/174: 正常/P876_M100_B_FoG_trial_1_emg.csv\n",
      "135/174: 正常/P876_M100_B_FoG_trial_2_emg.csv\n",
      "136/174: 正常/P876_M100_B_FoG_trial_3_emg.csv\n",
      "137/174: 正常/P876_M100_B_Walking_trial_4_emg.csv\n",
      "138/174: 正常/P876_M100_B_Walking_trial_6_emg.csv\n",
      "139/174: 正常/P940_M050_2_A_FoG_trial_3_emg.csv\n",
      "140/174: 正常/P940_M050_2_A_FoG_trial_4_emg.csv\n",
      "141/174: 正常/P940_M050_2_A_Walking_trial_2_emg.csv\n",
      "142/174: 正常/P940_M050_2_B_FoG_trial_1_emg.csv\n",
      "143/174: 正常/P940_M050_2_B_Walking_trial_2_emg.csv\n",
      "144/174: 正常/P940_M050_2_B_Walking_trial_4_emg.csv\n",
      "145/174: 正常/P940_M050_2_B_Walking_trial_6_emg.csv\n",
      "146/174: 正常/P940_M050_A_FoG_trial_2_emg.csv\n",
      "147/174: 正常/P940_M050_A_FoG_trial_3_emg.csv\n",
      "148/174: 正常/P940_M050_A_Walking_trial_2_emg.csv\n",
      "149/174: 正常/P940_M050_A_Walking_trial_4_emg.csv\n",
      "150/174: 正常/P940_M050_A_Walking_trial_6_emg.csv\n",
      "151/174: 正常/P940_M050_B_FoG_trial_1_emg.csv\n",
      "152/174: 正常/P940_M050_B_FoG_trial_2_emg.csv\n",
      "153/174: 正常/P940_M050_B_FoG_trial_3_emg.csv\n",
      "154/174: 正常/P940_M050_B_Walking_trial_2_emg.csv\n",
      "155/174: 正常/P940_M050_B_Walking_trial_4_emg.csv\n",
      "156/174: 正常/P940_M050_B_Walking_trial_6_emg.csv\n",
      "157/174: 正常/P940_M100_A_FoG_trial_1_emg.csv\n",
      "158/174: 正常/P940_M100_A_FoG_trial_2_emg.csv\n",
      "159/174: 正常/P940_M100_A_FoG_trial_3_emg.csv\n",
      "160/174: 正常/P940_M100_A_Walking_trial_2_emg.csv\n",
      "161/174: 正常/P940_M100_A_Walking_trial_4_emg.csv\n",
      "162/174: 正常/P940_M100_A_Walking_trial_6_emg.csv\n",
      "163/174: 正常/P940_M100_B_FoG_trial_2_emg.csv\n",
      "164/174: 正常/P940_M100_B_FoG_trial_3_emg.csv\n",
      "165/174: 正常/P940_M100_B_Walking_2_trial_2_emg.csv\n",
      "166/174: 正常/P940_M100_B_Walking_2_trial_6_emg.csv\n",
      "167/174: 正常/P940_MSham_A_FoG_trial_1_emg.csv\n",
      "168/174: 正常/P940_MSham_A_FoG_trial_3_emg.csv\n",
      "169/174: 正常/P940_MSham_A_Walking_trial_2_emg.csv\n",
      "170/174: 正常/P940_MSham_A_Walking_trial_4_emg.csv\n",
      "171/174: 正常/P940_MSham_A_Walking_trial_6_emg.csv\n",
      "172/174: 正常/P940_MSham_B_Walking_trial_2_emg.csv\n",
      "173/174: 正常/P940_MSham_B_Walking_trial_4_emg.csv\n",
      "174/174: 正常/P940_MSham_B_Walking_trial_6_emg.csv\n",
      "Duration: 1899.594240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data and labels of df2 or df3\n",
    "sc = StandardScaler(with_mean = True)\n",
    "#sc = MinMaxScaler()\n",
    "ind = df2.iloc[1].isna()\n",
    "files = np.concatenate([np.array(df.columns),np.array('正常/'+df2.columns[ind])])\n",
    "#files = np.array(df.columns[[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]])#[[2,5,6,7,8,10,11,12,13,16,17,18,19,20]])\n",
    "N = len(files)\n",
    "#sc = StandardScaler(with_mean=False)\n",
    "width = 1024\n",
    "stride = 512\n",
    "start = time.time()\n",
    "i = 0\n",
    "X = []\n",
    "Y = []\n",
    "X2 = []\n",
    "Y2 = []\n",
    "X3 = []\n",
    "Y3 = []\n",
    "F = []\n",
    "F2 = []\n",
    "F3 = []\n",
    "for file in files:\n",
    "    i += 1\n",
    "    if file.find('G04')==0:\n",
    "        continue\n",
    "    emg_data = pd.read_csv('./data/'+file)\n",
    "    #emg_data.iloc[:,3:] = \n",
    "    emg_data = emg_data.dropna().reset_index(drop=True)\n",
    "    #emg_data.iloc[:,3:]=normalize(emg_data.iloc[:,3:],axis=0)\n",
    "    #emg_data.iloc[:,3:] = sc.fit_transform(emg_data.iloc[:,3:])\n",
    "    #for j in (0,1,4,5):\n",
    "        #ind = abs(zscore(emg_data.iloc[:,j+3]))>10\n",
    "        #emg_data=emg_data.loc[~ind,:]\n",
    "        #ind_p = zscore(emg_data.iloc[:,j+3])>10\n",
    "        #ind_n = zscore(emg_data.iloc[:,j+3])<-10\n",
    "        #emg_data.loc[ind_p,emg_data.columns[3+j]] = emg_data.loc[~ind_p,emg_data.columns[3+j]].max()\n",
    "        #emg_data.loc[ind_n,emg_data.columns[3+j]] = emg_data.loc[~ind_n,emg_data.columns[3+j]].min()\n",
    "    fn = 300\n",
    "    wn=2*fn/1000\n",
    "    fn1 = 400\n",
    "    wn1 = 2*fn1/1000\n",
    "    #fs = 1000.0  # Sample frequency (Hz)\n",
    "    #f0 = 50  # Frequency to be removed from signal (Hz)\n",
    "    #Q = 100.0  # Quality factor\n",
    "    # Design notch filter\n",
    "    #b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "    #b, a = signal.butter(4, [wn,wn1], 'bandpass')\n",
    "    b, a = signal.butter(4, [wn], 'lowpass')\n",
    "    #b, a = signal.butter(4, [wn], 'highpass')\n",
    "    #for j in ['LEFT_TA','LEFT_TS','LEFT_BF','LEFT_RF','RIGHT_TA','RIGHT_TS','RIGHT_BF','RIGHT_RF']:\n",
    "        #emg_data.loc[:,j] = signal.filtfilt(b, a, emg_data.loc[:,j])\n",
    "        #emg_data.loc[:,j] = signal.filtfilt(b1, a1, emg_data.loc[:,j])\n",
    "    \"\"\"if file==df.columns[5]:\n",
    "        print(file)\n",
    "        fs = 1000.0  # Sample frequency (Hz)\n",
    "        f0 = 72  # Frequency to be removed from signal (Hz)\n",
    "        Q = 50.0  # Quality factor\n",
    "        # Design notch filter\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TA'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TA'])\n",
    "        f0 = 75  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TS'])\n",
    "        f0 = 13.2  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'RIGHT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'RIGHT_TS'])\n",
    "    if file==df.columns[6]:\n",
    "        print(file)\n",
    "        fs = 1000.0  # Sample frequency (Hz)\n",
    "        f0 = 40  # Frequency to be removed from signal (Hz)\n",
    "        Q = 100.0  # Quality factor\n",
    "        # Design notch filter\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TA'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TA'])\n",
    "        f0 = 26.5  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TS'])\n",
    "        f0 = 13.2  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TS'])\n",
    "        f0 = 48  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        #emg_data.loc[:,'LEFT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TS'])\n",
    "        f0 = 50  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'RIGHT_TA'] = signal.filtfilt(b1, a1, emg_data.loc[:,'RIGHT_TA'])\n",
    "    if file==df.columns[7]:\n",
    "        print(file)\n",
    "        fs = 1000.0  # Sample frequency (Hz)\n",
    "        f0 = 13.2  # Frequency to be removed from signal (Hz)\n",
    "        Q = 50.0  # Quality factor\n",
    "        # Design notch filter\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TS'])\n",
    "        emg_data.loc[:,'RIGHT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'RIGHT_TS'])\n",
    "        f0 = 26.5  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TS'])\n",
    "        f0 = 50  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TS'])\n",
    "    \"\"\"\n",
    "    #emg_data.iloc[:,3:] = sc.fit_transform(emg_data.iloc[:,3:])\n",
    "    #emg_data.iloc[:,3:]=normalize(emg_data.iloc[:,3:],axis=0)\n",
    "    x_raw,y = dp.generate_window_slide_data_time_continue(emg_data,width=width,\n",
    "                                        stride=stride,\n",
    "                                        scaler=True,\n",
    "                                        same_label=True)\n",
    "    #x=np.abs(x)\n",
    "    #x=dp.lowpass_filter(x,300)\n",
    "    #x=dp.mean_smooth(x)\n",
    "    shape = x_raw.shape\n",
    "    x = np.zeros(shape)\n",
    "    #x = x_raw\n",
    "    for n in range(shape[0]):\n",
    "        x[n,:,:] = dp.detrend(x_raw[n,:,:],50)\n",
    "        for c in range(shape[2]):\n",
    "            x[n,:,c] = signal.filtfilt(b,a,x[n,:,c])\n",
    "    \n",
    "    ind1 = []\n",
    "    ind2 = []\n",
    "    ind3 = []\n",
    "    l = len(y)\n",
    "    for j in set(y):\n",
    "        ind = np.where(y == j)[0].tolist()\n",
    "        l_t = len(ind)\n",
    "        if (j==1)&((file==df.columns[5])|(file==df.columns[17])):\n",
    "            continue\n",
    "        if file == files[6]:\n",
    "            ind3 += ind\n",
    "            print('X2')\n",
    "        else:\n",
    "            ind1 += ind[:int(l_t*0.8)]\n",
    "            ind2 += ind[int(l_t*0.8):int(l_t*1)]\n",
    "            ind3 += ind[int(l_t*1):]\n",
    "\n",
    "    l1 = len(ind1)\n",
    "    l2 = len(ind2)\n",
    "    l3 = len(ind3)\n",
    "\n",
    "    fi = [file]*len(ind1)\n",
    "    fi2 = [file]*len(ind2)\n",
    "    fi3 = [file]*len(ind3)\n",
    "    \n",
    "    X += x[ind1].tolist()\n",
    "    Y += y[ind1].tolist()\n",
    "    \n",
    "    X2 += x[ind2].tolist()\n",
    "    Y2 += y[ind2].tolist()\n",
    "    \n",
    "    X3 += x[ind3].tolist()\n",
    "    Y3 += y[ind3].tolist()\n",
    "    \n",
    "    F += fi\n",
    "    F2 += fi2\n",
    "    F3 += fi3\n",
    "    print('%d/%d: '%(i,N)+file)\n",
    "\n",
    "ind_c = [True,True,False,False,True,True,False,False]\n",
    "X = np.array(X)#[:,:,ind_c]\n",
    "Y = np.array(Y)\n",
    "X2 = np.array(X2)#[:,:,ind_c]\n",
    "Y2 = np.array(Y2)\n",
    "X3 = np.array(X3)#[:,:,ind_c]\n",
    "Y3 = np.array(Y3)\n",
    "end = time.time()\n",
    "duration = end-start\n",
    "print('Duration: %f'%(duration))\n",
    "player.Beep(1000,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = ((Y==1)|(Y==2)|(Y==6))\n",
    "ind2 = ((Y2==1)|(Y2==2)|(Y2==6))\n",
    "ind3 = ((Y3==1)|(Y3==2)|(Y3==6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "#width = 256\n",
    "threshold_WAMP = 1\n",
    "threshold_ZC = 0.00\n",
    "#threshold_ZC = np.linspace(-4,4,11)\n",
    "threshold_SSC = 0.001\n",
    "bins=3\n",
    "bound = 3\n",
    "HIST_range = (-bound,bound)\n",
    "level = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = dp.generate_feature_pd(X,threshold_WAMP=threshold_WAMP,\n",
    "                              threshold_ZC=threshold_ZC,\n",
    "                              threshold_SSC=threshold_SSC,\n",
    "                              bins=bins,ranges=HIST_range,\n",
    "                                level = level)\n",
    "feature2 = dp.generate_feature_pd(X2,threshold_WAMP=threshold_WAMP,\n",
    "                              threshold_ZC=threshold_ZC,\n",
    "                              threshold_SSC=threshold_SSC,\n",
    "                              bins=bins,ranges=HIST_range,\n",
    "                                 level = level)\n",
    "feature3 = dp.generate_feature_pd(X3,threshold_WAMP=threshold_WAMP,\n",
    "                              threshold_ZC=threshold_ZC,\n",
    "                              threshold_SSC=threshold_SSC,\n",
    "                              bins=bins,ranges=HIST_range,\n",
    "                                 level = level)\n",
    "player.Beep(1000,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = MinMaxScaler((0,1))\n",
    "scaler = StandardScaler(with_mean=True)\n",
    "feature = scaler.fit_transform(feature)\n",
    "feature2 = scaler.fit_transform(feature2)\n",
    "feature3 = scaler.fit_transform(feature3)\n",
    "# feature = normalize(feature)\n",
    "# feature2 = normalize(feature2)\n",
    "# feature3 = normalize(feature3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skb = SelectKBest(chi2, k=150)\n",
    "skb = SelectKBest(mutual_info_classif, k=50)\n",
    "feature_new=skb.fit_transform(feature, Y)\n",
    "feature_new2=skb.transform(feature2)\n",
    "feature_new3=skb.transform(feature3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vt = VarianceThreshold(threshold=0.005)\n",
    "feature_new=vt.fit_transform(feature)\n",
    "feature_new2=vt.transform(feature2)\n",
    "feature_new3=vt.transform(feature3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(estimator=LogisticRegression(max_iter=10000), n_features_to_select=200)\n",
    "feature_new = rfe.fit_transform(feature[ind1],Y[ind1])\n",
    "feature_new2 = rfe.transform(feature2[ind2])\n",
    "feature_new3 = rfe.transform(feature3[ind3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfm = SelectFromModel(GradientBoostingClassifier(),max_features=80)\n",
    "feature_new = sfm.fit_transform(feature[ind1],Y[ind1])\n",
    "feature_new2 = sfm.transform(feature2[ind2])\n",
    "feature_new3 = sfm.transform(feature3[ind3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=200,copy=True)\n",
    "feature_new = pca.fit_transform(feature)\n",
    "feature_new2 = pca.transform(feature2)\n",
    "feature_new3 = pca.transform(feature3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(n_components=2)\n",
    "feature_new = lda.fit_transform(feature_new,Y[ind1])\n",
    "feature_new2 = lda.transform(feature_new2)\n",
    "feature_new3 = lda.transform(feature_new3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = get_features_from_dwt2(X[:,:,[0,1,4,5]],level=5)\n",
    "feature2 = get_features_from_dwt2(X2[:,:,[0,1,4,5]],level=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6611,)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_processing' from 'E:\\\\Document\\\\jupyter\\\\Master Thesis\\\\data_processing.py'>"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp\n",
    "imp.reload(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Data_rest.File=='P812_M050_2_B_FoG_trial_1_emg.csv').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 256\n",
    "threshold_WAMP = 30\n",
    "threshold_ZC = 0\n",
    "threshold_SSC = 1\n",
    "bins=9\n",
    "bound = 70\n",
    "HIST_range = (-bound,bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './processed data/data_set_after_window_W128_S64_sameLabel_rect.hdf5'\n",
    "with h5py.File(path,'r') as f:\n",
    "    x = f['data'][...]\n",
    "    y = f['label2'][...]\n",
    "#feature = dp.generate_feature(x,threshold_WAMP=threshold_WAMP,\n",
    "#                              threshold_ZC=threshold_ZC,\n",
    "#                              threshold_SSC=threshold_SSC,\n",
    "#                              bins=bins,ranges=HIST_range)\n",
    "#feature2 = dp.generate_feature(x2)\n",
    "player.Beep(1000,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./processed data/nfeatures_W256_S64_WAMP30.hdf5','r') as f:\n",
    "    feature = f['features'][...]\n",
    "    y = f['labels'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature2,y2 = dp.pipeline_feature(path2,width=256,stride=64,\n",
    "                                  scaler=False,\n",
    "                                  threshold_WAMP=threshold_WAMP,\n",
    "                                  threshold_ZC=threshold_ZC,\n",
    "                                  threshold_SSC=threshold_SSC,\n",
    "                                  bins=bins,ranges=HIST_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./processed data/nfeatures_rest_W256_S64.hdf5','r') as f:\n",
    "    feature2 = f['features'][...]\n",
    "    y2 = f['labels'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler(with_mean=True)\n",
    "feature_sc = sc.fit_transform(feature)\n",
    "feature2_sc = sc.transform(feature2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,Model,callbacks,regularizers,models\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from sklearn.metrics import precision_recall_curve,confusion_matrix,accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder,normalize,MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE,BorderlineSMOTE,ADASYN,SVMSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,feature,y,binary=True,file=None):\n",
    "    if binary:\n",
    "        ind = ((y==0)|(y==1)|(y==2)|(y==6))\n",
    "        #ind1 = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "        y_01 = y[ind].copy()\n",
    "        ind1 = ((y_01==1)|(y_01==2)|(y_01==6))\n",
    "        y_01[ind1] = 1\n",
    "        oc = OneHotEncoder()\n",
    "        y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "        cw = None#{0:1,1:5}\n",
    "    else:\n",
    "        ind = ((y==1)|(y==2)|(y==6))\n",
    "        #ind = ((y==1)|(y==6))\n",
    "        y_01 = y[ind].copy()\n",
    "        oc = OneHotEncoder()\n",
    "        y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "        cw = None#{0:5,1:1,2:1}#{0:2,1:1,2:10,3:2}\n",
    "    x_full,x_test,y_full,y_test = train_test_split(np.array(feature)[ind,:],y_01,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=123,\n",
    "                                                   shuffle=True)\n",
    "    x_train,x_valid,y_train,y_valid = train_test_split(x_full,y_full,\n",
    "                                                       test_size=0.25,\n",
    "                                                       random_state=555,\n",
    "                                                       shuffle=True)\n",
    "    \n",
    "    #sm = BorderlineSMOTE(random_state=50,kind='borderline-2')\n",
    "    #sm = SMOTE(random_state=50)\n",
    "    #print(y_full.shape)\n",
    "    #x_full,y_full = sm.fit_resample(x_full,y_full)\n",
    "    #print(y_full_n.shape)\n",
    "    sc = StandardScaler(with_mean=True)\n",
    "    #sc = MinMaxScaler()\n",
    "    x_train = sc.fit_transform(x_full)\n",
    "    pca = PCA(n_components=100)\n",
    "    #x_train = pca.fit_transform(x_train)\n",
    "    #x_valid = sc.transform(x_valid)\n",
    "    x_test = sc.transform(x_test)\n",
    "    #x_test = pca.transform(x_test)\n",
    "    #x_train = x_full\n",
    "    \n",
    "    early_stopping = callbacks.EarlyStopping(patience = 20,\n",
    "                                             monitor = 'val_loss', \n",
    "                                             restore_best_weights=True)\n",
    "    history = model.fit(x_train,y_full,validation_data=(x_test,y_test),\n",
    "                        epochs=300,batch_size=32,class_weight=cw,\n",
    "                        callbacks=[early_stopping],\n",
    "                        shuffle=True)\n",
    "    \n",
    "    if binary:\n",
    "        y_pred_t=model.predict(x_test)\n",
    "        test = metrics.accuracy_score(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1))\n",
    "        #test = metrics.accuracy_score(y_test,y_pred_t>0.5)\n",
    "        \n",
    "        #y_pred_v=model.predict(x_valid)\n",
    "        #valid = metrics.accuracy_score(y_valid,np.argmax(y_pred_v,axis=1))\n",
    "        y_pred_ta=model.predict(x_train)\n",
    "        train = metrics.accuracy_score(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1))\n",
    "        #train = metrics.accuracy_score(y_full,y_pred_ta>0.5)\n",
    "        \n",
    "        print('train: \\n',metrics.confusion_matrix(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1)))\n",
    "        #print('valid: \\n',metrics.confusion_matrix(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1)))\n",
    "        print('test: \\n',metrics.confusion_matrix(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1)))\n",
    "        \n",
    "        #print('train: \\n',metrics.confusion_matrix(y_full,y_pred_ta>0.5))\n",
    "        #print('test: \\n',metrics.confusion_matrix(y_test,y_pred_t>0.5))\n",
    "\n",
    "    else:\n",
    "        y_pred_t=model.predict(x_test)\n",
    "        test = metrics.accuracy_score(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1))\n",
    "        #y_pred_v=model.predict(x_valid)\n",
    "        #valid = metrics.accuracy_score(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1))\n",
    "        y_pred_ta=model.predict(x_train)\n",
    "        train = metrics.accuracy_score(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1))        \n",
    "        print('train: \\n',metrics.confusion_matrix(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1)))\n",
    "        #print('valid: \\n',metrics.confusion_matrix(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1)))\n",
    "        print('test: \\n',metrics.confusion_matrix(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1)))\n",
    "    print('test:%f'%test)\n",
    "    #print('valid:%f'%valid)\n",
    "    print('train:%f'%train)\n",
    "    if file != None:\n",
    "        model.save_model(file)\n",
    "    return train,test,sc,pca\n",
    "\n",
    "def test_model(model,feature,y,sc,pca,binary=True):\n",
    "    if binary:\n",
    "        ind = ((y==0)|(y==1)|(y==2)|(y==3)|(y==4)|(y==6))\n",
    "        #ind1 = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "        y_01 = y[ind].copy()\n",
    "        ind1 = ((y_01==1)|(y_01==2)|(y_01==6))\n",
    "        y_01[ind1] = 1\n",
    "        oc = OneHotEncoder()\n",
    "        y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "        cw = None#{0:1,1:5}\n",
    "    else:\n",
    "        ind = ((y==1)|(y==2)|(y==6))\n",
    "        #ind = ((y==1)|(y==6))\n",
    "        y_01 = y[ind].copy()\n",
    "        oc = OneHotEncoder()\n",
    "        y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "        cw = None#{0:5,1:1,2:1}#{0:2,1:1,2:10,3:2}\n",
    "\n",
    "    #print(y_01)\n",
    "    feature=sc.transform(feature[ind])\n",
    "    #feature = feature[ind]\n",
    "    #feature=pca.transform(feature)\n",
    "    y_pred=model.predict(feature)\n",
    "    test = metrics.accuracy_score(np.argmax(y_01,axis=1),np.argmax(y_pred,axis=1))\n",
    "    #test = metrics.accuracy_score(y_01,y_pred>0.5)\n",
    "    \n",
    "    print('acc:%f'%test)\n",
    "    print(metrics.confusion_matrix(np.argmax(y_01,axis=1),np.argmax(y_pred,axis=1)))\n",
    "    #print(metrics.confusion_matrix(y_01,y_pred>0.5))\n",
    "    return test\n",
    "\n",
    "def sparse_cost_sensitive_loss (y_true,y_pred):\n",
    "    #cost_matrix = tf.constant([[0,1.,1,1.],\n",
    "    #              [2,0,5,5],\n",
    "    #              [1,1,0,1],\n",
    "    #              [1.,2.,1,0]])\n",
    "    cost_matrix = tf.constant([[0,2.,2],\n",
    "                  [1,0,1],\n",
    "                  [1.0,1.,0]])\n",
    "    #cost_matrix = tf.constant([[0,1.],\n",
    "    #              [5.,0]])\n",
    "    batch_cost_matrix = tf.nn.embedding_lookup(cost_matrix, tf.argmax(y_true,axis=1))\n",
    "    eps = 1e-6\n",
    "    probability = tf.clip_by_value(y_pred, eps, 1-eps)\n",
    "    cost_values = tf.math.log(1-probability)*batch_cost_matrix\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(cost_values, axis=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_no_shuffle(model,feature,y,binary=True,file=None):\n",
    "    if binary:\n",
    "        ind1 = ((y[0]==0)|(y[0]==1)|(y[0]==2)|(y[0]==3)|(y[0]==4)|(y[0]==6))\n",
    "        ind2 = ((y[1]==0)|(y[1]==1)|(y[1]==2)|(y[1]==3)|(y[1]==4)|(y[1]==6))\n",
    "        ind3 = ((y[2]==0)|(y[2]==1)|(y[2]==2)|(y[2]==3)|(y[2]==4)|(y[2]==6))\n",
    "        ind01 = ((y[0]==4)|(y[0]==1)|(y[0]==2)|(y[0]==3)|(y[0]==6))\n",
    "        ind11 = ((y[1]==4)|(y[1]==1)|(y[1]==2)|(y[1]==3)|(y[1]==6))\n",
    "        ind21 = ((y[2]==4)|(y[2]==1)|(y[2]==2)|(y[2]==3)|(y[2]==6))\n",
    "        \n",
    "        y_01 = y[0][ind1].copy()\n",
    "        y_02 = y[1][ind2].copy()\n",
    "        y_03 = y[2][ind3].copy()\n",
    "\n",
    "        #ind1 = ((y_01==1)|(y_01==2)|(y_01==6))\n",
    "        y_01[ind01] = 1\n",
    "        y_02[ind11] = 1\n",
    "        y_03[ind21] = 1 \n",
    "        oc = OneHotEncoder()\n",
    "        y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "        cw = None#{0:1,1:10}\n",
    "    else:\n",
    "        ind1 = ((y[0]==2)|(y[0]==6))\n",
    "        ind2 = ((y[1]==2)|(y[1]==6))\n",
    "        ind3 = ((y[2]==2)|(y[2]==6))\n",
    "        #ind = ((y==2)|(y==6))\n",
    "        y_01 = y[0][ind1].copy()\n",
    "        y_02 = y[1][ind2].copy()\n",
    "        y_03 = y[2][ind3].copy()\n",
    "        oc = OneHotEncoder()\n",
    "        y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "        cw = None#{0:4,1:1,2:1}#{0:2,1:1,2:10,3:2}\n",
    "\n",
    "\n",
    "    x_train = feature[0][ind1]\n",
    "    y_train = y_01\n",
    "    x_valid = feature[1][ind2]\n",
    "    y_valid = oc.transform(y_02[:,np.newaxis]).toarray()\n",
    "    x_test = feature[2][ind3]\n",
    "    y_test = oc.transform(y_03[:,np.newaxis]).toarray()\n",
    "    \n",
    "    early_stopping = callbacks.EarlyStopping(patience = 10,\n",
    "                                             monitor = 'val_loss', \n",
    "                                             #baseline = 0.9,\n",
    "                                             restore_best_weights=True)\n",
    "    history = model.fit(x_train,y_train,validation_data=(x_valid,y_valid),\n",
    "                        epochs=300,batch_size=32,class_weight=cw,\n",
    "                        callbacks=[early_stopping],\n",
    "                        shuffle=True)\n",
    "    \n",
    "    if binary:\n",
    "        y_pred_t=model.predict(x_test)\n",
    "        test = accuracy_score(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1))\n",
    "        #test = accuracy_score(np.argmax(y_test,axis=1),y_test,y_pred_t>0.5)\n",
    "        \n",
    "        y_pred_v=model.predict(x_valid)\n",
    "        valid = accuracy_score(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1))\n",
    "        y_pred_ta=model.predict(x_train)\n",
    "        train = accuracy_score(np.argmax(y_train,axis=1),np.argmax(y_pred_ta,axis=1))\n",
    "        #train = accuracy_score(y_full,y_pred_ta>0.5)\n",
    "        \n",
    "        print('train: \\n',confusion_matrix(np.argmax(y_train,axis=1),np.argmax(y_pred_ta,axis=1)))\n",
    "        print('valid: \\n',confusion_matrix(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1)))\n",
    "        print('test: \\n',confusion_matrix(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1)))\n",
    "        \n",
    "        #print('train: \\n',metrics.confusion_matrix(y_full,y_pred_ta>0.5))\n",
    "        #print('test: \\n',metrics.confusion_matrix(y_test,y_pred_t>0.5))\n",
    "        print('test:%f'%test)\n",
    "        print('valid:%f'%valid)\n",
    "        print('train:%f'%train)\n",
    "\n",
    "    else:\n",
    "        y_pred_t=model.predict(x_test)\n",
    "        test = accuracy_score(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1))\n",
    "        y_pred_v=model.predict(x_valid)\n",
    "        valid = accuracy_score(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1))\n",
    "        y_pred_ta=model.predict(x_train)\n",
    "        train = accuracy_score(np.argmax(y_train,axis=1),np.argmax(y_pred_ta,axis=1))        \n",
    "        print('train: \\n',confusion_matrix(np.argmax(y_train,axis=1),np.argmax(y_pred_ta,axis=1)))\n",
    "        print('valid: \\n',confusion_matrix(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1)))\n",
    "        print('test: \\n',confusion_matrix(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1)))\n",
    "        print('test:%f'%test)\n",
    "        print('valid:%f'%valid)\n",
    "        print('train:%f'%train)\n",
    "    if file != None:\n",
    "        model.save_model(file)\n",
    "    #return train,test    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = ((y==1)|(y==2)|(y==3)|(y==6))\n",
    "#ind = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "ind_f = [0,1,6,42,46,57,62]\n",
    "#y_01 = y[ind].copy()\n",
    "#y_01[y_01==1]=0\n",
    "#y_01[y_01==2]=1\n",
    "#y_01[y_01==3]=2\n",
    "#y_01[y_01==6]=3\n",
    "y_01 = y[ind].copy()\n",
    "#y_01[ind] = 1\n",
    "oh_ec = OneHotEncoder()\n",
    "y_oh = oh_ec.fit_transform(y_01[:,np.newaxis]).toarray()\n",
    "x_full,x_test,y_full,y_test = train_test_split(feature.loc[ind,:],y_01,test_size=0.2,random_state=123,shuffle=False)\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(x_full,y_full,test_size=0.2,random_state=555,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = layers.Input(shape=feature.shape[1:])#feature.shape[1:]\n",
    "l1 = layers.Dense(128,activation='elu',\n",
    "                  #kernel_initializer='lecun_normal',\n",
    "                  #kernel_regularizer = regularizers.l2(0.001),\n",
    "                 )(input_)\n",
    "#drop1 = layers.Dropout(0.2)(l1)\n",
    "bn1 = layers.BatchNormalization()(l1)\n",
    "l2 = layers.Dense(64,activation='elu',\n",
    "                  #kernel_initializer='lecun_normal',\n",
    "                  #kernel_regularizer = regularizers.l2(0.001),\n",
    "                 )(bn1)\n",
    "#drop2 = layers.Dropout(0.2)(l2)\n",
    "bn2 = layers.BatchNormalization()(l2)\n",
    "l3 = layers.Dense(32,activation='elu',\n",
    "                  #kernel_initializer='lecun_normal',\n",
    "                  #kernel_regularizer = regularizers.l2(0.001),\n",
    "                 )(bn2)\n",
    "#drop3 = layers.Dropout(0.2)(l3)\n",
    "bn3 = layers.BatchNormalization()(l3)\n",
    "l4 = layers.Dense(16,activation='elu',\n",
    "                 #kernel_regularizer = regularizers.l2(0.001),\n",
    "                 )(bn3)\n",
    "#drop4 = layers.Dropout(0.2)(l4)\n",
    "#l5 = layers.Dense(8,activation='relu')(drop4)\n",
    "#drop5 = layers.Dropout(0.5)(l5)\n",
    "output = layers.Dense(3,activation='softmax')(l4)\n",
    "model = Model(inputs=[input_],outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = regularizers.l2(0)\n",
    "\n",
    "model = models.Sequential()\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(128,#activation='elu',\n",
    "                  #kernel_initializer='lecun_normal',\n",
    "                  kernel_regularizer = reg,\n",
    "                       #use_bias=False\n",
    "                 ))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.Dense(64,#activation='elu',\n",
    "                  #kernel_initializer='lecun_normal',\n",
    "                  kernel_regularizer = reg,\n",
    "                      # use_bias=False\n",
    "                 ))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.Dense(32,#activation='elu',\n",
    "                  #kernel_initializer='lecun_normal',\n",
    "                  kernel_regularizer = reg,\n",
    "                       #use_bias=False\n",
    "                 ))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.Dense(16,#activation='elu',\n",
    "                 kernel_regularizer = reg,\n",
    "                       #use_bias=False\n",
    "                 ))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.Dense(2,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.9231 - accuracy: 0.5045 - val_loss: 0.6614 - val_accuracy: 0.5210\n",
      "Epoch 2/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.6147 - accuracy: 0.6674 - val_loss: 0.6102 - val_accuracy: 0.6218\n",
      "Epoch 3/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.5109 - accuracy: 0.7602 - val_loss: 0.5545 - val_accuracy: 0.7563\n",
      "Epoch 4/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4155 - accuracy: 0.8439 - val_loss: 0.5043 - val_accuracy: 0.8151\n",
      "Epoch 5/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.3907 - accuracy: 0.8529 - val_loss: 0.4296 - val_accuracy: 0.8403\n",
      "Epoch 6/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.3288 - accuracy: 0.8846 - val_loss: 0.3619 - val_accuracy: 0.8655\n",
      "Epoch 7/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.2816 - accuracy: 0.9276 - val_loss: 0.3152 - val_accuracy: 0.8908\n",
      "Epoch 8/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.2661 - accuracy: 0.9321 - val_loss: 0.2759 - val_accuracy: 0.8992\n",
      "Epoch 9/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.2382 - accuracy: 0.9367 - val_loss: 0.2446 - val_accuracy: 0.9160\n",
      "Epoch 10/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.2067 - accuracy: 0.9480 - val_loss: 0.2162 - val_accuracy: 0.9244\n",
      "Epoch 11/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.2199 - accuracy: 0.9253 - val_loss: 0.1939 - val_accuracy: 0.9412\n",
      "Epoch 12/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.1717 - accuracy: 0.9683 - val_loss: 0.1989 - val_accuracy: 0.9328\n",
      "Epoch 13/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.1858 - accuracy: 0.9457 - val_loss: 0.1893 - val_accuracy: 0.9328\n",
      "Epoch 14/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.1537 - accuracy: 0.9593 - val_loss: 0.1772 - val_accuracy: 0.9412\n",
      "Epoch 15/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.1493 - accuracy: 0.9661 - val_loss: 0.1957 - val_accuracy: 0.9160\n",
      "Epoch 16/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.1250 - accuracy: 0.9774 - val_loss: 0.2187 - val_accuracy: 0.8908\n",
      "Epoch 17/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.1150 - accuracy: 0.9706 - val_loss: 0.2281 - val_accuracy: 0.8908\n",
      "Epoch 18/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.1072 - accuracy: 0.9774 - val_loss: 0.2294 - val_accuracy: 0.8992\n",
      "Epoch 19/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.1280 - accuracy: 0.9615 - val_loss: 0.2520 - val_accuracy: 0.8739\n",
      "Epoch 20/300\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.1234 - accuracy: 0.9706 - val_loss: 0.2578 - val_accuracy: 0.8739\n",
      "Epoch 21/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.0830 - accuracy: 0.9910 - val_loss: 0.2615 - val_accuracy: 0.8824\n",
      "Epoch 22/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.0780 - accuracy: 0.9819 - val_loss: 0.2711 - val_accuracy: 0.8824\n",
      "Epoch 23/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.0738 - accuracy: 0.9864 - val_loss: 0.2646 - val_accuracy: 0.8908\n",
      "Epoch 24/300\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.0817 - accuracy: 0.9796 - val_loss: 0.2711 - val_accuracy: 0.8992\n",
      "train: \n",
      " [[266  12]\n",
      " [  0 164]]\n",
      "valid: \n",
      " [[75  2]\n",
      " [ 5 37]]\n",
      "test: \n",
      " [[57  0]\n",
      " [42 50]]\n",
      "test:0.718121\n",
      "valid:0.941176\n",
      "train:0.972851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model_no_shuffle(model,(feature,feature2,feature3),\n",
    "                       (np.array(Y),np.array(Y2),np.array(Y3)),\n",
    "                       False)\n",
    "player.Beep(1000,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sc = StandardScaler(with_mean=True)\n",
    "#feature_sc = sc.fit_transform(feature)\n",
    "#feature2_sc = sc.transform(feature2)\n",
    "#pca = PCA(n_components=160,copy=True)\n",
    "#feature_pca = pca.fit_transform(feature)\n",
    "#feature2_pca = pca.transform(feature2)\n",
    "train,test,sc,pca = train_model(model,feature,np.array(Y),False)\n",
    "#rest = test_model(model,feature2,np.array(y2),sc)\n",
    "#acc = [train,valid,test,rest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.476773\n",
      "[[  0  88   0]\n",
      " [  0 195   0]\n",
      " [  0 126   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4767726161369193"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model,np.array(feature2),np.array(Y2),sc,pca,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ind = ((y2==1)|(y2==2)|(y2==6))\n",
    "oc = OneHotEncoder()\n",
    "y_01 = oc.fit_transform(y2[ind,np.newaxis]).toarray()\n",
    "y_pred=model.predict(sc.transform(np.array(feature2)[ind,:]))\n",
    "ind2=np.where(np.argmax(y_pred,axis=1) != np.argmax(y_01,axis=1))\n",
    "#ind2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model/ann2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       ...,\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = np.array((y==0)|(y==1)|(y==2)|(y==6))\n",
    "#feature\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model(model,feature2_sc,np.array(y2))\n",
    "ind = ((y==1)|(y==2)|(y==6))\n",
    "y_01 = y[ind].copy()\n",
    "oc = OneHotEncoder()\n",
    "y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "pred=model.predict(feature_sc[ind,:])\n",
    "np.argmax(pred,axis=1)\n",
    "np.argmax(y_01,axis=1)\n",
    "metrics.confusion_matrix(np.argmax(y_01,axis=1),np.argmax(pred,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc_ann = pd.DataFrame(acc,columns=['dnn1'],index=['train','valid','test','rest data'])\n",
    "#acc_ann_com=pd.concat([acc_ann_com,acc_ann.T])\n",
    "#acc_ann.loc['drop_mDWT',:]=[train,valid,test,rest]\n",
    "acc_ann_com.loc['dnn2',:]=acc\n",
    "acc_ann_com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test feature from Left or Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = layers.Input(shape=feature.loc[:,ind_temp].shape[1:])\n",
    "l1 = layers.Dense(128,activation='elu')(input_)\n",
    "drop1 = layers.Dropout(0.2)(l1)\n",
    "l2 = layers.Dense(64,activation='elu')(drop1)\n",
    "drop2 = layers.Dropout(0.2)(l2)\n",
    "#l3 = layers.Dense(32,activation='elu')(drop2)\n",
    "#drop3 = layers.Dropout(0.2)(l3)\n",
    "#l4 = layers.Dense(16,activation='selu')(l3)\n",
    "#drop4 = layers.Dropout(0.2)(l4)\n",
    "output = layers.Dense(1,activation='sigmoid')(drop2)\n",
    "model = Model(inputs=[input_],outputs=[output])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23125 samples, validate on 5782 samples\n",
      "Epoch 1/200\n",
      "23125/23125 [==============================] - 4s 167us/sample - loss: 1.9657 - accuracy: 0.5308 - val_loss: 1.3537 - val_accuracy: 0.6404\n",
      "Epoch 2/200\n",
      "23125/23125 [==============================] - 3s 109us/sample - loss: 1.3913 - accuracy: 0.6479 - val_loss: 1.1555 - val_accuracy: 0.7008\n",
      "Epoch 3/200\n",
      "23125/23125 [==============================] - 3s 110us/sample - loss: 1.2045 - accuracy: 0.7052 - val_loss: 1.0439 - val_accuracy: 0.7771\n",
      "Epoch 4/200\n",
      "23125/23125 [==============================] - 2s 105us/sample - loss: 1.1076 - accuracy: 0.7394 - val_loss: 0.9467 - val_accuracy: 0.7608\n",
      "Epoch 5/200\n",
      "23125/23125 [==============================] - 2s 86us/sample - loss: 1.0526 - accuracy: 0.7501 - val_loss: 0.9212 - val_accuracy: 0.7508\n",
      "Epoch 6/200\n",
      "23125/23125 [==============================] - 2s 93us/sample - loss: 0.9990 - accuracy: 0.7677 - val_loss: 0.8937 - val_accuracy: 0.7975\n",
      "Epoch 7/200\n",
      "23125/23125 [==============================] - 2s 98us/sample - loss: 0.9561 - accuracy: 0.7806 - val_loss: 0.8888 - val_accuracy: 0.7916\n",
      "Epoch 8/200\n",
      "23125/23125 [==============================] - 2s 85us/sample - loss: 0.9064 - accuracy: 0.8004 - val_loss: 0.8675 - val_accuracy: 0.8127\n",
      "Epoch 9/200\n",
      "23125/23125 [==============================] - 2s 96us/sample - loss: 0.8361 - accuracy: 0.8109 - val_loss: 0.9959 - val_accuracy: 0.8388\n",
      "Epoch 10/200\n",
      "23125/23125 [==============================] - 2s 91us/sample - loss: 0.8394 - accuracy: 0.8178 - val_loss: 0.8477 - val_accuracy: 0.8080\n",
      "Epoch 11/200\n",
      "23125/23125 [==============================] - 2s 89us/sample - loss: 0.8182 - accuracy: 0.8169 - val_loss: 0.8487 - val_accuracy: 0.8193\n",
      "Epoch 12/200\n",
      "23125/23125 [==============================] - 2s 87us/sample - loss: 0.7716 - accuracy: 0.8304 - val_loss: 1.0611 - val_accuracy: 0.8345\n",
      "Epoch 13/200\n",
      "23125/23125 [==============================] - 2s 87us/sample - loss: 0.8293 - accuracy: 0.8183 - val_loss: 0.9155 - val_accuracy: 0.8390\n",
      "Epoch 14/200\n",
      "23125/23125 [==============================] - 2s 82us/sample - loss: 0.8102 - accuracy: 0.8220 - val_loss: 0.8263 - val_accuracy: 0.8255\n",
      "Epoch 15/200\n",
      "23125/23125 [==============================] - 2s 82us/sample - loss: 0.7460 - accuracy: 0.8396 - val_loss: 0.7475 - val_accuracy: 0.8295\n",
      "Epoch 16/200\n",
      "23125/23125 [==============================] - 3s 109us/sample - loss: 0.7118 - accuracy: 0.8471 - val_loss: 0.8684 - val_accuracy: 0.8412\n",
      "Epoch 17/200\n",
      "23125/23125 [==============================] - 3s 135us/sample - loss: 0.7334 - accuracy: 0.8410 - val_loss: 0.8593 - val_accuracy: 0.8258\n",
      "Epoch 18/200\n",
      "23125/23125 [==============================] - 3s 125us/sample - loss: 0.7021 - accuracy: 0.8490 - val_loss: 0.8953 - val_accuracy: 0.8506\n",
      "Epoch 19/200\n",
      "23125/23125 [==============================] - 3s 121us/sample - loss: 0.6847 - accuracy: 0.8514 - val_loss: 0.9428 - val_accuracy: 0.8566\n",
      "Epoch 20/200\n",
      "23125/23125 [==============================] - 3s 133us/sample - loss: 0.7248 - accuracy: 0.8508 - val_loss: 0.9025 - val_accuracy: 0.8392\n",
      "Epoch 21/200\n",
      "23125/23125 [==============================] - 3s 118us/sample - loss: 0.7017 - accuracy: 0.8458 - val_loss: 0.8396 - val_accuracy: 0.8525\n",
      "Epoch 22/200\n",
      "23125/23125 [==============================] - 3s 110us/sample - loss: 0.6808 - accuracy: 0.8589 - val_loss: 0.8343 - val_accuracy: 0.8454\n",
      "Epoch 23/200\n",
      "23125/23125 [==============================] - 3s 111us/sample - loss: 0.6569 - accuracy: 0.8586 - val_loss: 0.9713 - val_accuracy: 0.8646\n",
      "Epoch 24/200\n",
      "23125/23125 [==============================] - 2s 91us/sample - loss: 0.7113 - accuracy: 0.8530 - val_loss: 0.9063 - val_accuracy: 0.8663\n",
      "Epoch 25/200\n",
      "23125/23125 [==============================] - 2s 87us/sample - loss: 0.6886 - accuracy: 0.8540 - val_loss: 1.0114 - val_accuracy: 0.8729\n",
      "Epoch 26/200\n",
      "23125/23125 [==============================] - 2s 83us/sample - loss: 0.6437 - accuracy: 0.8703 - val_loss: 1.0360 - val_accuracy: 0.8737\n",
      "Epoch 27/200\n",
      "23125/23125 [==============================] - 2s 82us/sample - loss: 0.6396 - accuracy: 0.8684 - val_loss: 1.0674 - val_accuracy: 0.8762\n",
      "Epoch 28/200\n",
      "23125/23125 [==============================] - 2s 87us/sample - loss: 0.5848 - accuracy: 0.8806 - val_loss: 1.3498 - val_accuracy: 0.8853\n",
      "Epoch 29/200\n",
      "23125/23125 [==============================] - 2s 79us/sample - loss: 0.6586 - accuracy: 0.8602 - val_loss: 0.9367 - val_accuracy: 0.8565\n",
      "Epoch 30/200\n",
      "23125/23125 [==============================] - 2s 75us/sample - loss: 0.6776 - accuracy: 0.8599 - val_loss: 1.0857 - val_accuracy: 0.8727\n",
      "Epoch 31/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.6196 - accuracy: 0.8752 - val_loss: 1.0133 - val_accuracy: 0.8807\n",
      "Epoch 32/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.6562 - accuracy: 0.8634 - val_loss: 0.8697 - val_accuracy: 0.8708\n",
      "Epoch 33/200\n",
      "23125/23125 [==============================] - 2s 73us/sample - loss: 0.5973 - accuracy: 0.8750 - val_loss: 0.9950 - val_accuracy: 0.8753\n",
      "Epoch 34/200\n",
      "23125/23125 [==============================] - 2s 73us/sample - loss: 0.5699 - accuracy: 0.8852 - val_loss: 0.9541 - val_accuracy: 0.8786\n",
      "Epoch 35/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.5811 - accuracy: 0.8883 - val_loss: 1.2480 - val_accuracy: 0.8967\n",
      "Epoch 36/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.6094 - accuracy: 0.8775 - val_loss: 0.9992 - val_accuracy: 0.8833\n",
      "Epoch 37/200\n",
      "23125/23125 [==============================] - 2s 80us/sample - loss: 0.5736 - accuracy: 0.8909 - val_loss: 1.0015 - val_accuracy: 0.8864\n",
      "Epoch 38/200\n",
      "23125/23125 [==============================] - 2s 77us/sample - loss: 0.5681 - accuracy: 0.8930 - val_loss: 0.8801 - val_accuracy: 0.8717\n",
      "Epoch 39/200\n",
      "23125/23125 [==============================] - 2s 77us/sample - loss: 0.5511 - accuracy: 0.8872 - val_loss: 1.2918 - val_accuracy: 0.8933\n",
      "Epoch 40/200\n",
      "23125/23125 [==============================] - 2s 74us/sample - loss: 0.5846 - accuracy: 0.8878 - val_loss: 1.0802 - val_accuracy: 0.8919\n",
      "Epoch 41/200\n",
      "23125/23125 [==============================] - 2s 83us/sample - loss: 0.5913 - accuracy: 0.8886 - val_loss: 0.7979 - val_accuracy: 0.8649\n",
      "Epoch 42/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.5855 - accuracy: 0.8799 - val_loss: 0.9434 - val_accuracy: 0.8912\n",
      "Epoch 43/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.5320 - accuracy: 0.8915 - val_loss: 1.4377 - val_accuracy: 0.8942\n",
      "Epoch 44/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.5590 - accuracy: 0.8926 - val_loss: 1.2363 - val_accuracy: 0.8945\n",
      "Epoch 45/200\n",
      "23125/23125 [==============================] - 2s 85us/sample - loss: 0.5660 - accuracy: 0.8974 - val_loss: 0.9355 - val_accuracy: 0.8770\n",
      "train: \n",
      " [[13662  1921]\n",
      " [    0  7542]]\n",
      "test: \n",
      " [[3359  562]\n",
      " [  35 1826]]\n",
      "test:0.896749\n",
      "train:0.916930\n",
      "[0 0 0 ... 0 0 0]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "acc:0.248125\n",
      "[[ 430    0]\n",
      " [1303    0]]\n",
      "0.24812463935372187\n",
      "Train on 23125 samples, validate on 5782 samples\n",
      "Epoch 1/200\n",
      "23125/23125 [==============================] - 2s 83us/sample - loss: 1.6699 - accuracy: 0.5641 - val_loss: 1.1863 - val_accuracy: 0.6956\n",
      "Epoch 2/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 1.2186 - accuracy: 0.6931 - val_loss: 1.0834 - val_accuracy: 0.7371\n",
      "Epoch 3/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 1.0666 - accuracy: 0.7425 - val_loss: 1.0060 - val_accuracy: 0.7791\n",
      "Epoch 4/200\n",
      "23125/23125 [==============================] - 2s 73us/sample - loss: 1.0038 - accuracy: 0.7606 - val_loss: 1.0101 - val_accuracy: 0.7757\n",
      "Epoch 5/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 1.0121 - accuracy: 0.7549 - val_loss: 0.9530 - val_accuracy: 0.7598\n",
      "Epoch 6/200\n",
      "23125/23125 [==============================] - 2s 75us/sample - loss: 0.8796 - accuracy: 0.7876 - val_loss: 1.0161 - val_accuracy: 0.8279\n",
      "Epoch 7/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.8478 - accuracy: 0.8073 - val_loss: 0.9871 - val_accuracy: 0.8248\n",
      "Epoch 8/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.8517 - accuracy: 0.8045 - val_loss: 0.8904 - val_accuracy: 0.8210\n",
      "Epoch 9/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.8091 - accuracy: 0.8131 - val_loss: 1.0212 - val_accuracy: 0.8426\n",
      "Epoch 10/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.7750 - accuracy: 0.8260 - val_loss: 1.0699 - val_accuracy: 0.8438\n",
      "Epoch 11/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.7591 - accuracy: 0.8365 - val_loss: 0.9963 - val_accuracy: 0.8513\n",
      "Epoch 12/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.7071 - accuracy: 0.8416 - val_loss: 1.0759 - val_accuracy: 0.8464\n",
      "Epoch 13/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.6955 - accuracy: 0.8444 - val_loss: 1.0100 - val_accuracy: 0.8442\n",
      "Epoch 14/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.7017 - accuracy: 0.8478 - val_loss: 1.0484 - val_accuracy: 0.8539\n",
      "Epoch 15/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.7070 - accuracy: 0.8503 - val_loss: 0.9769 - val_accuracy: 0.8648\n",
      "Epoch 16/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.6993 - accuracy: 0.8527 - val_loss: 0.8596 - val_accuracy: 0.8388\n",
      "Epoch 17/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.6776 - accuracy: 0.8485 - val_loss: 1.0544 - val_accuracy: 0.8644\n",
      "Epoch 18/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.6821 - accuracy: 0.8598 - val_loss: 0.9780 - val_accuracy: 0.8658\n",
      "Epoch 19/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.6573 - accuracy: 0.8584 - val_loss: 1.0270 - val_accuracy: 0.8687\n",
      "Epoch 20/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.6057 - accuracy: 0.8704 - val_loss: 1.1063 - val_accuracy: 0.8692\n",
      "Epoch 21/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.6138 - accuracy: 0.8713 - val_loss: 0.9826 - val_accuracy: 0.8469\n",
      "Epoch 22/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.6390 - accuracy: 0.8652 - val_loss: 1.1678 - val_accuracy: 0.8756\n",
      "Epoch 23/200\n",
      "23125/23125 [==============================] - 2s 75us/sample - loss: 0.6374 - accuracy: 0.8704 - val_loss: 1.1649 - val_accuracy: 0.8637\n",
      "Epoch 24/200\n",
      "23125/23125 [==============================] - 2s 73us/sample - loss: 0.6625 - accuracy: 0.8615 - val_loss: 1.0139 - val_accuracy: 0.8644\n",
      "Epoch 25/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.6115 - accuracy: 0.8678 - val_loss: 0.9688 - val_accuracy: 0.8634\n",
      "Epoch 26/200\n",
      "23125/23125 [==============================] - 2s 74us/sample - loss: 0.5845 - accuracy: 0.8767 - val_loss: 1.3254 - val_accuracy: 0.8826\n",
      "Epoch 27/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.5496 - accuracy: 0.8895 - val_loss: 1.3573 - val_accuracy: 0.8796\n",
      "Epoch 28/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.6670 - accuracy: 0.8630 - val_loss: 0.9269 - val_accuracy: 0.8490\n",
      "Epoch 29/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.5767 - accuracy: 0.8770 - val_loss: 1.3259 - val_accuracy: 0.8819\n",
      "Epoch 30/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.6089 - accuracy: 0.8846 - val_loss: 1.0698 - val_accuracy: 0.8691\n",
      "Epoch 31/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.5645 - accuracy: 0.8845 - val_loss: 1.0752 - val_accuracy: 0.8822\n",
      "Epoch 32/200\n",
      "23125/23125 [==============================] - 2s 71us/sample - loss: 0.6610 - accuracy: 0.8730 - val_loss: 0.9069 - val_accuracy: 0.8675\n",
      "Epoch 33/200\n",
      "23125/23125 [==============================] - 2s 70us/sample - loss: 0.5818 - accuracy: 0.8803 - val_loss: 1.0766 - val_accuracy: 0.8833\n",
      "Epoch 34/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.5978 - accuracy: 0.8772 - val_loss: 1.0967 - val_accuracy: 0.8841\n",
      "Epoch 35/200\n",
      "23125/23125 [==============================] - 2s 83us/sample - loss: 0.5508 - accuracy: 0.8896 - val_loss: 1.0682 - val_accuracy: 0.8744\n",
      "Epoch 36/200\n",
      "23125/23125 [==============================] - 2s 81us/sample - loss: 0.5556 - accuracy: 0.8859 - val_loss: 1.1288 - val_accuracy: 0.8796\n",
      "Epoch 37/200\n",
      "23125/23125 [==============================] - 2s 79us/sample - loss: 0.5555 - accuracy: 0.8897 - val_loss: 1.0734 - val_accuracy: 0.8862\n",
      "Epoch 38/200\n",
      "23125/23125 [==============================] - 2s 73us/sample - loss: 0.5966 - accuracy: 0.8803 - val_loss: 0.9272 - val_accuracy: 0.8686\n",
      "Epoch 39/200\n",
      "23125/23125 [==============================] - 2s 75us/sample - loss: 0.5482 - accuracy: 0.8869 - val_loss: 1.3462 - val_accuracy: 0.8966\n",
      "Epoch 40/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.5505 - accuracy: 0.8925 - val_loss: 1.0365 - val_accuracy: 0.8893\n",
      "Epoch 41/200\n",
      "23125/23125 [==============================] - 2s 72us/sample - loss: 0.5973 - accuracy: 0.8802 - val_loss: 0.9774 - val_accuracy: 0.8852\n",
      "Epoch 42/200\n",
      "23125/23125 [==============================] - 2s 81us/sample - loss: 0.5678 - accuracy: 0.8853 - val_loss: 1.2457 - val_accuracy: 0.8910\n",
      "Epoch 43/200\n",
      "23125/23125 [==============================] - 2s 84us/sample - loss: 0.5707 - accuracy: 0.8997 - val_loss: 1.3619 - val_accuracy: 0.8935\n",
      "Epoch 44/200\n",
      "23125/23125 [==============================] - 2s 75us/sample - loss: 0.5471 - accuracy: 0.8922 - val_loss: 1.0956 - val_accuracy: 0.8872\n",
      "Epoch 45/200\n",
      "23125/23125 [==============================] - 2s 74us/sample - loss: 0.5946 - accuracy: 0.8762 - val_loss: 1.0329 - val_accuracy: 0.8777\n",
      "Epoch 46/200\n",
      "23125/23125 [==============================] - 2s 74us/sample - loss: 0.5659 - accuracy: 0.8837 - val_loss: 0.9916 - val_accuracy: 0.8812\n",
      "Epoch 47/200\n",
      "23125/23125 [==============================] - 2s 74us/sample - loss: 0.5310 - accuracy: 0.8933 - val_loss: 0.9934 - val_accuracy: 0.8860\n",
      "Epoch 48/200\n",
      "23125/23125 [==============================] - 2s 73us/sample - loss: 0.5431 - accuracy: 0.8881 - val_loss: 1.0694 - val_accuracy: 0.8884\n",
      "Epoch 49/200\n",
      "23125/23125 [==============================] - 2s 73us/sample - loss: 0.5159 - accuracy: 0.8993 - val_loss: 1.1309 - val_accuracy: 0.8942\n",
      "train: \n",
      " [[13597  1986]\n",
      " [    0  7542]]\n",
      "test: \n",
      " [[3361  560]\n",
      " [  38 1823]]\n",
      "test:0.896576\n",
      "train:0.914119\n",
      "[0 0 0 ... 0 0 0]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "acc:0.248125\n",
      "[[ 430    0]\n",
      " [1303    0]]\n",
      "0.24812463935372187\n"
     ]
    }
   ],
   "source": [
    "acc={}\n",
    "cols = ['LEFT','RIGHT']\n",
    "\n",
    "#sc = StandardScaler(with_mean=True)\n",
    "\n",
    "for col in cols:\n",
    "    ind_temp=feature.columns.str.contains(col)\n",
    "    #feature_sc = sc.fit_transform(feature.loc[:,ind_temp])\n",
    "    #feature2_sc = sc.transform(feature2.loc[:,ind_temp])\n",
    "    train,test,sc = train_model(model,np.array(feature.loc[:,ind_temp]),np.array(y))\n",
    "    \n",
    "    acc_rest=test_model(model,np.array(feature2.loc[:,ind_temp]),np.array(y2),sc)\n",
    "    #acc[col] = [train,valid,test,acc_rest]\n",
    "    print(acc_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_lr=pd.DataFrame(acc,index=['train','valid','test','rest data'])#.to_csv('./results/acc_lr_ann.csv')\n",
    "#acc_com=pd.concat([acc_ann_com.drop(['dnn','dnn1'],'index'),acc_lr.T])\n",
    "acc_com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test features from 2 of 8 signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test features from 2 of 8 signals\n",
    "\n",
    "acc_2f={}\n",
    "cols = [ 'LEFT_TA','LEFT_TS','LEFT_BF', 'LEFT_RF',\n",
    "        'RIGHT_TA','RIGHT_TS','RIGHT_BF', 'RIGHT_RF']\n",
    "\n",
    "\n",
    "for p in combinations(cols[:4],2):\n",
    "    ind_temp=feature.columns.str.contains(p[0])| feature.columns.str.contains(p[1])\n",
    "    #feature_sc = sc.fit_transform(feature.loc[:,ind_temp])\n",
    "    #feature2_sc = sc.transform(feature2.loc[:,ind_temp])\n",
    "    train,valid,test,sc = train_model(model,np.array(feature.loc[:,ind_temp]),np.array(y))\n",
    "    acc_rest=test_model(model,np.array(feature2.loc[:,ind_temp]),np.array(y2),sc)\n",
    "    acc_2f[p[0]+'_'+p[1]] = [train,valid,test,acc_rest]\n",
    "    print(acc_rest)\n",
    "    \n",
    "for p in combinations(cols[4:],2):\n",
    "    ind_temp=feature.columns.str.contains(p[0])| feature.columns.str.contains(p[1])\n",
    "    #feature_sc = sc.fit_transform(feature.loc[:,ind_temp])\n",
    "    #feature2_sc = sc.transform(feature2.loc[:,ind_temp])\n",
    "    train,valid,test,sc = train_model(model,np.array(feature.loc[:,ind_temp]),np.array(y))\n",
    "    acc_rest=test_model(model,np.array(feature2.loc[:,ind_temp]),np.array(y2),sc)\n",
    "    acc_2f[p[0]+'_'+p[1]] = [train,valid,test,acc_rest]\n",
    "    print(acc_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc_2f_ann = pd.DataFrame(acc_2f,index=['train','valid','test','rest data']).T\n",
    "#acc_com = pd.concat([acc_com,acc_2f_ann])\n",
    "acc_com.to_csv('./results/dropna/acc_com_ann.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(acc_2f,index=['train','valid','test','rest data']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on some of rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_of_rest = ['正常/P940_MSham_B_Walking_trial_6_emg.csv',\n",
    "                '正常/P940_M050_B_Walking_trial_4_emg.csv',\n",
    "                '正常/P812_M100_A_Walking_trial_3_emg.csv',\n",
    "                '正常/P645_M050_A_Walking_trial_3_emg.csv',\n",
    "                '正常/P623_Msham_B_Walking_trial_2_emg.csv',\n",
    "                '正常/P551_M50_B_Walking_trial_6_emg.csv',\n",
    "                'P379_M050_2_OFF_A_FoG_trial_1_emg.csv',\n",
    "                'P551_M050_2_B_FoG_trial_2_emg.csv']\n",
    "#booster = xgb.Booster()\n",
    "#booster.load_model('./model/XGBoost_W256_S64_Left.json')\n",
    "#model = xgb.XGBClassifier()\n",
    "#model._Booster = booster\n",
    "acc = []\n",
    "columns=['LEFT_TA','LEFT_TS','LEFT_BF', 'LEFT_RF']\n",
    "for file in some_of_rest:\n",
    "    path = './data/'+file\n",
    "    feature2,y2 = dp.pipeline_feature(path,width=256,stride=64,scaler=False,\n",
    "                                      threshold_WAMP=threshold_WAMP,\n",
    "                                      threshold_ZC=threshold_ZC,\n",
    "                                      threshold_SSC=threshold_SSC,\n",
    "                                      bins=bins,\n",
    "                                      ranges=HIST_range,\n",
    "                                      show_para=False,\n",
    "                                      filt = 250)\n",
    "    feature2_sc = sc.transform(feature2)\n",
    "    acc += [test_model(model,feature2_sc,y2)]\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(feature2_sc)\n",
    "metrics.accuracy_score(y2,y_pred>0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:1 (16583, 8)\n",
      "i:2 (4489, 8)\n",
      "i:3 (35121, 8)\n",
      "i:4 (31027, 8)\n",
      "i:5 (30783, 8)\n",
      "i:6 (159539, 8)\n",
      "i:7 (225455, 8)\n",
      "i:8 (362273, 8)\n",
      "i:9 (28868, 8)\n",
      "i:10 (20878, 8)\n",
      "i:11 (25322, 8)\n",
      "i:12 (24242, 8)\n",
      "i:13 (21728, 8)\n",
      "i:14 (27916, 8)\n",
      "i:15 (27146, 8)\n",
      "i:16 (25947, 8)\n",
      "i:17 (26203, 8)\n",
      "i:18 (89500, 8)\n",
      "i:19 (70282, 8)\n",
      "i:20 (79595, 8)\n",
      "i:21 (73280, 8)\n",
      "Duration: 4.539805\n"
     ]
    }
   ],
   "source": [
    "files = np.array(df.columns)\n",
    "X = []\n",
    "Y = []\n",
    "#sc = StandardScaler(with_mean=False)\n",
    "i = 0\n",
    "start = time.time()\n",
    "for file in files:\n",
    "    emg_data = pd.read_csv('./data/'+file)\n",
    "    #emg_data = emg_data.fillna({'LEFT_TA':emg_data.LEFT_TA.mean(),\n",
    "    #                            'LEFT_TS':emg_data.LEFT_TS.mean(),\n",
    "    #                            'LEFT_BF':emg_data.LEFT_BF.mean(),\n",
    "    #                            'LEFT_RF':emg_data.LEFT_RF.mean(),\n",
    "    #                            'RIGHT_TA':emg_data.RIGHT_TA.mean(),\n",
    "    #                            'RIGHT_TS':emg_data.RIGHT_TS.mean(),\n",
    "    #                            'RIGHT_BF':emg_data.RIGHT_BF.mean(),\n",
    "    #                            'RIGHT_RF':emg_data.RIGHT_RF.mean()})\n",
    "    #emg_data = emg_data[emg_data.Label1 == emg_data.Label2].reset_index(drop=True)\n",
    "    emg_data = emg_data.dropna().reset_index(drop=True)\n",
    "    x = np.array(emg_data.iloc[:,3:])\n",
    "    y = np.array(emg_data.Label2)\n",
    "    #x,y = dp.generate_window_slide_data(emg_data,width=width,stride=stride,scaler=False,same_label=True)\n",
    "    X += x.tolist()\n",
    "    Y += y.tolist()\n",
    "    i += 1\n",
    "    print('i:%d'%i,x.shape)\n",
    "end = time.time()\n",
    "duration = end-start\n",
    "print('Duration: %f'%(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,feature,y,binary=True,file=None):\n",
    "    if binary:\n",
    "        ind = ((y==0)|(y==1)|(y==2)|(y==3)|(y==4)|(y==6))\n",
    "        ind1 = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "        y_01 = y.copy()\n",
    "        #ind1 = ((y_01==1)|(y_01==2)|(y_01==6))\n",
    "        y_01[ind1] = 1\n",
    "        oc = OneHotEncoder()\n",
    "        y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "        cw = None#{0:1,1:5}\n",
    "    else:\n",
    "        ind = ((y==1)|(y==2)|(y==6))\n",
    "        #ind = ((y==1)|(y==6))\n",
    "        y_01 = y[ind].copy()\n",
    "        oc = OneHotEncoder()\n",
    "        y_01 = oc.fit_transform(np.array(y_01)[:,np.newaxis]).toarray()\n",
    "        cw = None#{0:5,1:1,2:1}#{0:2,1:1,2:10,3:2}\n",
    "    x_full,x_test,y_full,y_test = train_test_split(np.array(feature)[ind,:],y_01,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=123,\n",
    "                                                   shuffle=True)\n",
    "    x_train,x_valid,y_train,y_valid = train_test_split(x_full,y_full,\n",
    "                                                       test_size=0.25,\n",
    "                                                       random_state=555,\n",
    "                                                       shuffle=True)\n",
    "    \n",
    "    #sm = BorderlineSMOTE(random_state=50,kind='borderline-1')\n",
    "    #sm = SMOTE(random_state=50)\n",
    "    #print(y_full.shape)\n",
    "    #x_full,y_full = sm.fit_resample(x_full,y_full)\n",
    "    #print(y_full_n.shape)\n",
    "    x_full = np.reshape(x_full,(-1,128*8))\n",
    "    sc = StandardScaler(with_mean=False)\n",
    "    #sc = MinMaxScaler((0,1))\n",
    "    x_train = sc.fit_transform(x_full)\n",
    "    x_train = x_train.reshape((-1,128,8))\n",
    "    #pca = PCA(n_components=150)\n",
    "    #x_train = pca.fit_transform(x_train)\n",
    "    #x_valid = sc.transform(x_valid)\n",
    "    x_test = np.reshape(x_test,(-1,128*8))\n",
    "    x_test = sc.transform(x_test)\n",
    "    x_test = x_test.reshape((-1,128,8))\n",
    "    #x_test = pca.transform(x_test)\n",
    "    #x_train = x_full\n",
    "    \n",
    "    early_stopping = callbacks.EarlyStopping(patience = 10,\n",
    "                                             monitor = 'val_accuracy', \n",
    "                                             restore_best_weights=True)\n",
    "    history = model.fit(x_train,y_full,validation_data=[x_test,y_test],\n",
    "                        epochs=300,batch_size=500,class_weight=cw,\n",
    "                        callbacks=[early_stopping],\n",
    "                        shuffle=True)\n",
    "    \n",
    "    if binary:\n",
    "        y_pred_t=model.predict(x_test)\n",
    "        test = metrics.accuracy_score(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1))\n",
    "        #test = metrics.accuracy_score(y_test,y_pred_t>0.5)\n",
    "        \n",
    "        #y_pred_v=model.predict(x_valid)\n",
    "        #valid = metrics.accuracy_score(y_valid,np.argmax(y_pred_v,axis=1))\n",
    "        y_pred_ta=model.predict(x_train)\n",
    "        train = metrics.accuracy_score(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1))\n",
    "        #train = metrics.accuracy_score(y_full,y_pred_ta>0.5)\n",
    "        \n",
    "        print('train: \\n',metrics.confusion_matrix(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1)))\n",
    "        #print('valid: \\n',metrics.confusion_matrix(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1)))\n",
    "        print('test: \\n',metrics.confusion_matrix(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1)))\n",
    "        \n",
    "        #print('train: \\n',metrics.confusion_matrix(y_full,y_pred_ta>0.5))\n",
    "        #print('test: \\n',metrics.confusion_matrix(y_test,y_pred_t>0.5))\n",
    "\n",
    "    else:\n",
    "        y_pred_t=model.predict(x_test)\n",
    "        test = metrics.accuracy_score(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1))\n",
    "        #y_pred_v=model.predict(x_valid)\n",
    "        #valid = metrics.accuracy_score(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1))\n",
    "        y_pred_ta=model.predict(x_train)\n",
    "        train = metrics.accuracy_score(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1))        \n",
    "        print('train: \\n',metrics.confusion_matrix(np.argmax(y_full,axis=1),np.argmax(y_pred_ta,axis=1)))\n",
    "        #print('valid: \\n',metrics.confusion_matrix(np.argmax(y_valid,axis=1),np.argmax(y_pred_v,axis=1)))\n",
    "        print('test: \\n',metrics.confusion_matrix(np.argmax(y_test,axis=1),np.argmax(y_pred_t,axis=1)))\n",
    "    print('test:%f'%test)\n",
    "    #print('valid:%f'%valid)\n",
    "    print('train:%f'%train)\n",
    "    if file != None:\n",
    "        model.save_model(file)\n",
    "    return train,test,sc#,pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_re = feature_sc.reshape((-1,X.shape[1],1))\n",
    "x_full,x_test,y_full,y_test = train_test_split(np.array(feature_re),np.array(y_01),test_size=0.2,random_state=123)\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(x_full,y_full,test_size=0.2,random_state=555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = layers.Input(shape=[128,8])\n",
    "lstm1 = layers.LSTM(50,return_sequences=True)(input_)\n",
    "drop1 = layers.Dropout(0.2)(lstm1)\n",
    "lstm2 = layers.LSTM(50,return_sequences=True)(drop1)\n",
    "drop2 = layers.Dropout(0.2)(lstm2)\n",
    "#lstm3 = layers.LSTM(50,return_sequences=True)(drop2)\n",
    "#drop3 = layers.Dropout(0.2)(lstm3)\n",
    "lstm4 = layers.LSTM(50)(drop2)\n",
    "drop4 = layers.Dropout(0.2)(lstm4)\n",
    "dense1 = layers.Dense(64,activation='relu')(drop4)\n",
    "drop5 = layers.Dropout(0.2)(dense1)\n",
    "dense2 = layers.Dense(32,activation='relu')(drop5)\n",
    "output = layers.Dense(3,activation='softmax')(dense2)\n",
    "model = Model(inputs=[input_],outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stopping = callbacks.EarlyStopping(patience = 20,\n",
    "                                         monitor = 'val_accuracy', \n",
    "                                         restore_best_weights=True)\n",
    "history = model.fit(x_train,y_train,validation_data=[x_valid,y_valid],\n",
    "                    epochs=100,batch_size=500,\n",
    "                   callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8872 samples, validate on 2219 samples\n",
      "Epoch 1/300\n",
      "8872/8872 [==============================] - 88s 10ms/sample - loss: 0.9841 - accuracy: 0.4876 - val_loss: 0.8857 - val_accuracy: 0.5002\n",
      "Epoch 2/300\n",
      "8872/8872 [==============================] - 82s 9ms/sample - loss: 0.8572 - accuracy: 0.5571 - val_loss: 0.8288 - val_accuracy: 0.6471\n",
      "Epoch 3/300\n",
      "8872/8872 [==============================] - 81s 9ms/sample - loss: 0.8051 - accuracy: 0.6443 - val_loss: 0.7573 - val_accuracy: 0.6670\n",
      "Epoch 4/300\n",
      "8872/8872 [==============================] - 81s 9ms/sample - loss: 0.7433 - accuracy: 0.6677 - val_loss: 0.7142 - val_accuracy: 0.6791\n",
      "Epoch 5/300\n",
      "8872/8872 [==============================] - 81s 9ms/sample - loss: 0.7212 - accuracy: 0.6787 - val_loss: 0.6996 - val_accuracy: 0.6886\n",
      "Epoch 6/300\n",
      "8872/8872 [==============================] - 87s 10ms/sample - loss: 0.7071 - accuracy: 0.6878 - val_loss: 0.6952 - val_accuracy: 0.7008\n",
      "Epoch 7/300\n",
      "8872/8872 [==============================] - 81s 9ms/sample - loss: 0.6882 - accuracy: 0.6948 - val_loss: 0.6657 - val_accuracy: 0.7012\n",
      "Epoch 8/300\n",
      "8872/8872 [==============================] - 81s 9ms/sample - loss: 0.6646 - accuracy: 0.7073 - val_loss: 0.6449 - val_accuracy: 0.7265\n",
      "Epoch 9/300\n",
      "8872/8872 [==============================] - 84s 9ms/sample - loss: 0.6623 - accuracy: 0.7076 - val_loss: 0.6351 - val_accuracy: 0.7206\n",
      "Epoch 10/300\n",
      "8872/8872 [==============================] - 81s 9ms/sample - loss: 0.6435 - accuracy: 0.7187 - val_loss: 0.6351 - val_accuracy: 0.7274\n",
      "Epoch 11/300\n",
      "8872/8872 [==============================] - 85s 10ms/sample - loss: 0.6427 - accuracy: 0.7163 - val_loss: 0.6309 - val_accuracy: 0.7260\n",
      "Epoch 12/300\n",
      "8872/8872 [==============================] - 82s 9ms/sample - loss: 0.6384 - accuracy: 0.7187 - val_loss: 0.6255 - val_accuracy: 0.7256\n",
      "Epoch 13/300\n",
      "8872/8872 [==============================] - 87s 10ms/sample - loss: 0.6292 - accuracy: 0.7271 - val_loss: 0.6225 - val_accuracy: 0.7165\n",
      "Epoch 14/300\n",
      "8872/8872 [==============================] - 83s 9ms/sample - loss: 0.6213 - accuracy: 0.7299 - val_loss: 0.6190 - val_accuracy: 0.7210\n",
      "Epoch 15/300\n",
      "3000/8872 [=========>....................] - ETA: 57s - loss: 0.6155 - accuracy: 0.7276 WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-6c283b32f09c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-d3cca95f67ba>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, feature, y, binary, file)\u001b[0m\n\u001b[0;32m     50\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                         shuffle=True)\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train,test,sc = train_model(model,np.array(x),np.array(y),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature2_re = feature2_sc.reshape((-1,feature.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_matrix=np.array([[0,1,1,1],\n",
    "                  [1,0,1,1],\n",
    "                  [10,100,0,10],\n",
    "                  [1.,1.,1,0]])\n",
    "cost_matrix=np.array([[0,2.],\n",
    "                  [5.,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "P0 = 55080/(55080+8181)\n",
    "P1 = 8181/(55080+8181)\n",
    "P = [P0,P1]\n",
    "cost_vec = np.zeros(2)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        if i == j:\n",
    "            continue\n",
    "        cost_vec[i]+=1/(1-P[i])*P[j]*cost_matrix[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 5.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1567100, shape=(4, 4), dtype=float64, numpy=\n",
       "array([[  0.,   1.,  10.,   1.],\n",
       "       [  1.,   0., 100.,   1.],\n",
       "       [  1.,   1.,   0.,   1.],\n",
       "       [  1.,   1.,  10.,   0.]])>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [2.],\n",
       "       [5.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(cost_matrix,(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
