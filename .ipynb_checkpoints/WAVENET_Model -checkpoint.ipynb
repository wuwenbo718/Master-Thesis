{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.config import Config\n",
    "from lib.data_set import Dataset\n",
    "from lib.model import NNModel\n",
    "from lib import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import data_processing as dp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers as KL\n",
    "from tensorflow.keras import models as KM\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model' from 'E:\\\\Document\\\\jupyter\\\\Master Thesis\\\\model.py'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp\n",
    "imp.reload(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File name read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file name of data with various Labels\n",
    "df = pd.read_csv('./useful_data_label.csv',index_col=0) \n",
    "# read file name of data with only label 0\n",
    "df2 = pd.read_csv('./unuseful_data_label.csv',index_col=0)\n",
    "# read some of the data with only label 0\n",
    "df3 = pd.read_csv('./data/file_name.txt',header=None)\n",
    "player = ctypes.windll.kernel32\n",
    "\n",
    "ind = df2.iloc[1].isna()\n",
    "files = np.concatenate([np.array(df.columns),np.array('normal/'+df2.columns[ind])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override the base class of Config and Features for WAVENET Model\n",
    "class WAVENET_Config(Config):\n",
    "    NAME = 'WAVENET'\n",
    "    NUM_CLASSES = 2\n",
    "    EPOCHS = 300\n",
    "    BATCH_SIZE = 32\n",
    "    COST_SENSITIVE = False\n",
    "    CLASS_WEIGHTS = None\n",
    "    TEST_FILES = files[[6,30,31,32,33,34,35]]\n",
    "    \n",
    "class Rect_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self,config):\n",
    "        super(Rect_dataset,self).__init__(config)\n",
    "        self.n_env = config.N_ENV\n",
    "    \n",
    "    def rectify_data(self):\n",
    "        \n",
    "        self.Xr = dp.rectify_emg_moving_average(self.X,self.n_env)\n",
    "        self.Xr2 = dp.rectify_emg_moving_average(self.X2,self.n_env)\n",
    "        self.Xr3 = dp.rectify_emg_moving_average(self.X3,self.n_env)\n",
    "    \n",
    "    @property\n",
    "    def train_set(self):\n",
    "        return self.Xr,self.Y,self.F\n",
    "    \n",
    "    @property\n",
    "    def valid_set(self):\n",
    "        return self.Xr2,self.Y2,self.F2\n",
    "    \n",
    "    @property\n",
    "    def test_set(self):\n",
    "        return self.Xr3,self.Y3,self.F3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate WAVENET configuration\n",
    "config = WAVENET_Config()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Rect_dataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n",
      "skip\n",
      "3/174: G06_FoG_trial_1_emg.csv\n",
      "4/174: G06_FoG_trial_2_emg.csv\n",
      "5/174: G06_FoG_trial_3_emg.csv\n",
      "6/174: G07_Freezing_Trial1_trial_1_emg.csv\n",
      "7/174: G08_FoG_1_trial_1_emg.csv\n",
      "8/174: G08_FoG_2_trial_1_emg.csv\n",
      "9/174: G11_FoG_trial_1_emg.csv\n",
      "10/174: G11_FoG_trial_2_emg.csv\n",
      "11/174: P379_M050_2_OFF_A_FoG_trial_1_emg.csv\n",
      "12/174: P379_M050_2_OFF_A_FoG_trial_2_emg.csv\n",
      "13/174: P379_M050_2_OFF_A_FoG_trial_3_emg.csv\n",
      "14/174: P379_M050_2_OFF_B_FoG_trial_1_emg.csv\n",
      "15/174: P379_M050_2_OFF_B_FoG_trial_2_emg.csv\n",
      "16/174: P379_M050_2_OFF_B_FoG_trial_3_emg.csv\n",
      "17/174: P551_M050_2_A_FoG_trial_1_emg.csv\n",
      "18/174: P551_M050_2_B_FoG_trial_1_emg.csv\n",
      "19/174: P551_M050_2_B_FoG_trial_2_emg.csv\n",
      "20/174: P812_M050_2_B_FoG_trial_1_emg.csv\n",
      "21/174: P812_M050_2_B_FoG_trial_2_emg.csv\n",
      "22/174: normal/G02_Walking_trial_1_emg.csv\n",
      "23/174: normal/G03_Walking_trial_1_emg.csv\n",
      "24/174: normal/G03_Walking_trial_2_emg.csv\n",
      "25/174: normal/G05_Walking_struct_fixed_trial_1_emg.csv\n",
      "26/174: normal/G05_Walking_struct_fixed_trial_2_emg.csv\n",
      "27/174: normal/G05_Walking_struct_fixed_trial_3_emg.csv\n",
      "28/174: normal/G09_FoG_trial_1_emg.csv\n",
      "29/174: normal/G09_FoG_trial_2_emg.csv\n",
      "30/174: normal/G09_FoG_trial_3_emg.csv\n",
      "31/174: normal/G09_Walking_trial_2_emg.csv\n",
      "32/174: normal/G09_Walking_trial_4_emg.csv\n",
      "33/174: normal/G09_Walking_trial_6_emg.csv\n",
      "34/174: normal/G11_Walking_trial_2_emg.csv\n",
      "35/174: normal/G11_Walking_trial_4_emg.csv\n",
      "36/174: normal/P231_M050_A_Walking_trial_2_emg.csv\n",
      "37/174: normal/P231_M050_A_Walking_trial_4_emg.csv\n",
      "38/174: normal/P231_M050_A_Walking_trial_6_emg.csv\n",
      "39/174: normal/P231_M050_B_Walking_trial_2_emg.csv\n",
      "40/174: normal/P231_M050_B_Walking_trial_4_emg.csv\n",
      "41/174: normal/P231_M050_B_Walking_trial_6_emg.csv\n",
      "42/174: normal/P231_M100_2_A_FoG_trial_3_emg.csv\n",
      "43/174: normal/P231_M100_2_A_Walking_trial_4_emg.csv\n",
      "44/174: normal/P231_M100_2_A_Walking_trial_6_emg.csv\n",
      "45/174: normal/P231_M100_ON_A_Walking_trial_2_emg.csv\n",
      "46/174: normal/P231_M100_ON_A_Walking_trial_4_emg.csv\n",
      "47/174: normal/P231_M100_ON_A_Walking_trial_6_emg.csv\n",
      "48/174: normal/P231_Msham_A_Walking_trial_2_emg.csv\n",
      "49/174: normal/P231_Msham_A_Walking_trial_6_emg.csv\n",
      "50/174: normal/P231_Msham_B_Walking_trial_2_emg.csv\n",
      "51/174: normal/P351_M050_2_A_FoG_trial_1_emg.csv\n",
      "52/174: normal/P351_M050_2_A_FoG_trial_2_emg.csv\n",
      "53/174: normal/P351_M050_2_A_FoG_trial_3_emg.csv\n",
      "54/174: normal/P351_M050_2_A_Walking_trial_2_emg.csv\n",
      "55/174: normal/P351_M050_2_A_Walking_trial_4_emg.csv\n",
      "56/174: normal/P351_M050_2_A_Walking_trial_6_emg.csv\n",
      "57/174: normal/P351_M050_2_B_FoG_trial_1_emg.csv\n",
      "58/174: normal/P351_M050_2_B_FoG_trial_2_emg.csv\n",
      "59/174: normal/P351_M050_2_B_FoG_trial_3_emg.csv\n",
      "60/174: normal/P351_M050_2_B_Walking_trial_2_emg.csv\n",
      "61/174: normal/P351_M050_2_B_Walking_trial_4_emg.csv\n",
      "62/174: normal/P351_M050_2_B_Walking_trial_6_emg.csv\n",
      "63/174: normal/P351_M050_A_FoG_trial_1_emg.csv\n",
      "64/174: normal/P351_M050_A_FoG_trial_2_emg.csv\n",
      "65/174: normal/P351_M050_A_FoG_trial_3_emg.csv\n",
      "66/174: normal/P351_M050_A_Walking_trial_2_emg.csv\n",
      "67/174: normal/P351_M050_A_Walking_trial_4_emg.csv\n",
      "68/174: normal/P351_M050_B_FoG_trial_1_emg.csv\n",
      "69/174: normal/P351_M050_B_FoG_trial_2_emg.csv\n",
      "70/174: normal/P351_M050_B_FoG_trial_3_emg.csv\n",
      "71/174: normal/P351_M050_B_Walking_trial_2_emg.csv\n",
      "72/174: normal/P351_M050_B_Walking_trial_4_emg.csv\n",
      "73/174: normal/P351_M050_B_Walking_trial_6_emg.csv\n",
      "74/174: normal/P351_Msham_A_FoG_trial_1_emg.csv\n",
      "75/174: normal/P351_Msham_A_FoG_trial_2_emg.csv\n",
      "76/174: normal/P351_Msham_A_FoG_trial_3_emg.csv\n",
      "77/174: normal/P351_Msham_A_Walking_trial_2_emg.csv\n",
      "78/174: normal/P351_Msham_A_Walking_trial_4_emg.csv\n",
      "79/174: normal/P351_Msham_A_Walking_trial_6_emg.csv\n",
      "80/174: normal/P351_Msham_B_FoG_trial_1_emg.csv\n",
      "81/174: normal/P351_Msham_B_FoG_trial_2_emg.csv\n",
      "82/174: normal/P351_Msham_B_FoG_trial_3_emg.csv\n",
      "83/174: normal/P351_Msham_B_Walking_trial_2_emg.csv\n",
      "84/174: normal/P351_Msham_B_Walking_trial_4_emg.csv\n",
      "85/174: normal/P351_Msham_B_Walking_trial_6_emg.csv\n",
      "86/174: normal/P379_M050_A_Walking_trial_2_emg.csv\n",
      "87/174: normal/P379_M050_A_Walking_trial_3_emg.csv\n",
      "88/174: normal/P379_M050_B_Walking_trial_2_emg.csv\n",
      "89/174: normal/P379_Msham_B_Walking_trial_6_emg.csv\n",
      "90/174: normal/P533_M050_A_Walking_trial_1_emg.csv\n",
      "91/174: normal/P533_M050_A_Walking_trial_2_emg.csv\n",
      "92/174: normal/P533_M050_B_Walking_trial_2_emg.csv\n",
      "93/174: normal/P533_M050_B_Walking_trial_3_emg.csv\n",
      "94/174: normal/P533_M100_A_Walking_trial_2_emg.csv\n",
      "95/174: normal/P533_M100_B_Walking_trial_4_emg.csv\n",
      "96/174: normal/P551_M50_B_Walking_trial_6_emg.csv\n",
      "97/174: normal/P623_M050_2_A_Walking_trial_2_emg.csv\n",
      "98/174: normal/P623_M050_2_A_Walking_trial_4_emg.csv\n",
      "skip\n",
      "100/174: normal/P623_M050_2_B_Walking_trial_2_emg.csv\n",
      "101/174: normal/P623_M050_2_B_Walking_trial_6_emg.csv\n",
      "102/174: normal/P623_M050_A_Walking_trial_4_emg.csv\n",
      "103/174: normal/P623_M100_A_Walking_trial_4_emg.csv\n",
      "104/174: normal/P623_M100_B_Walking_trial_4_emg.csv\n",
      "105/174: normal/P623_Msham_A_Walking_trial_4_emg.csv\n",
      "106/174: normal/P623_Msham_A_Walking_trial_6_emg.csv\n",
      "107/174: normal/P623_Msham_B_Walking_trial_2_emg.csv\n",
      "108/174: normal/P623_Msham_B_Walking_trial_4_emg.csv\n",
      "109/174: normal/P645_M050_A_Walking_trial_2_emg.csv\n",
      "110/174: normal/P645_M050_A_Walking_trial_3_emg.csv\n",
      "111/174: normal/P645_M050_B_Walking_trial_2_emg.csv\n",
      "112/174: normal/P645_M050_B_Walking_trial_3_emg.csv\n",
      "113/174: normal/P812_M050_2_A_FoG_trial_1_emg.csv\n",
      "114/174: normal/P812_M050_2_A_FoG_trial_3_emg.csv\n",
      "115/174: normal/P812_M050_2_A_Walking_trial_2_emg.csv\n",
      "116/174: normal/P812_M050_2_A_Walking_trial_3_emg.csv\n",
      "117/174: normal/P812_M050_2_B_Walking_1_trial_4_emg.csv\n",
      "118/174: normal/P812_M050_A_FoG_trial_1_emg.csv\n",
      "119/174: normal/P812_M050_A_FoG_trial_2_emg.csv\n",
      "120/174: normal/P812_M050_A_FoG_trial_3_emg.csv\n",
      "121/174: normal/P812_M050_A_Walking_trial_1_emg.csv\n",
      "122/174: normal/P812_M050_A_Walking_trial_2_emg.csv\n",
      "123/174: normal/P812_M050_B_FoG_trial_1_emg.csv\n",
      "124/174: normal/P812_M050_B_FoG_trial_2_emg.csv\n",
      "125/174: normal/P812_M050_B_FoG_trial_3_emg.csv\n",
      "126/174: normal/P812_M050_B_Walking_trial_1_emg.csv\n",
      "127/174: normal/P812_M050_B_Walking_trial_2_emg.csv\n",
      "128/174: normal/P812_M100_A_FoG_trial_1_emg.csv\n",
      "129/174: normal/P812_M100_A_Walking_trial_3_emg.csv\n",
      "130/174: normal/P812_M100_B_FoG_trial_1_emg.csv\n",
      "131/174: normal/P812_M100_B_FoG_trial_3_emg.csv\n",
      "132/174: normal/P812_M100_B_Walking2_trial_1_emg.csv\n",
      "133/174: normal/P812_M100_B_Walking2_trial_2_emg.csv\n",
      "134/174: normal/P876_M100_B_FoG_trial_1_emg.csv\n",
      "135/174: normal/P876_M100_B_FoG_trial_2_emg.csv\n",
      "136/174: normal/P876_M100_B_FoG_trial_3_emg.csv\n",
      "137/174: normal/P876_M100_B_Walking_trial_4_emg.csv\n",
      "138/174: normal/P876_M100_B_Walking_trial_6_emg.csv\n",
      "139/174: normal/P940_M050_2_A_FoG_trial_3_emg.csv\n",
      "140/174: normal/P940_M050_2_A_FoG_trial_4_emg.csv\n",
      "141/174: normal/P940_M050_2_A_Walking_trial_2_emg.csv\n",
      "142/174: normal/P940_M050_2_B_FoG_trial_1_emg.csv\n",
      "143/174: normal/P940_M050_2_B_Walking_trial_2_emg.csv\n",
      "144/174: normal/P940_M050_2_B_Walking_trial_4_emg.csv\n",
      "145/174: normal/P940_M050_2_B_Walking_trial_6_emg.csv\n",
      "146/174: normal/P940_M050_A_FoG_trial_2_emg.csv\n",
      "147/174: normal/P940_M050_A_FoG_trial_3_emg.csv\n",
      "148/174: normal/P940_M050_A_Walking_trial_2_emg.csv\n",
      "149/174: normal/P940_M050_A_Walking_trial_4_emg.csv\n",
      "150/174: normal/P940_M050_A_Walking_trial_6_emg.csv\n",
      "151/174: normal/P940_M050_B_FoG_trial_1_emg.csv\n",
      "152/174: normal/P940_M050_B_FoG_trial_2_emg.csv\n",
      "153/174: normal/P940_M050_B_FoG_trial_3_emg.csv\n",
      "154/174: normal/P940_M050_B_Walking_trial_2_emg.csv\n",
      "155/174: normal/P940_M050_B_Walking_trial_4_emg.csv\n",
      "156/174: normal/P940_M050_B_Walking_trial_6_emg.csv\n",
      "157/174: normal/P940_M100_A_FoG_trial_1_emg.csv\n",
      "158/174: normal/P940_M100_A_FoG_trial_2_emg.csv\n",
      "159/174: normal/P940_M100_A_FoG_trial_3_emg.csv\n",
      "160/174: normal/P940_M100_A_Walking_trial_2_emg.csv\n",
      "161/174: normal/P940_M100_A_Walking_trial_4_emg.csv\n",
      "162/174: normal/P940_M100_A_Walking_trial_6_emg.csv\n",
      "163/174: normal/P940_M100_B_FoG_trial_2_emg.csv\n",
      "164/174: normal/P940_M100_B_FoG_trial_3_emg.csv\n",
      "165/174: normal/P940_M100_B_Walking_2_trial_2_emg.csv\n",
      "166/174: normal/P940_M100_B_Walking_2_trial_6_emg.csv\n",
      "167/174: normal/P940_MSham_A_FoG_trial_1_emg.csv\n",
      "168/174: normal/P940_MSham_A_FoG_trial_3_emg.csv\n",
      "169/174: normal/P940_MSham_A_Walking_trial_2_emg.csv\n",
      "170/174: normal/P940_MSham_A_Walking_trial_4_emg.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/174: normal/P940_MSham_A_Walking_trial_6_emg.csv\n",
      "172/174: normal/P940_MSham_B_Walking_trial_2_emg.csv\n",
      "173/174: normal/P940_MSham_B_Walking_trial_4_emg.csv\n",
      "174/174: normal/P940_MSham_B_Walking_trial_6_emg.csv\n"
     ]
    }
   ],
   "source": [
    "# Load data from files\n",
    "data.load_data(files)\n",
    "\n",
    "# Rectify the data to get envelope\n",
    "data.rectify_data()\n",
    "\n",
    "X_train,Y_train,_ = data.train_set\n",
    "X_valid,Y_valid,_ = data.valid_set\n",
    "X_test, Y_test, _ = data.test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override base class of SimpleMode for WAVENET\n",
    "class WAVENET_Model(NNModel):\n",
    "    \n",
    "    def __init__(self,name,mode,config,model_dir):\n",
    "        super(WAVENET_Model,self).__init__(name,config,model_dir)\n",
    "    \n",
    "    def build(self,config):\n",
    "        \n",
    "        self.input_shape = [config.WINDOW_SIZE, len(config.CHANNELS)]\n",
    "        \n",
    "        kernel_size=3\n",
    "        reg=keras.regularizers.l2(1e-4)\n",
    "        drop_rate = 0.\n",
    "        kernel_initializer = 'glorot_normal'\n",
    "        mo = 0.8\n",
    "        st = 1\n",
    "        axis = 2\n",
    "        \n",
    "        model = KM.Sequential()\n",
    "        model.add(KL.InputLayer(input_shape=self.input_shape))\n",
    "        for rate in (2, 4, 8,16,32,64,128,256):\n",
    "            model.add(KL.Conv1D(filters=32, kernel_size=2, padding=\"causal\",\n",
    "                                            activation=\"relu\", dilation_rate=rate))\n",
    "        model.add(KL.Conv1D(filters=16, kernel_size=1))\n",
    "        model.add(KL.GlobalAveragePooling1D())\n",
    "        #model.add(layers.Conv1D(filters=16, kernel_size=5,padding='same'))\n",
    "        #model.add(layers.BatchNormalization(momentum=0.8))\n",
    "        #model.add(layers.Activation('relu'))\n",
    "        #model.add(layers.AveragePooling2D((1,2)))\n",
    "        #model.add(layers.Dropout(0.2))\n",
    "  \n",
    "        model.add(KL.Dense(config.NUM_CLASSES,activation='softmax'))\n",
    "               \n",
    "        model.summary()\n",
    "        \n",
    "        if config.COST_SENSITIVE:\n",
    "            self.cost_matrix = config.COST_MATRIX\n",
    "            model.compile(loss=self.sparse_cost_sensitive_loss, optimizer=\"adam\", metrics=['accuracy'])\n",
    "            print('Using cost sensitive with cost matrix:\\n',np.array(self.cost_matrix))\n",
    "        else:\n",
    "            model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "            if config.CLASS_WEIGHTS != None:\n",
    "                print('Using categorical crossentropy with class weights:\\n',config.CLASS_WEIGHTS)\n",
    "            else:\n",
    "                print('Using categorical crossentropy without class weights.')\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def sparse_cost_sensitive_loss (self,y_true,y_pred):\n",
    "        cost_matrix = self.cost_matrix\n",
    "        batch_cost_matrix = tf.nn.embedding_lookup(cost_matrix, tf.argmax(y_true,axis=1))\n",
    "        eps = 1e-6\n",
    "        probability = tf.clip_by_value(y_pred, eps, 1-eps)\n",
    "        cost_values = tf.math.log(1-probability)*batch_cost_matrix\n",
    "        loss = tf.reduce_mean(-tf.reduce_sum(cost_values, axis=1))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split and processing for model\n",
    "class_id = [2,6]\n",
    "binary = True\n",
    "x_train,y_train,x_valid,y_valid,x_test,y_test = utils.data_split_oh((X_train,X_valid,X_test),\n",
    "                                                                    (Y_train,Y_valid,Y_test),\n",
    "                                                                    class_id,\n",
    "                                                                    binary,\n",
    "                                                                    random_state = 555)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 1024, 32)          544       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1024, 32)          2080      \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1024, 32)          2080      \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1024, 32)          2080      \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 1024, 32)          2080      \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 1024, 32)          2080      \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1024, 32)          2080      \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 1024, 32)          2080      \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 1024, 16)          528       \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 15,666\n",
      "Trainable params: 15,666\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Using categorical crossentropy without class weights.\n"
     ]
    }
   ],
   "source": [
    "#cost_matrix = tf.constant([[0,1.5,1,1.2],\n",
    "#              [1,0,1,1],\n",
    "#              [5,10,0,5],\n",
    "#              [1.,1.,1,0]])\n",
    "#cost_matrix = tf.constant([[0.,1.,1.],\n",
    "#              [10.,0.,1.],\n",
    "#              [10.,4.,0.]])\n",
    "config.COST_MATRIX = tf.constant([[0,1.],\n",
    "              [10,0]])\n",
    "\n",
    "if binary:\n",
    "    config.COST_SENSITIVE = True\n",
    "    config.NUM_CLASSES = 2\n",
    "else:\n",
    "    config.COST_SENSITIVE = False\n",
    "    config.NUM_CLASSES = len(class_id)\n",
    "\n",
    "wavenet_Model = WAVENET_Model('WAVENET','training',config,'./model/WAVENET/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0.\n",
      "\n",
      "Checkpoint Path: ./model/WAVENET/wavenet20210119T1416\\WAVENET_wavenet_{epoch:04d}.h5\n",
      "Epoch 1/300\n",
      "14/14 [==============================] - 8s 445ms/step - loss: 0.6849 - accuracy: 0.5383 - val_loss: 0.6111 - val_accuracy: 0.6809\n",
      "Epoch 2/300\n",
      "14/14 [==============================] - 4s 275ms/step - loss: 0.6110 - accuracy: 0.6428 - val_loss: 0.4849 - val_accuracy: 0.6809\n",
      "Epoch 3/300\n",
      "14/14 [==============================] - 4s 286ms/step - loss: 0.4999 - accuracy: 0.6204 - val_loss: 0.3305 - val_accuracy: 0.9574\n",
      "Epoch 4/300\n",
      "14/14 [==============================] - 4s 268ms/step - loss: 0.3288 - accuracy: 0.8568 - val_loss: 0.1925 - val_accuracy: 0.9362\n",
      "Epoch 5/300\n",
      "14/14 [==============================] - 4s 291ms/step - loss: 0.2635 - accuracy: 0.8996 - val_loss: 0.3542 - val_accuracy: 0.8511\n",
      "Epoch 6/300\n",
      "14/14 [==============================] - 4s 272ms/step - loss: 0.2957 - accuracy: 0.8776 - val_loss: 0.1691 - val_accuracy: 0.9433\n",
      "Epoch 7/300\n",
      "14/14 [==============================] - 4s 288ms/step - loss: 0.2244 - accuracy: 0.9173 - val_loss: 0.1440 - val_accuracy: 0.9433\n",
      "Epoch 8/300\n",
      "14/14 [==============================] - 5s 346ms/step - loss: 0.1758 - accuracy: 0.9176 - val_loss: 0.1397 - val_accuracy: 0.9433\n",
      "Epoch 9/300\n",
      "14/14 [==============================] - 5s 336ms/step - loss: 0.1725 - accuracy: 0.9478 - val_loss: 0.1416 - val_accuracy: 0.9433\n",
      "Epoch 10/300\n",
      "14/14 [==============================] - 5s 326ms/step - loss: 0.1873 - accuracy: 0.9186 - val_loss: 0.1338 - val_accuracy: 0.9433\n",
      "Epoch 11/300\n",
      "14/14 [==============================] - 4s 301ms/step - loss: 0.1609 - accuracy: 0.9369 - val_loss: 0.1323 - val_accuracy: 0.9504\n",
      "Epoch 12/300\n",
      "14/14 [==============================] - 4s 286ms/step - loss: 0.1543 - accuracy: 0.9442 - val_loss: 0.1962 - val_accuracy: 0.9433\n",
      "Epoch 13/300\n",
      "14/14 [==============================] - 4s 304ms/step - loss: 0.2497 - accuracy: 0.9005 - val_loss: 0.1414 - val_accuracy: 0.9504\n",
      "Epoch 14/300\n",
      "14/14 [==============================] - 4s 305ms/step - loss: 0.1684 - accuracy: 0.9447 - val_loss: 0.1534 - val_accuracy: 0.9504\n",
      "Epoch 15/300\n",
      "14/14 [==============================] - 5s 328ms/step - loss: 0.1604 - accuracy: 0.9404 - val_loss: 0.1521 - val_accuracy: 0.9504\n",
      "Epoch 16/300\n",
      "14/14 [==============================] - 4s 312ms/step - loss: 0.1344 - accuracy: 0.9532 - val_loss: 0.1455 - val_accuracy: 0.9433\n",
      "Epoch 17/300\n",
      "14/14 [==============================] - 4s 294ms/step - loss: 0.1278 - accuracy: 0.9482 - val_loss: 0.1372 - val_accuracy: 0.9362\n",
      "Epoch 18/300\n",
      "14/14 [==============================] - 4s 305ms/step - loss: 0.1491 - accuracy: 0.9410 - val_loss: 0.1324 - val_accuracy: 0.9433\n",
      "Epoch 19/300\n",
      "14/14 [==============================] - 4s 316ms/step - loss: 0.1299 - accuracy: 0.9521 - val_loss: 0.1902 - val_accuracy: 0.9433\n",
      "Epoch 20/300\n",
      "14/14 [==============================] - 4s 326ms/step - loss: 0.1756 - accuracy: 0.9378 - val_loss: 0.1362 - val_accuracy: 0.9504\n",
      "Epoch 21/300\n",
      "14/14 [==============================] - 4s 317ms/step - loss: 0.0875 - accuracy: 0.9646 - val_loss: 0.1455 - val_accuracy: 0.9433\n"
     ]
    }
   ],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(patience = 10,\n",
    "                                             monitor = 'val_loss', \n",
    "                                             #baseline = 0.9,\n",
    "                                             restore_best_weights=True)\n",
    "wavenet_Model.train((x_train,y_train),(x_valid,y_valid),config.EPOCHS,config.BATCH_SIZE,[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_train: 0.945238\n",
      "confusion_matrix:\n",
      " [[244  15]\n",
      " [  8 153]]\n",
      "acc_valid: 0.950355\n",
      "confusion_matrix:\n",
      " [[91  5]\n",
      " [ 2 43]]\n",
      "acc_test: 0.919463\n",
      "confusion_matrix:\n",
      " [[53  4]\n",
      " [ 8 84]]\n"
     ]
    }
   ],
   "source": [
    "acc_train,cm_train = wavenet_Model.model_metrics(x_train,y_train)\n",
    "acc_valid,cm_valid = wavenet_Model.model_metrics(x_valid,y_valid)\n",
    "acc_test,cm_test = wavenet_Model.model_metrics(x_test,y_test)\n",
    "print('acc_train: %f\\nconfusion_matrix:\\n'%acc_train,cm_train)\n",
    "print('acc_valid: %f\\nconfusion_matrix:\\n'%acc_valid,cm_valid)\n",
    "print('acc_test: %f\\nconfusion_matrix:\\n'%acc_test,cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
