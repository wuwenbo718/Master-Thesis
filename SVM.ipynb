{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder,normalize\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "import data_processing as dp\n",
    "from scipy import signal\n",
    "from scipy.stats import skew,pearsonr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "from itertools import combinations\n",
    "from sklearn.feature_selection import SelectKBest,f_classif,chi2,mutual_info_classif,VarianceThreshold,RFE,SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('.\\data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emg_data = pd.read_csv('.\\data\\P812_M050_2_B_FoG_trial_1_emg.csv')\n",
    "emg_data2 = pd.read_csv('.\\data\\P812_M050_2_B_FoG_trial_2_emg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emg_data = emg_data.fillna({'LEFT_BF':emg_data.LEFT_BF.mean(),\n",
    "                           'LEFT_RF':emg_data.LEFT_RF.mean(),\n",
    "                           'RIGHT_TA':emg_data.RIGHT_TA.mean(),\n",
    "                           'RIGHT_TS':emg_data.RIGHT_TS.mean(),\n",
    "                           'RIGHT_BF':emg_data.RIGHT_BF.mean(),\n",
    "                           'RIGHT_RF':emg_data.RIGHT_RF.mean()})\n",
    "emg_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv('./processed data/featurePcwtf_W256_S64_WS32_DWTLmax_dropna_samelabel.csv')\n",
    "\n",
    "# read file name of data with various Labels\n",
    "df = pd.read_csv('./useful_data_label.csv',index_col=0) \n",
    "drop = 'P551_M050_2_B_FoG_trial_2_emg.csv'\n",
    "ind_drop = df.columns!=drop\n",
    "\n",
    "# read file name of data with only label 0\n",
    "df2 = pd.read_csv('./unuseful_data_label.csv',index_col=0)\n",
    "# read some of the data with only label 0\n",
    "df3 = pd.read_csv('./data/file_name.txt',header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = np.concatenate([np.array(df.columns),np.array(df3.loc[:,0])])\n",
    "ind = Data.File.isin(files)\n",
    "Data_sel = Data[ind]\n",
    "Data_rest = Data[~ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_col = ['_IEMG','_MAV','_SSI','_VAR','_RMS',\n",
    "               '_WL','_ZC','_SSC','_WAMP','_skew',\n",
    "               '_Acti','_AR','_HIST','_MDF','_MNF','_mDWT']\n",
    "\n",
    "feature_all = Data_sel.iloc[:,1:-1]\n",
    "#ind_temp1 = feature_all.columns.str.contains('_IEMG')\n",
    "#ind_temp2 = feature_all.columns.str.contains('_skew')\n",
    "#ind_temp3 = feature_all.columns.str.contains('_mDWT')\n",
    "#ind_temp = ind_temp2|ind_temp3\n",
    "ind_temp = feature_all.columns.str.contains('_mDWT')\n",
    "feature = feature_all.loc[:,~ind_temp]\n",
    "#feature = Data_sel.iloc[:,1:-1]\n",
    "y = Data_sel.iloc[:,0]\n",
    "feature2_all = Data_rest.iloc[:,1:-1]\n",
    "feature2 = feature2_all.loc[:,~ind_temp]\n",
    "#feature2 = Data_rest.iloc[:,1:-1]\n",
    "y2 = Data_rest.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = generate_window_slide_data(emg_data)\n",
    "x2,y2 = generate_window_slide_data(emg_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './processed data/data_set_after_window_withoutSC.hdf5'\n",
    "with h5py.File(path,'r') as f:\n",
    "    x = f['cwt_data'][...]\n",
    "    y = f['label2'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = dp.generate_feature(x,2,0,bins=9,ranges=(-70,70))\n",
    "#feature2 = dp.generate_feature(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler(with_mean=False)\n",
    "feature_sc = sc.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "player = ctypes.windll.kernel32\n",
    "player.Beep(1000,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file name of data with various Labels\n",
    "df = pd.read_csv('./useful_data_label.csv',index_col=0) \n",
    "#drop = 'G08_FoG_1_trial_1_emg.csv'\n",
    "#ind_drop = df.columns!=drop\n",
    "\n",
    "# read file name of data with only label 0\n",
    "df2 = pd.read_csv('./unuseful_data_label.csv',index_col=0)\n",
    "# read some of the data with only label 0\n",
    "df3 = pd.read_csv('./data/file_name.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data and labels of df2 or df3\n",
    "sc = StandardScaler(with_mean = True)\n",
    "#sc = MinMaxScaler()\n",
    "ind = df2.iloc[1].isna()\n",
    "files = np.concatenate([np.array(df.columns),np.array('正常/'+df2.columns[ind])])\n",
    "#files = np.array(df.columns[[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]])#[[2,5,6,7,8,10,11,12,13,16,17,18,19,20]])\n",
    "N = len(files)\n",
    "#sc = StandardScaler(with_mean=False)\n",
    "width = 1024\n",
    "stride = 512\n",
    "start = time.time()\n",
    "i = 0\n",
    "X = []\n",
    "Y = []\n",
    "X2 = []\n",
    "Y2 = []\n",
    "X3 = []\n",
    "Y3 = []\n",
    "F = []\n",
    "F2 = []\n",
    "F3 = []\n",
    "for file in files:\n",
    "    i += 1\n",
    "    if file.find('G04')==0:\n",
    "        continue\n",
    "    emg_data = pd.read_csv('./data/'+file)\n",
    "    #emg_data.iloc[:,3:] = \n",
    "    emg_data = emg_data.dropna().reset_index(drop=True)\n",
    "    #emg_data.iloc[:,3:]=normalize(emg_data.iloc[:,3:],axis=0)\n",
    "    #emg_data.iloc[:,3:] = sc.fit_transform(emg_data.iloc[:,3:])\n",
    "    #for j in (0,1,4,5):\n",
    "        #ind = abs(zscore(emg_data.iloc[:,j+3]))>10\n",
    "        #emg_data=emg_data.loc[~ind,:]\n",
    "        #ind_p = zscore(emg_data.iloc[:,j+3])>10\n",
    "        #ind_n = zscore(emg_data.iloc[:,j+3])<-10\n",
    "        #emg_data.loc[ind_p,emg_data.columns[3+j]] = emg_data.loc[~ind_p,emg_data.columns[3+j]].max()\n",
    "        #emg_data.loc[ind_n,emg_data.columns[3+j]] = emg_data.loc[~ind_n,emg_data.columns[3+j]].min()\n",
    "    fn = 300\n",
    "    wn=2*fn/1000\n",
    "    fn1 = 400\n",
    "    wn1 = 2*fn1/1000\n",
    "    #fs = 1000.0  # Sample frequency (Hz)\n",
    "    #f0 = 50  # Frequency to be removed from signal (Hz)\n",
    "    #Q = 100.0  # Quality factor\n",
    "    # Design notch filter\n",
    "    #b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "    #b, a = signal.butter(4, [wn,wn1], 'bandpass')\n",
    "    b, a = signal.butter(4, [wn], 'lowpass')\n",
    "    #b, a = signal.butter(4, [wn], 'highpass')\n",
    "    #for j in ['LEFT_TA','LEFT_TS','LEFT_BF','LEFT_RF','RIGHT_TA','RIGHT_TS','RIGHT_BF','RIGHT_RF']:\n",
    "        #emg_data.loc[:,j] = signal.filtfilt(b, a, emg_data.loc[:,j])\n",
    "        #emg_data.loc[:,j] = signal.filtfilt(b1, a1, emg_data.loc[:,j])\n",
    "    \"\"\"if file==df.columns[5]:\n",
    "        print(file)\n",
    "        fs = 1000.0  # Sample frequency (Hz)\n",
    "        f0 = 72  # Frequency to be removed from signal (Hz)\n",
    "        Q = 50.0  # Quality factor\n",
    "        # Design notch filter\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TA'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TA'])\n",
    "        f0 = 75  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TS'])\n",
    "        f0 = 13.2  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'RIGHT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'RIGHT_TS'])\n",
    "    if file==df.columns[6]:\n",
    "        print(file)\n",
    "        fs = 1000.0  # Sample frequency (Hz)\n",
    "        f0 = 40  # Frequency to be removed from signal (Hz)\n",
    "        Q = 100.0  # Quality factor\n",
    "        # Design notch filter\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TA'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TA'])\n",
    "        f0 = 26.5  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TS'])\n",
    "        f0 = 13.2  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TS'])\n",
    "        f0 = 48  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        #emg_data.loc[:,'LEFT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TS'])\n",
    "        f0 = 50  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'RIGHT_TA'] = signal.filtfilt(b1, a1, emg_data.loc[:,'RIGHT_TA'])\n",
    "    if file==df.columns[7]:\n",
    "        print(file)\n",
    "        fs = 1000.0  # Sample frequency (Hz)\n",
    "        f0 = 13.2  # Frequency to be removed from signal (Hz)\n",
    "        Q = 50.0  # Quality factor\n",
    "        # Design notch filter\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TS'])\n",
    "        emg_data.loc[:,'RIGHT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'RIGHT_TS'])\n",
    "        f0 = 26.5  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TS'])\n",
    "        f0 = 50  # Frequency to be removed from signal (Hz)\n",
    "        b1, a1 = signal.iirnotch(f0, Q, fs)\n",
    "        emg_data.loc[:,'LEFT_TS'] = signal.filtfilt(b1, a1, emg_data.loc[:,'LEFT_TS'])\n",
    "    \"\"\"\n",
    "    #emg_data.iloc[:,3:] = sc.fit_transform(emg_data.iloc[:,3:])\n",
    "    #emg_data.iloc[:,3:]=normalize(emg_data.iloc[:,3:],axis=0)\n",
    "    x_raw,y = dp.generate_window_slide_data_time_continue_fremove(emg_data,width=width,\n",
    "                                        stride=stride,\n",
    "                                        scaler=True,\n",
    "                                        same_label=True)\n",
    "    #x=np.abs(x)\n",
    "    #x=dp.lowpass_filter(x,300)\n",
    "    #x=dp.mean_smooth(x)\n",
    "    shape = x_raw.shape\n",
    "    x = np.zeros(shape)\n",
    "    #x = x_raw\n",
    "    for n in range(shape[0]):\n",
    "        x[n,:,:] = dp.detrend(x_raw[n,:,:],50)\n",
    "        for c in range(shape[2]):\n",
    "            x[n,:,c] = signal.filtfilt(b,a,x[n,:,c])\n",
    "    \n",
    "    ind1 = []\n",
    "    ind2 = []\n",
    "    ind3 = []\n",
    "    l = len(y)\n",
    "    for j in set(y):\n",
    "        ind = np.where(y == j)[0].tolist()\n",
    "        l_t = len(ind)\n",
    "#         if (j==1)&((file==df.columns[5])|(file==df.columns[17])):\n",
    "#             continue\n",
    "        if file == files[6]:\n",
    "            ind3 += ind\n",
    "            print('X2')\n",
    "        else:\n",
    "            ind1 += ind[:int(l_t*0.8)]\n",
    "            ind2 += ind[int(l_t*0.8):int(l_t*1)]\n",
    "        #ind3 += ind[int(l_t*0.8):]\n",
    "\n",
    "    l1 = len(ind1)\n",
    "    l2 = len(ind2)\n",
    "    l3 = len(ind3)\n",
    "\n",
    "    fi = [file]*len(ind1)\n",
    "    fi2 = [file]*len(ind2)\n",
    "    fi3 = [file]*len(ind3)\n",
    "    \n",
    "    X += x[ind1].tolist()\n",
    "    Y += y[ind1].tolist()\n",
    "    \n",
    "    X2 += x[ind2].tolist()\n",
    "    Y2 += y[ind2].tolist()\n",
    "    \n",
    "    X3 += x[ind3].tolist()\n",
    "    Y3 += y[ind3].tolist()\n",
    "    \n",
    "    F += fi\n",
    "    F2 += fi2\n",
    "    F3 += fi3\n",
    "    print('%d/%d: '%(i,N)+file)\n",
    "\n",
    "ind_c = [True,True,False,False,True,True,False,False]\n",
    "X = np.array(X)#[:,:,ind_c]\n",
    "Y = np.array(Y)\n",
    "X2 = np.array(X2)#[:,:,ind_c]\n",
    "Y2 = np.array(Y2)\n",
    "X3 = np.array(X3)#[:,:,ind_c]\n",
    "Y3 = np.array(Y3)\n",
    "end = time.time()\n",
    "duration = end-start\n",
    "print('Duration: %f'%(duration))\n",
    "player.Beep(1000,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './processed data/data_set_after_window_W1024_S512_D50_F300_SD_leave_file6_out.hdf5'\n",
    "with h5py.File(file,'r') as f:\n",
    "    X = f['X'][...]\n",
    "    Y = f['Y'][...]\n",
    "    X2 = f['X2'][...]\n",
    "    Y2 = f['Y2'][...]\n",
    "    X3 = f['X3'][...]\n",
    "    Y3 = f['Y3'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "imp.reload(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_pd(data,threshold_WAMP=30,\n",
    "                     threshold_ZC=0,\n",
    "                     threshold_SSC=0,\n",
    "                     bins=9,\n",
    "                     ranges=(-10,10),\n",
    "                     fbins=5,\n",
    "                     franges=(0,300),\n",
    "                     threshold_F=0.5,\n",
    "                     num = 3,\n",
    "                     wavelet='db7',\n",
    "                     level=3):\n",
    "    columns = pd.Index(['LEFT_TA', 'LEFT_TS', 'LEFT_BF', 'LEFT_RF',\n",
    "       'RIGHT_TA', 'RIGHT_TS', 'RIGHT_BF', 'RIGHT_RF'])\n",
    "    IEMG = pd.DataFrame(dp.compute_IEMG(data),columns=columns+'_IEMG')\n",
    "    #MAV = pd.DataFrame(compute_MAV(data),columns=columns+'_MAV')\n",
    "    SSI = pd.DataFrame(dp.compute_SSI(data),columns=columns+'_SSI')\n",
    "    #VAR = pd.DataFrame(compute_VAR(data),columns=columns+'_VAR')\n",
    "    #RMS = pd.DataFrame(compute_RMS(data),columns=columns+'_RMS')\n",
    "    WL = pd.DataFrame(dp.compute_WL(data),columns=columns+'_WL')\n",
    "    ZC = pd.DataFrame(dp.compute_ZC(data,threshold_ZC),columns=columns+'_ZC')\n",
    "    #ZC = compute_ZC_expand_pd(data,threshold_ZC)\n",
    "    ku = pd.DataFrame(dp.compute_ku(data),columns=columns+'_ku')\n",
    "    SSC = pd.DataFrame(dp.compute_SSC(data,threshold_SSC),columns=columns+'_SSC')\n",
    "    WAMP = pd.DataFrame(dp.compute_WAMP(data,threshold_WAMP),columns=columns+'_WAMP')\n",
    "    skew = pd.DataFrame(dp.compute_Skewness(data),columns=columns+'_skew')\n",
    "    Acti = pd.DataFrame(dp.compute_Acti(data),columns=columns+'_Acti')\n",
    "#     Mobi = pd.DataFrame(dp.compute_Mobi(data),columns=columns+'_Mobi')\n",
    "#     Comp = pd.DataFrame(dp.compute_complexity(data),columns=columns+'_Comp')\n",
    "    AR = pd.DataFrame(dp.compute_AR(data),columns=columns+'_AR')\n",
    "    #AR = compute_AR_pd(data)\n",
    "#     CC = dp.compute_CC_pd(data)\n",
    "    HIST = dp.compute_HIST_pd(data,bins=bins,ranges=ranges)\n",
    "    #FHIST = compute_FHIST_pd(data,bins=fbins,ranges=franges,threshold=threshold_F)\n",
    "#     MF = dp.compute_MaxFreq_pd(data,num=num)\n",
    "    MDF = pd.DataFrame(dp.compute_MDF(data),columns=columns+'_MDF')\n",
    "    MNF = pd.DataFrame(dp.compute_MNF(data),columns=columns+'_MNF')\n",
    "    mDWT = dp.compute_mDWT_pd(data,wavelet,level)\n",
    "    feature = pd.concat([IEMG,SSI,WL,ZC,ku,SSC,WAMP,skew,Acti,AR,HIST,MDF,MNF,mDWT],axis =1)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#width = 256\n",
    "threshold_WAMP = 0.1\n",
    "threshold_ZC = 0.001\n",
    "#threshold_ZC = np.linspace(-4,4,11)\n",
    "threshold_SSC = 0.01\n",
    "bins=4\n",
    "bound = 3\n",
    "HIST_range = (-bound,bound)\n",
    "level = 3\n",
    "num = 3\n",
    "wavelet='db7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = generate_feature_pd(X,threshold_WAMP=threshold_WAMP,\n",
    "                              threshold_ZC=threshold_ZC,\n",
    "                              threshold_SSC=threshold_SSC,\n",
    "                              bins=bins,ranges=HIST_range,\n",
    "                                level = level)\n",
    "feature2 = generate_feature_pd(X2,threshold_WAMP=threshold_WAMP,\n",
    "                              threshold_ZC=threshold_ZC,\n",
    "                              threshold_SSC=threshold_SSC,\n",
    "                              bins=bins,ranges=HIST_range,\n",
    "                                 level = level)\n",
    "feature3 = generate_feature_pd(X3,threshold_WAMP=threshold_WAMP,\n",
    "                              threshold_ZC=threshold_ZC,\n",
    "                              threshold_SSC=threshold_SSC,\n",
    "                              bins=bins,ranges=HIST_range,\n",
    "                                 level = level)\n",
    "player.Beep(1000,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = ((Y==0)|(Y==2)|(Y==6))\n",
    "ind2 = ((Y2==0)|(Y2==2)|(Y2==6))\n",
    "ind3 = ((Y3==0)|(Y3==2)|(Y3==6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler((0,1))\n",
    "#scaler = StandardScaler(with_mean=True)\n",
    "feature = scaler.fit_transform(feature)\n",
    "feature2 = scaler.fit_transform(feature2)\n",
    "feature3 = scaler.fit_transform(feature3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skb = SelectKBest(chi2, k=200)\n",
    "skb = SelectKBest(mutual_info_classif, k=200)\n",
    "feature_new=skb.fit_transform(feature, Y)\n",
    "feature_new2=skb.transform(feature2)\n",
    "feature_new3=skb.transform(feature3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vt = VarianceThreshold(threshold=0.01)\n",
    "feature_new=vt.fit_transform(feature)\n",
    "feature_new2=vt.transform(feature2)\n",
    "feature_new3=vt.transform(feature3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=150,copy=True)\n",
    "feature_new = pca.fit_transform(feature)\n",
    "feature_new2 = pca.transform(feature2)\n",
    "feature_new3 = pca.transform(feature3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfm = SelectFromModel(GradientBoostingClassifier(),max_features=200)\n",
    "feature_new = sfm.fit_transform(feature,Y)\n",
    "feature_new2 = sfm.transform(feature2)\n",
    "feature_new3 = sfm.transform(feature3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(estimator=LogisticRegression(max_iter=10000), n_features_to_select=200)\n",
    "feature_new = rfe.fit_transform(feature,Y)\n",
    "feature_new2 = rfe.transform(feature2)\n",
    "feature_new3 = rfe.transform(feature3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,feature,y,binary=True,file=None):\n",
    "    if binary:\n",
    "        ind = ((y==0)|(y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "        ind1 = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "        y_01 = y.copy()\n",
    "        y_01[ind1] = 1\n",
    "        metric = 'error'\n",
    "    else:\n",
    "        ind = ((y==1)|(y==2)|(y==6))\n",
    "        y_01 = y[ind].copy()\n",
    "        metric = 'merror'\n",
    "    x_full,x_test,y_full,y_test = train_test_split(feature.loc[ind,:],y_01,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=123,\n",
    "                                                   shuffle=False)\n",
    "    x_train,x_valid,y_train,y_valid = train_test_split(x_full,y_full,\n",
    "                                                       test_size=0.2,\n",
    "                                                       random_state=555,\n",
    "                                                       shuffle=True)\n",
    "    \n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred_t=model.predict(x_test)\n",
    "    test = metrics.accuracy_score(y_test,y_pred_t)\n",
    "    y_pred_v=model.predict(x_valid)\n",
    "    valid = metrics.accuracy_score(y_valid,y_pred_v)\n",
    "    y_pred_ta=model.predict(x_train)\n",
    "    train = metrics.accuracy_score(y_train,y_pred_ta)\n",
    "    if binary == False:\n",
    "        print('train: \\n',metrics.confusion_matrix(y_train,y_pred_ta))\n",
    "        print('valid: \\n',metrics.confusion_matrix(y_valid,y_pred_v))\n",
    "        print('test: \\n',metrics.confusion_matrix(y_test,y_pred_t))\n",
    "    print('test:%f'%test)\n",
    "    print('valid:%f'%valid)\n",
    "    print('train:%f'%train)\n",
    "    if file != None:\n",
    "        model.save_model(file)\n",
    "    return train,valid,test\n",
    "\n",
    "def test_model(model,feature,y):\n",
    "    ind = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "    y_01 = y.copy()\n",
    "    y_01[ind] = 1\n",
    "    y_pred=model.predict(feature)\n",
    "    test = metrics.accuracy_score(y_01,y_pred>0.5)\n",
    "    print('acc:%f'%test)\n",
    "    return test\n",
    "\n",
    "def test_load_model(path,feature,y):\n",
    "    ind = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "    y_01 = y.copy()\n",
    "    y_01[ind] = 1\n",
    "    booster = xgb.Booster()\n",
    "    booster.load_model(path)\n",
    "    model = xgb.XGBClassifier()\n",
    "    model._Booster = booster\n",
    "    y_pred=model.predict(feature)\n",
    "    test = metrics.accuracy_score(y_01,y_pred)\n",
    "    print('acc:%f'%test)\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_no_shuffle(model,feature,y,trans,binary=True,file=None):\n",
    "    if binary:\n",
    "        ind1 = ((y[0]==0)|(y[0]==1)|(y[0]==2)|(y[0]==3)|(y[0]==4)|(y[0]==6))\n",
    "        ind2 = ((y[1]==0)|(y[1]==1)|(y[1]==2)|(y[1]==3)|(y[1]==4)|(y[1]==6))\n",
    "        ind3 = ((y[2]==0)|(y[2]==1)|(y[2]==2)|(y[2]==3)|(y[2]==4)|(y[2]==6))\n",
    "        ind01 = ((y[0]==4)|(y[0]==1)|(y[0]==2)|(y[0]==3)|(y[0]==6))\n",
    "        ind11 = ((y[1]==4)|(y[1]==1)|(y[1]==2)|(y[1]==3)|(y[1]==6))\n",
    "        ind21 = ((y[2]==4)|(y[2]==1)|(y[2]==2)|(y[2]==3)|(y[2]==6))\n",
    "        \n",
    "        y_01 = y[0][ind1].copy()\n",
    "        y_02 = y[1][ind2].copy()\n",
    "        y_03 = y[2][ind3].copy()\n",
    "\n",
    "        #ind1 = ((y_01==1)|(y_01==2)|(y_01==6))\n",
    "        y_01[ind01] = 1\n",
    "        y_02[ind11] = 1\n",
    "        y_03[ind21] = 1\n",
    "        \n",
    "    else:\n",
    "        ind1 = ((y[0]==2)|(y[0]==6))\n",
    "        ind2 = ((y[1]==2)|(y[1]==6))\n",
    "        ind3 = ((y[2]==2)|(y[2]==6))\n",
    "        #ind = ((y==2)|(y==6))\n",
    "        y_01 = y[0][ind1].copy()\n",
    "        y_02 = y[1][ind2].copy()\n",
    "        y_03 = y[2][ind3].copy()\n",
    "\n",
    "\n",
    "    x_train = feature[0][ind1]\n",
    "    y_train = y_01\n",
    "    x_valid = feature[1][ind2]\n",
    "    y_valid = y_02\n",
    "    x_train,x_valid,y_train,y_valid = train_test_split(np.concatenate([x_train,x_valid]),np.concatenate([y_train,y_valid]),\n",
    "                                                       test_size=0.25,\n",
    "                                                       random_state=555,\n",
    "                                                       shuffle=True)\n",
    "    x_test = feature[2][ind3]\n",
    "    y_test = y_03\n",
    "#     scaler = StandardScaler(with_mean=True)\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_valid = scaler.fit_transform(x_valid)\n",
    "    x_test = scaler.fit_transform(x_test)\n",
    "#     pca = PCA(n_components=150,copy=True)\n",
    "    x_train = trans.fit_transform(x_train)\n",
    "    x_valid = trans.transform(x_valid)\n",
    "    x_test = trans.transform(x_test)\n",
    "    #sm = BorderlineSMOTE(random_state=50)\n",
    "    #x_train,y_train = sm.fit_resample(x_train,y_train)\n",
    "#     s_ind1 = y_train == 0\n",
    "#     s_ind2 = y_train == 2\n",
    "#     s_ind3 = y_train == 6\n",
    "#     samples_weights = np.zeros(y_train.shape)\n",
    "#     samples_weights[s_ind1]=1\n",
    "#     samples_weights[s_ind2]=100\n",
    "#     samples_weights[s_ind3]=100\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    if binary:\n",
    "        y_pred_t=model.predict(x_test)\n",
    "        test = metrics.accuracy_score(y_test,y_pred_t)\n",
    "        y_pred_v=model.predict(x_valid)\n",
    "        valid = metrics.accuracy_score(y_valid,y_pred_v)\n",
    "        y_pred_ta=model.predict(x_train)\n",
    "        train = metrics.accuracy_score(y_train,y_pred_ta)        \n",
    "        print('train: \\n',metrics.confusion_matrix(y_train,y_pred_ta))\n",
    "        print('valid: \\n',metrics.confusion_matrix(y_valid,y_pred_v))\n",
    "        print('test: \\n',metrics.confusion_matrix(y_test,y_pred_t))\n",
    "        print('test:%f'%test)\n",
    "        print('valid:%f'%valid)\n",
    "        print('train:%f'%train)\n",
    "\n",
    "    else:\n",
    "        y_pred_t=model.predict(x_test)\n",
    "        test = metrics.accuracy_score(y_test,y_pred_t)\n",
    "        y_pred_v=model.predict(x_valid)\n",
    "        valid = metrics.accuracy_score(y_valid,y_pred_v)\n",
    "        y_pred_ta=model.predict(x_train)\n",
    "        train = metrics.accuracy_score(y_train,y_pred_ta)        \n",
    "        print('train: \\n',metrics.confusion_matrix(y_train,y_pred_ta))\n",
    "        print('valid: \\n',metrics.confusion_matrix(y_valid,y_pred_v))\n",
    "        print('test: \\n',metrics.confusion_matrix(y_test,y_pred_t))\n",
    "        print('test:%f'%test)\n",
    "        print('valid:%f'%valid)\n",
    "        print('train:%f'%train)\n",
    "#     if file != None:\n",
    "#         model.save_model(file)\n",
    "    #return train,test     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = ((y==0)|(y==2))\n",
    "y_02 = y[ind]\n",
    "y_02[y_02==2] = 1\n",
    "oh_ec = OneHotEncoder()\n",
    "y_oh = oh_ec.fit_transform(y[:,np.newaxis]).toarray()\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(feature[ind],y_02,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = ((y==1)|(y==2)|(y==3)|(y==6))\n",
    "#ind = ((y==4)|(y==1)|(y==2)|(y==3)|(y==6))\n",
    "ind_f = [0,1,6,42,46,57,62]\n",
    "y_01 = y[ind]\n",
    "y_01[y_01==1]=0\n",
    "y_01[y_01==2]=1\n",
    "y_01[y_01==3]=2\n",
    "y_01[y_01==6]=3\n",
    "#y_01 = y\n",
    "#y_01[ind] = 1\n",
    "oh_ec = OneHotEncoder()\n",
    "y_oh = oh_ec.fit_transform(y_01[:,np.newaxis]).toarray()\n",
    "x_full,x_test,y_full,y_test = train_test_split(feature_sc[ind][:-2000],y_01[:-2000],test_size=0.2,random_state=123)\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(x_full,y_full,test_size=0.2,random_state=555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel=\"rbf\",C=150,\n",
    "            #class_weight={0:1,2:10,6:10},\n",
    "#             class_weight={0:1,1:5},\n",
    "            gamma='auto',\n",
    "           #decision_function_shape='ovo'\n",
    "           )\n",
    "#model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: \n",
      " [[257   2]\n",
      " [  0 161]]\n",
      "valid: \n",
      " [[87  9]\n",
      " [ 0 45]]\n",
      "test: \n",
      " [[57  0]\n",
      " [19 73]]\n",
      "test:0.872483\n",
      "valid:0.936170\n",
      "train:0.995238\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=150,copy=True)\n",
    "sfm = SelectFromModel(GradientBoostingClassifier(),max_features=80)\n",
    "rfe = RFE(estimator=LogisticRegression(max_iter=10000), n_features_to_select=200)\n",
    "vt = VarianceThreshold(threshold=0.01)\n",
    "train_model_no_shuffle(model,(np.array(feature),np.array(feature2),np.array(feature3)),\n",
    "                       (np.array(Y),np.array(Y2),np.array(Y3)),\n",
    "                       pca,\n",
    "                       False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler(with_mean=True)\n",
    "feature_sc = sc.fit_transform(feature)\n",
    "feature2_sc = sc.transform(feature2)\n",
    "acc={}\n",
    "train,valid,test = train_model(model,feature,np.array(y),True)\n",
    "acc_rest=test_model(model,feature2,y2)\n",
    "acc['SVM'] = [train,valid,test,acc_rest]\n",
    "print(acc_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_01[28844:],y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sc.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[10,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
